#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import re
import os
import sys
import time
import shutil
import atexit
import datetime
import argparse
import commands
import urllib
import shelve
import copy
import types
import json
import copy

# default cloud/site
defaultCloud = None
defaultSite  = 'AUTO'

# max lookup for cross site option
maxCrossSite = 50

# error code
EC_Config    = 10
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80
EC_Submit    = 90

usage = """prun [options]

  HowTo is available at https://twiki.cern.ch/twiki/bin/view/PanDA/PandaRun"""

# command-line parameters
optP = argparse.ArgumentParser(usage=usage, conflict_handler="resolve")
optP.add_argument('--version',action='store_const',const=True,dest='version',default=False,
                  help='Displays version')
optP.add_argument('--inDS',action='store',dest='inDS',default='',
                  help='Name of an input dataset or dataset container')
optP.add_argument('--inDsTxt',action='store',dest='inDsTxt',default='',
                  help='A text file which contains the list of datasets to run over. Newlines are replaced by commas and the result is set to --inDS. Lines starting with # are ignored')
optP.add_argument('--inOutDsJson', action='store', dest='inOutDsJson', default='',
                  help="A json file to specify input and output datasets for bulk submission. It contains a json dump of [{'inDS': a comma-concatenated input dataset names, 'outDS': output dataset name}, ...]")
optP.add_argument('--goodRunListXML', action='store', dest='goodRunListXML', default='',
                  help='Good Run List XML which will be converted to datasets by AMI')
optP.add_argument('--goodRunListDataType', action='store', dest='goodRunDataType', default='',
                  help='specify data type when converting Good Run List XML to datasets, e.g, AOD (default)')
optP.add_argument('--goodRunListProdStep', action='store', dest='goodRunProdStep', default='',
                  help='specify production step when converting Good Run List to datasets, e.g, merge (default)')
optP.add_argument('--goodRunListDS', action='store', dest='goodRunListDS', default='',
                  help='A comma-separated list of pattern strings. Datasets which are converted from Good Run List XML will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. If this option is omitted all datasets will be used')
optP.add_argument('--eventPickEvtList',action='store',dest='eventPickEvtList',default='',
                  help='a file name which contains a list of runs/events for event picking')
optP.add_argument('--eventPickDataType',action='store',dest='eventPickDataType',default='',
                  help='type of data for event picking. one of AOD,ESD,RAW')
optP.add_argument('--ei_api',action='store',dest='ei_api',default='',
                  help='flag to signalise mc in event picking')
optP.add_argument('--eventPickStreamName',action='store',dest='eventPickStreamName',default='',
                  help='stream name for event picking. e.g., physics_CosmicCaloEM')
optP.add_argument('--eventPickDS',action='store',dest='eventPickDS',default='',
                  help='A comma-separated list of pattern strings. Datasets which are converted from the run/event list will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. e.g., data\*')
optP.add_argument('--eventPickStagedDS',action='store',dest='eventPickStagedDS',default='',
                  help='--eventPick options create a temporary dataset to stage-in interesting files when those files are available only on TAPE, and then a stage-in request is automatically sent to DaTRI. Once DaTRI transfers the dataset to DISK you can use the dataset as an input using this option')
optP.add_argument('--eventPickAmiTag',action='store',dest='eventPickAmiTag',default='',
                  help='AMI tag used to match TAG collections names. This option is required when you are interested in older data than the latest one. Either \ or "" is required when a wild-card is used. e.g., f2\*')
optP.add_argument('--eventPickNumSites',action='store',dest='eventPickNumSites',default=1,
                type=int, help='The event picking service makes a temprary dataset container to stage files to DISK. The consistuent datasets are distibued to N sites (N=1 by default)')
optP.add_argument('--eventPickSkipDaTRI',action='store_const',const=True,dest='eventPickSkipDaTRI',default=False,
                help='Skip sending a staging request to DaTRI for event picking')
optP.add_argument('--eventPickWithGUID',action='store_const',const=True,dest='eventPickWithGUID',default=False,
                help='Using GUIDs together with run and event numbers in eventPickEvtList to skip event lookup')
optP.add_argument('--express', action='store_const',const=True,dest='express',default=False,
                help="Send the job using express quota to have higher priority. The number of express subjobs in the queue and the total execution time used by express subjobs are limited (a few subjobs and several hours per day, respectively). This option is intended to be used for quick tests before large submission. Note that buildXYZ is not included in quota calculation. If this option is used when quota has already exceeded, the panda server will ignore the option so that subjobs have normal priorities. Also, if you submit 1 buildXYZ and N runXYZ subjobs when you only have quota of M (M < N),  only the first M runXYZ subjobs will have higher priorities")
optP.add_argument('--debugMode', action='store_const',const=True,dest='debugMode',default=False,
                help="Send the job with the debug mode on. If this option is specified the subjob will send stdout to the panda monitor every 5 min. The number of debug subjobs per user is limited. When this option is used and the quota has already exceeded, the panda server supresses the option so that subjobs will run without the debug mode. If you submit multiple subjobs in a single job, only the first subjob will set the debug mode on. Note that you can turn the debug mode on/off by using pbook after jobs are submitted" )
optP.add_argument('--useContElementBoundary',action='store_const',const=True,dest='useContElementBoundary',default=False,
                help="Split job in such a way that sub jobs do not mix files of different datasets in the input container. See --useNthFieldForLFN too")
optP.add_argument('--addNthFieldOfInDSToLFN',action='store',dest='addNthFieldOfInDSToLFN',default='',
                help="A middle name is added to LFNs of output files when they are produced from one dataset in the input container or input dataset list. The middle name is extracted from the dataset name. E.g., if --addNthFieldOfInDSToLFN=2 and the dataset name is data10_7TeV.00160387.physics_Muon..., 00160387 is extracted and LFN is something like user.hoge.TASKID.00160387.blah. Concatenate multiple field numbers with commas if necessary, e.g., --addNthFieldOfInDSToLFN=2,6.")
optP.add_argument('--addNthFieldOfInFileToLFN',action='store',dest='addNthFieldOfInFileToLFN',default='',
                help="A middle name is added to LFNs of output files similarly as --addNthFieldOfInDSToLFN, but strings are extracted from input file names")
optP.add_argument('--buildInLastChunk',action='store_const',const=True,dest='buildInLastChunk',default=False,
                help="Produce lib.tgz in the last chunk when jobs are split to multiple chunks due to the limit on the number of files in each chunk or due to --useContElementBoundary/--loadXML")
optP.add_argument('--followLinks',action='store_const',const=True,dest='followLinks',default=False,
                help="Resolve symlinks to directories when building the input tarball. This option requires python2.6 or higher")
optP.add_argument('--outDS',action='store',dest='outDS',default='',
                help='Name of an output dataset. OUTDS will contain all output files')
optP.add_argument('--destSE',action='store', dest='destSE',default='',
                help='Destination strorage element')
optP.add_argument('--libDS',action='store',dest='libDS',default='',
                help='Name of a library dataset')
optP.add_argument('--removedDS', action='store', dest='removedDS', default='',
                  help="don't use datasets in the input dataset container")
optP.add_argument('--useHomeDir', action='store_const',const=True,dest='useHomeDir',default=False,
                help='execute prun just under the HOME dir')
optP.add_argument('--noBuild', action='store_const',const=True,dest='noBuild',default=False,
                help='Skip buildGen')
optP.add_argument('--bulkSubmission', action='store_const', const=True, dest='bulkSubmission', default=False,
                help='Bulk submit tasks. When this option is used, --inOutDsJson is required while --inDS and --outDS are ignored. It is possible to use %%DATASET_IN and %%DATASET_OUT in --exec which are replaced with actual dataset names when tasks are submitted, and %%BULKSEQNUMBER which is replaced with a sequential number of tasks in the bulk submission')
optP.add_argument('--noCompile', action='store_const',const=True,dest='noCompile',default=False,
                help='Just upload a tarball in the build step to avoid the tighter size limit imposed by --noBuild. The tarball contains binaries compiled on your local computer, so that compilation is skipped in the build step on remote WN')
optP.add_argument('--individualOutDS',action='store_const',const=True,dest='individualOutDS',default=False,
                help='Create individual output dataset for each data-type. By default, all output files are added to one output dataset')
optP.add_argument('--transferredDS',action='store', dest='transferredDS',default='',
                help='Specify a comma-separated list of patterns so that only datasets which match the given patterns are transferred when --destSE is set. Either \ or "" is required when a wildcard is used. If omitted, all datasets are transferred')
optP.add_argument('--secondaryDSs',action='store',dest='secondaryDSs',default='',
                help='List of secondary datasets when the job requires multiple inputs. See PandaRun wiki page for detail')
optP.add_argument('--reusableSecondary',action='store',dest='reusableSecondary',default='',
                help='A comma-separated list of secondary streams which reuse files when all files are used')
optP.add_argument('--site',action='store',dest='site',default=defaultSite,
                help='Site name where jobs are sent. If omitted, jobs are automatically sent to sites where input is available. A comma-separated list of sites can be specified (e.g. siteA,siteB,siteC), so that best sites are chosen from the given site list. If AUTO is appended at the end of the list (e.g. siteA,siteB,siteC,AUTO), jobs are sent to any sites if input is not found in the previous sites')
optP.add_argument('--cloud',action='store',dest='cloud',default=None,
                help='Cloud where jobs are submitted. default is set according to your VOMS country group')
optP.add_argument('--match',action='store',dest='match',default='',
                help='Use only files matching with given pattern')
optP.add_argument('--antiMatch',action='store',dest='antiMatch',default='',
                help='Skip files matching with given pattern')
optP.add_argument('--notSkipLog',action='store_const',const=True,dest='notSkipLog',default=False,
                help="Don't skip log files in input datasets (obsolete. use --useLogAsInput instead)")
optP.add_argument('--skipScan',action='store_const',const=True,dest='skipScan',default=False,
                help='Skip LFC lookup at job submission')
optP.add_argument('--memory',action='store',dest='memory',default=-1,type=int,
                help='Required memory size in MB. e.g., for 1GB --memory 1024')
optP.add_argument('--nCore', action='store', dest='nCore', default=-1, type=int,
                help='The number of CPU cores. Note that the system distinguishes only nCore=1 and nCore>1. This means that even if you set nCore=2 jobs can go to sites with nCore=8 and your application must use the 8 cores there. The number of available cores is defined in an environment variable, $ATHENA_PROC_NUMBER, on WNs. Your application must check the env variable when starting up to dynamically change the number of cores')
optP.add_argument('--maxCpuCount',action='store',dest='maxCpuCount',default=-1,type=int,
                help='Required CPU count in seconds. Mainly to extend time limit for looping job detection')
optP.add_argument('--safetySize',action='store',dest='safetySize',default=500,type=int,
                help='Specify the expected size of input sandbox in MB. 500 by default')
optP.add_argument('--useShortLivedReplicas', action='store_const', const=True, dest='useShortLivedReplicas', default=False,
                help="Use replicas even if they have very sort lifetime")
optP.add_argument('--useDirectIOSites', action='store_const', const=True, dest='useDirectIOSites', default=False,
                help="Use only sites which use directIO to read input files")
optP.add_argument('--official',action='store_const',const=True,dest='official',default=False,
                help='Produce official dataset')
optP.add_argument('--unlimitNumOutputs', action='store_const', const=True, dest='unlimitNumOutputs',  default=False,
                help='Remove the limit on the number of outputs. Note that having too many outputs per job causes a severe load on the system. You may be banned if you carelessly use this option') 
optP.add_argument('--descriptionInLFN',action='store',dest='descriptionInLFN',default='',
                help='LFN is user.nickname.jobsetID.something (e.g. user.harumaki.12345.AOD._00001.pool) by default. This option allows users to put a description string into LFN. i.e., user.nickname.jobsetID.description.something')
optP.add_argument('--useRootCore',action='store_const',const=True,dest='useRootCore',default=False,
                help='Use RootCore. See PandaRun wiki page for detail')
optP.add_argument('--useMana',action='store_const',const=True,dest='useMana',default=False,
                help='Use Mana. See PandaRun wiki page for detail')
optP.add_argument('--manaVer',action='store',dest='manaVer',default='',
                help='Specify a mana version if you want to use an older version. If omitted, the latest version is used. e.g., --manaVer=20130301' )
optP.add_argument('--useAthenaPackages',action='store_const',const=True,dest='useAthenaPackages',default=False,
                help='Use Athena packages. See PandaRun wiki page for detail')
optP.add_argument('--gluePackages', action='store', dest='gluePackages',  default='',
                help='list of glue packages which pathena cannot fine due to empty i686-slc4-gcc34-opt. e.g., External/AtlasHepMC,External/Lhapdf')
optP.add_argument('--nFiles',action='store',dest='nFiles',default=0,type=int,
                help='Use a limited number of files in the input dataset')
optP.add_argument('--nSkipFiles',action='store',dest='nSkipFiles',default=0,type=int,
                help='Skip N files in the input dataset')
optP.add_argument('--nFilesPerJob',action='store',dest='nFilesPerJob',default=None,type=int,
                help='Number of files on which each sub-job runs (default 50). Note that this is the number of files per sub-job in the primary dataset even if --secondaryDSs is used')
optP.add_argument('--nJobs',action='store',dest='nJobs',default=-1,type=int,
                help='Maximum number of sub-jobs. If the number of input files (N_in) is less than nJobs*nFilesPerJob, only N_in/nFilesPerJob sub-jobs will be instantiated')
optP.add_argument('--nEvents',action='store',dest='nEvents',default=-1,type=int,
                help='The total number of events to be processed. This option is considered only when either --inDS or --pfnList is not used')
optP.add_argument('--nEventsPerJob', action='store', dest='nEventsPerJob',  default=-1, type=int,
                help='Number of events per subjob. This is used mainly for job splitting. If you set nEventsPerFile, the total number of subjobs is nEventsPerFile*nFiles/nEventsPerJob. Otherwise, it gets from rucio the number of events in each input file and subjobs are created accordingly. Note that you need to explicitly specify in --exec some parameters like %%MAXEVENTS, %%SKIPEVENTS and %%FIRSTEVENT and your application needs to process only an event chunk accordingly, to avoid subjobs processing the same events. All parameters descibed in https://twiki.cern.ch/twiki/bin/view/PanDA/PandaAthena#example_8_How_to_run_production are available')
optP.add_argument('--nEventsPerFile', action='store', dest='nEventsPerFile',  default=0, type=int,
                help='Number of events per file')
optP.add_argument('--nEventsPerChunk',action='store',dest='nEventsPerChunk',default=-1,type=int,
                help='Set granuarity to split events. The number of events per job is multiples of nEventsPerChunk. This option is considered only when --nEvents is used but --nJobs is not used. If this option is not set, nEvents/20 is used as nEventsPerChunk')
optP.add_argument('--nGBPerJob',action='store',dest='nGBPerJob',default=-1,
                help='Instantiate one sub job per NGBPERJOB GB of input files. --nGBPerJob=MAX sets the size to the default maximum value')
optP.add_argument('--maxFileSize',action='store',dest='maxFileSize',default=1024*1024,type=int,
                help='Maximum size of files to be sent to WNs (default 1024*1024B)')
optP.add_argument('--athenaTag',action='store',dest='athenaTag',default='',
                help='Tags to setup Athena on remote WNs, e.g., --athenaTag=AtlasProduction,14.2.24.3')
optP.add_argument('--rootVer',action='store',dest='rootVer',default='',
                help='Specify a ROOT version which is not included in Athena. This option can not be used together with --noBuild, e.g., --rootVer=5.28/00' )
optP.add_argument('--workDir',action='store',dest='workDir',default='.',
                help='All files under WORKDIR will be transfered to WNs (default=./)')
optP.add_argument('--extFile',action='store',dest='extFile',default='',
                help='root or large files under WORKDIR are not sent to WNs by default. If you want to send some skipped files, specify their names, e.g., data.root,data.tgz')
optP.add_argument('--excludeFile',action='store',dest='excludeFile',default='',
                help='specify a comma-separated string to exclude files and/or directories when gathering files in local working area. Either \ or "" is required when a wildcard is used. e.g., doc,\*.C')
optP.add_argument('--inputFileList', action='store', dest='inputFileListName', default='',
                  help='name of file which contains a list of files to be run in the input dataset')
optP.add_argument('--crossSite',action='store',dest='crossSite',default=maxCrossSite,
                type=int,help='submit jobs to N sites at most when datasets in container split over many sites (N=%s by default)' % maxCrossSite)
optP.add_argument('--outputs',action='store',dest='outputs',default='',
                help='Names of output files. Comma separated. e.g., --outputs out1.dat,out2.txt. You can specify a suffix for each output container like <datasetNameSuffix>:<outputFileName>. e.g., --outputs AAA:out1.dat,BBB:out2.txt. In this case output container names are outDS_AAA/ and outDS_BBB/ instead of outDS_out1.dat/ and outDS_out2.txt/')
optP.add_argument('--allowNoOutput',action='store',dest='allowNoOutput',default='',
                help='A comma-separated list of regexp patterns. Output files are allowed not to be produced if their filenames match with one of regexp patterns. Jobs go to finished even if they are not produced on WN')
optP.add_argument('--excludedSite', action='append', dest='excludedSite',  default=[],
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_argument('--useLogAsInput',action='store_const',const=True,dest='useLogAsInput',default=False,
                help="log.tgz files in inDS are ignored by default. This option allows log files to be used as input")
optP.add_argument('--noSubmit',action='store_const',const=True,dest='noSubmit',default=False,
                help="Don't submit jobs")
optP.add_argument('--prodSourceLabel', action='store', dest='prodSourceLabel', default='',
                help="set prodSourceLabel")
optP.add_argument('--processingType', action='store', dest='processingType', default='prun',
                help="set processingType")
optP.add_argument('--seriesLabel', action='store', dest='seriesLabel',  default='',
                help="set seriesLabel")
optP.add_argument('--workingGroup', action='store', dest='workingGroup',  default=None,
                help="set workingGroup")
optP.add_argument('--tmpDir',action='store',dest='tmpDir',default='',
                help='Temporary directory where an archive file is created')
optP.add_argument('--voms', action='store', dest='vomsRoles',  default=None, 
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_argument('--noEmail', action='store_const', const=True, dest='noEmail',  default=False,
                help='Suppress email notification')
optP.add_argument('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_argument('--spaceToken',action='store',dest='spaceToken',default='',
                help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_argument('--expertOnly_skipScout', action='store_const',const=True,dest='skipScout',default=False,
                help=argparse.SUPPRESS)
optP.add_argument('--respectSplitRule', action='store_const',const=True,dest='respectSplitRule',default=False,
                help="force scout jobs to follow split rules like nGBPerJob")
optP.add_argument('--nGBPerMergeJob',action='store',dest='nGBPerMergeJob',default=-1,
                help='Instantiate one merge job per NGBPERMERGEJOB GB of pre-merged files')
optP.add_argument('--devSrv',action='store_const',const=True,dest='devSrv',default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_argument('--intrSrv', action='store_const', const=True, dest='intrSrv',  default=False,
                help="Please don't use this option. Only for developers to use the intr panda server")
optP.add_argument('--outTarBall', action='store', dest='outTarBall', default='',
                  help='Save a gzipped tarball of local files which is the input to buildXYZ')
optP.add_argument('--inTarBall', action='store', dest='inTarBall', default='',
                  help='Use a gzipped tarball of local files as input to buildXYZ. Generall the tarball is created by using --outTarBall')
optP.add_argument('--exec',action='store',dest='jobParams',default='',
                help='execution string. e.g., --exec "./myscript arg1 arg2"')
optP.add_argument('--bexec',action='store',dest='bexec',default='',
                help='execution string for build stage. e.g., --bexec "make"')
optP.add_argument('--disableAutoRetry',action='store_const',const=True,dest='disableAutoRetry',default=False,
                help='disable automatic job retry on the server side')
optP.add_argument('--myproxy',action='store',dest='myproxy',default='myproxy.cern.ch',
                help='Name of the myproxy server')
optP.add_argument('--maxNFilesPerJob',action='store',dest='maxNFilesPerJob',default=200,type=int,
                help='The maximum number of files per job is 200 by default since too many input files result in a too long command-line argument on WN which crashes the job. This option relax the limit. In many cases it is better to use this option together with --writeInputToTxt')
optP.add_argument('--writeInputToTxt',action='store',dest='writeInputToTxt',default='',
                help='Write the input file list to a file so that your application gets the list from the file instead of stdin. The argument is a comma separated list of StreamName:FileName. e.g., IN:input1.txt,IN2:input2.txt')
optP.add_argument('--dbRelease',action='store',dest='dbRelease',default='',
                  help='DBRelease or CDRelease (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz. If --dbRelease=LATEST, the latest DBRelease is used. Most likely the --useAthenaPackages or --athenaTag option is required to setup Athena runtime on WN')
optP.add_argument('--dbRunNumber',action='store',dest='dbRunNumber', default='',
                  help='RunNumber for DBRelease or CDRelease. If this option is used some redundant files are removed to save disk usage when unpacking DBRelease tarball. e.g., 0091890')
optP.add_argument('--notExpandDBR',action='store_const',const=True,dest='notExpandDBR',default=False,
                help='By default, DBRelease.tar.gz is expanded on WN and gets deleted after changing environment variables accordingly. If you need tar.gz, use this option')
optP.add_argument('--mergeOutput', action='store_const', const=True, dest='mergeOutput', default=False,
                help="merge output files")
optP.add_argument('--mergeScript',action='store',dest='mergeScript',default='',
                help='Specify user-defied script execution string for output merging')
optP.add_argument('--provenanceID',action='store',dest='provenanceID',default=-1,type=int,
                help='provenanceID')
optP.add_argument('--useSiteGroup',action='store',dest='useSiteGroup',default=-1,type=int,
                help='Use only site groups which have group numbers not higher than --siteGroup. Group 0: T1 or undefined, 1,2,3,4: alpha,bravo,charlie,delta which are defined based on site reliability')
optP.add_argument('-v', '--verbose', action='store_const',const=True,dest='verbose',default=False,
                help='Verbose')
optP.add_argument('--long', action='store_const',const=True,dest='long',default=False,
                help='Send job to a long queue')
optP.add_argument('--pfnList',action='store',dest='pfnList',default='',
                help='Name of file which contains a list of input PFNs. Those files can be un-registered in DDM')
optP.add_argument('--outputPath',action='store',dest='outputPath', default='',
                  help='Physical path of output directory relative to a root path')
optP.add_argument('--useOldStyleOutput',action='store_const',const=True,dest='useOldStyleOutput',default=False,
                help="use output dataset and long LFN instead of output dataset container and short LFN")
optP.add_argument('--disableRebrokerage',action='store_const',const=True,dest='disableRebrokerage',default=False,
                help="disable auto-rebrokerage")
optP.add_argument('--useChirpServer',action='store',dest='useChirpServer', default='',
                  help='The CHIRP server where output files are written to. e.g., --useChirpServer voatlas92.cern.ch')
optP.add_argument('--useGOForOutput',action='store',dest='useGOForOutput',default='',metavar='GOENDPOINT',
                  help='The Globus Online server where output files are written to. e.g., --useGOForOutput voatlas92.cern.ch')
optP.add_argument('--enableJEM',action='store_const',const=True,dest='enableJEM',default=False,
                help="enable JEM")
optP.add_argument('--configJEM', action='store', dest='configJEM', default='',
                  help='configration parameters for JEM')
optP.add_argument('--cmtConfig', action='store', dest='cmtConfig', default=None,
                  help='CMTCONFIG=i686-slc5-gcc43-opt is used on remote worker-node by default even if you use another CMTCONFIG locally. This option allows you to use another CMTCONFIG remotely. e.g., --cmtConfig x86_64-slc5-gcc43-opt. If you use --libDS together with this option, make sure that the libDS was compiled with the same CMTCONFIG, in order to avoid failures due to inconsistency in binary files')
optP.add_argument('--requireLFC',action='store_const',const=True,dest='requireLFC',default=False,
                help='Require that LFC lookup is 100% successful (a single miss will fail the submission)')
optP.add_argument('--loadXML',action='store',dest='loadXML',default=None,
                help='Expert mode: load complete submission configuration from an XML file ')
optP.add_argument('--loadJson', action='store', dest='loadJson',default=None,
                  help='Read command-line parameters from a json file which contains a dict of {parameter: value}')
optP.add_argument('--dumpJson', action='store', dest='dumpJson', default=None,
                  help='Dump all command-line parameters and submission result such as returnCode, returnOut, jediTaskID, and bulkSeqNumber if --bulkSubmission is used, to a json file')
optP.add_argument('--forceStaged',action='store_const',const=True,dest='forceStaged',default=False,
                help='Force files from primary DS to be staged to local disk, even if direct-access is possible')
optP.add_argument('--forceStagedSecondary',action='store_const',const=True,dest='forceStagedSecondary',default=False,
                help='Force files from secondary DSs to be staged to local disk, even if direct-access is possible')
optP.add_argument('--queueData', action='store', dest='queueData', default='',
                  help="Please don't use this option. Only for developers")
optP.add_argument('--useNewCode',action='store_const',const=True,dest='useNewCode',default=False,
                help='When task are resubmitted with the same outDS, the original souce code is used to re-run on failed/unprocessed files. This option uploads new source code so that jobs will run with new binaries') 
optP.add_argument('--allowTaskDuplication',action='store_const',const=True,dest='allowTaskDuplication',default=False,
                help="As a general rule each task has a unique outDS and history of file usage is recorded per task. This option allows multiple tasks to contribute to the same outDS. Typically useful to submit a new task with the outDS which was used by another broken task. Use this option very carefully at your own risk, since file duplication happens when the second task runs on the same input which the first task successfully processed")
optP.add_argument('--useRucio',action='store_const',const=True,dest='useRucio',default=False,
                help="Use Rucio as DDM backend")
optP.add_argument('--skipFilesUsedBy', action='store',dest='skipFilesUsedBy',default='',
                  help='A comma-separated list of TaskIDs. Files used by those tasks are skipped when running a new task')
optP.add_argument('--maxAttempt', action='store', dest='maxAttempt', default=-1,
                type=int, help='Maximum number of reattempts for each job (3 by default and not larger than 50)')
optP.add_argument('--containerImage', action='store', dest='containerImage', default='',
                  help="Name of a container image")
optP.add_argument('--priority', action='store', dest='priority',  default=None, type=int,
                  help='Set priority of the task (1000 by default). The value must be between 900 and 1100. ' \
                       'Note that priorities of tasks are relevant only in ' \
                       "each user's share, i.e., your tasks cannot jump over other user's tasks " \
                       'even if you give higher priorities.')

from pandatools import MiscUtils

# parse options
options = optP.parse_args()
if options.verbose:
    print options
    print
# load json
jsonExecStr = ''
if options.loadJson is not None:
    loadOpts = MiscUtils.decodeJSON(options.loadJson)
    for k, v in loadOpts.iteritems():
        origK = k
        if k == 'exec':
            k = 'jobParams'
        if not hasattr(options, k):
            print "ERROR: unknown parameter {0} in {1}".format(k, options.loadJson) 
	    sys.exit(0)
        else:
            setattr(options,k, v)
        if v is True:
            jsonExecStr += ' --{0}'.format(origK)
        else:
            if isinstance(v, types.StringType):
                jsonExecStr += " --{0}='{1}'".format(origK, v)
            else:
                jsonExecStr += " --{0}={1}".format(origK, v)
    if options.verbose:
        print "options after loading json"
        print options
        print

# display version
from pandatools import PandaToolsPkgInfo
if options.version:
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import AppConfig
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# full execution string
fullExecString = PsubUtils.convSysArgv()
fullExecString += jsonExecStr

# set dummy CMTSITE
if not os.environ.has_key('CMTSITE'):
    os.environ['CMTSITE'] = ''

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# set default
appConf = AppConfig.AppConfig('prun')
for tmpAppConfKey,tmpAppConfVal in appConf.getConfig().iteritems():
    # check lower characters just in case
    tmpAppConfKeys = [tmpAppConfKey,tmpAppConfKey.lower()]
    for tmpKey in tmpAppConfKeys:
        if hasattr(options,tmpKey):
            tmpSetAttFlag = False
	    if getattr(options,tmpKey) in [-1,0,None,'','AUTO',False]:
		setattr(options,tmpKey,tmpAppConfVal)
		tmpSetAttFlag = True
            elif getattr(options,tmpKey) == []:
                tmpSetValue = tmpAppConfVal.split(',')
                if '' in tmpSetValue:
                    tmpSetValue.remove('')
                setattr(options,tmpKey,tmpSetValue)
                tmpSetAttFlag = True
            if tmpSetAttFlag:
                # append parameter to metadata
                if isinstance(tmpAppConfVal,types.BooleanType):
                    fullExecString += ' --%s' % tmpKey
                else:
                    fullExecString += ' --%s=%s' % (tmpKey,tmpAppConfVal)
                if options.verbose:
                    tmpLog.debug("Use default option in panda.cfg %s=%s" % (tmpKey,getattr(options,tmpKey)))
            break

# use dev server
if options.devSrv:
    Client.useDevServer()

# use INTR server
if options.intrSrv:
    Client.useIntrServer()
    
# version check
PsubUtils.checkPandaClientVer(options.verbose)

# obsolete options
if options.useOldStyleOutput:
    options.useOldStyleOutput = False
    tmpLog.warning("disabled --useOldStyleOutput since it is obsolete")

# noCompile uses noBuild stuff
if options.noCompile:
    options.noBuild = True

# not skip log files in inDS
if options.notSkipLog:
    options.useLogAsInput = True
    
# files to be deleted
delFilesOnExit = []
                            
# suffix for shadow dataset
suffixShadow = Client.suffixShadow

# load submission configuration from xml file (if provided)
xconfig = None
if options.loadXML != None:
    from pandatools import ParseJobXML
    xconfig = ParseJobXML.dom_parser(options.loadXML)
    tmpLog.info('dump XML config')
    xconfig.dump(options.verbose)
    if options.outDS=='':
        options.outDS=xconfig.outDS()
    options.outputs='all'
    options.jobParams='${XML_EXESTR}'
    options.inDS=xconfig.inDS()
    # check XML
    try:
        xconfig.files_in_DS(options.inDS)
    except:
        errtype,errvalue = sys.exc_info()[:2]
        print errvalue
        tmpLog.error('verification of XML failed')
        sys.exit(EC_Config)
    # inDS match and secondaryDS filter will be determined later from xconfig
    options.match=''
    options.secondaryDSs = xconfig.secondaryDSs_config(filter=False)
    # read XML
    xmlFH = open(options.loadXML)
    options.loadXML = xmlFH.read()
    xmlFH.close()

# save current dir
curDir = os.path.realpath(os.getcwd())

# remove whitespaces
if options.outputs != '':
    options.outputs = re.sub(' ','',options.outputs)

brokerageLogs  = []
userBrokerLogs = []

# exclude sites
if options.excludedSite != []:
    options.excludedSite = PsubUtils.splitCommaConcatenatedItems(options.excludedSite)

# use certain sites
includedSite = None
if re.search(',',options.site) != None:
    includedSite = PsubUtils.splitCommaConcatenatedItems([options.site])
    options.site = 'AUTO'

# set maxNFilesPerJob
PsubUtils.limit_maxNumInputs = options.maxNFilesPerJob 

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# cloud specified
if options.cloud != defaultCloud:
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.cloud,'cloud',userBrokerLogs)

# list of output files which can be skipped
options.allowNoOutput = options.allowNoOutput.split(',')

# read datasets from file
if options.inDsTxt != '':
    options.inDS = PsubUtils.readDsFromFile(options.inDsTxt)

# bulk submission
ioList = []
if options.inOutDsJson != '':
    options.bulkSubmission = True
if options.bulkSubmission:
    if options.inOutDsJson == '':
        tmpLog.error("--inOutDsJson is missing")
        sys.exit(EC_Config)
    if options.eventPickEvtList != '':
        tmpLog.error("cannnot use --eventPickEvtList and --inOutDsJson at the same time")
        sys.exit(EC_Config)
    ioList = MiscUtils.decodeJSON(options.inOutDsJson)
    for ioItem in ioList:
        if not ioItem['outDS'].endswith('/'):
            ioItem['outDS'] += '/'
    options.inDS = ioList[0]['inDS']
    options.outDS = ioList[0]['outDS']
else:
    ioList = [{'inDS': options.inDS, 'outDS': options.outDS}]

# use outputPath as outDS
if Client.isDQ2free(options.site):
    if options.outDS != '':
	options.outputPath = options.outDS
    else:
	options.outputPath = './'
    options.outDS = options.outputPath
else:
    # enforce to use output dataset container
    if not options.useOldStyleOutput and not options.outDS.endswith('/'):
        options.outDS = options.outDS + '/'

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0

usingContainerForOut = original_outDS_Name.endswith('/')

# disable expiring file check
if options.useShortLivedReplicas:
    Client.useExpiringFiles()
    
# libDS
libds_file = '%s/libds_prun.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS

# absolute path for PFN list
if options.pfnList != '':
    options.pfnList = os.path.realpath(options.pfnList)

# extract DBR from exec
tmpMatch = re.search('%DB:([^ \'\";]+)',options.jobParams)
if tmpMatch != None:
    options.dbRelease = tmpMatch.group(1)
    options.notExpandDBR = True

# check DBRelease
if options.dbRelease != '' and (options.dbRelease.find(':') == -1 and options.dbRelease !='LATEST'):
    tmpLog.error("invalid argument for --dbRelease. Must be DatasetName:FileName or LATEST")  
    sys.exit(EC_Config)

# check unmerge dataset
PsubUtils.checkUnmergedDataset(options.inDS,options.secondaryDSs)

# Good Run List
if options.goodRunListXML != '' and options.inDS != '':
    tmpLog.error("cannnot use --goodRunListXML and --inDS at the same time")
    sys.exit(EC_Config)

# event picking
if options.eventPickEvtList != '' and options.inDS != '':
    tmpLog.error("cannnot use --eventPickEvtList and --inDS at the same time")
    sys.exit(EC_Config)

# param check for event picking
if options.eventPickEvtList != '':
    if options.eventPickDataType == '':
        tmpLog.error("--eventPickDataType must be specified")
        sys.exit(EC_Config)

# check rootVer
if options.rootVer != '':
    if options.noBuild and not options.noCompile:
        tmpLog.error("--rootVer cannot be used together with --noBuild since ROOT is prepared in the build step")
        sys.exit(EC_Config)
    # change / to .
    options.rootVer = re.sub('/','.',options.rootVer)
    
# check writeInputToTxt
if options.writeInputToTxt != '':
    # remove %
    options.writeInputToTxt = options.writeInputToTxt.replace('%','')
    # loop over all StreamName:FileName
    for tmpItem in options.writeInputToTxt.split(','):
        tmpItems = tmpItem.split(':')
        if len(tmpItems) != 2:
            tmpLog.error("invalid StreamName:FileName in --writeInputToTxt : %s" % tmpItem)
            sys.exit(EC_Config)

# read list of files to be used                                                                                                                           
filesToBeUsed = []
if options.inputFileListName != '':
    rFile = open(options.inputFileListName)
    for line in rFile:
        line = re.sub('\n','',line)
        line = line.strip()
        if line != '':
            filesToBeUsed.append(line)
    rFile.close()
    
# remove whitespaces
if options.inDS != '':
    options.inDS = options.inDS.replace(' ', '')

# warning
if options.nFilesPerJob > 0 and options.nFilesPerJob < 5:
    tmpLog.warning("Very small --nFilesPerJob tends to generate so many short jobs which could send your task to exhausted state "
                   "after scouts are done, since short jobs are problematic for the grid. Please consider not to use the option.")

if options.maxNFilesPerJob < 5:
    tmpLog.warning("Very small --maxNFilesPerJob tends to generate so many short jobs which could send your task to exhausted state "
                   "after scouts are done, since short jobs are problematic for the grid. Please consider not to use the option.")

# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# add allowed sites
if not siteSpecified:
    tmpSt = Client.addAllowedSites(options.verbose)
    if not tmpSt:
        tmpLog.error("Failed to get allowed site list")
        sys.exit(EC_Config)

# removed datasets
if options.removedDS == '':
    options.removedDS = []
else:
    options.removedDS = options.removedDS.split(',')

# convert in/outTarBall to full path
if options.inTarBall != '':
    options.inTarBall = os.path.abspath(os.path.expanduser(options.inTarBall))
if options.outTarBall != '':
    options.outTarBall = os.path.abspath(os.path.expanduser(options.outTarBall))

# set workdir for mana
if options.useMana:
    tmpSt,tmpOut = MiscUtils.getManaSetupParam('workarea')
    if not tmpSt:
        tmpLog.error(tmpOut)
        sys.exit(EC_Config)
    options.workDir = tmpOut
    tmpLog.info("setting workDir=%s for mana" % options.workDir)
    # extract mana version number
    if options.manaVer == '':
        tmpSt,tmpOut = MiscUtils.getManaVer()
        if not tmpSt:
            tmpLog.error(tmpOut)
            sys.exit(EC_Config)
        options.manaVer = tmpOut    

# check working dir
options.workDir = os.path.realpath(options.workDir)
if options.workDir != curDir and (not curDir.startswith(options.workDir+'/')):
    tmpLog.error("you need to run prun in a directory under %s" % options.workDir)
    sys.exit(EC_Config)

# avoid gathering up the home dir
if os.environ.has_key('HOME') and not options.useHomeDir and not options.useAthenaPackages \
       and os.path.realpath(os.path.expanduser(os.environ['HOME'])) == options.workDir:
    tmpStr  = 'prun is executed just under the HOME directoy '
    tmpStr += 'and is going to send all files under the dir including ~/Mail/* and ~/private/*. '
    tmpStr += 'Do you really want that? (Please use --useHomeDir if you want to skip this confirmation)'
    tmpLog.warning(tmpStr)
    while True:
        tmpAnswer = raw_input('y/N: ')
        tmpAnswer = tmpAnswer.strip()
        if tmpAnswer in ['y','N']:
            break
    if tmpAnswer == 'N':
        sys.exit(EC_Config)
        
# run dir
runDir = '.'
if curDir != options.workDir:
    # remove special characters
    wDirString=re.sub('[\+]','.',options.workDir)
    runDir = re.sub('^'+wDirString+'/','',curDir)

# check maxCpuCount 
if options.maxCpuCount > Client.maxCpuCountLimit:
    tmpLog.error("too large maxCpuCount. Must be less than %s" % Client.maxCpuCountLimit)
    sys.exit(EC_Config)

# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (curDir,MiscUtils.wrappedUuidGen())
else:
    tmpDir = '%s/%s' % (os.path.abspath(options.tmpDir),MiscUtils.wrappedUuidGen())    
os.makedirs(tmpDir)

# exit action
def _onExit(dir,files):
    for tmpFile in files:
        commands.getoutput('rm -rf %s' % tmpFile)        
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir,delFilesOnExit)

# parse tag
athenaVer = ''
cacheVer  = ''
nightVer  = ''
groupArea = ''
cmtConfig = ''
if options.useAthenaPackages:
    # get Athena versions
    stA,retA = AthenaUtils.getAthenaVer()
    # failed
    if not stA:
        tmpLog.error("You need to setup Athena runtime to use --useAthenaPackages")
        sys.exit(EC_CMT)
    workArea  = retA['workArea'] 
    athenaVer = 'Atlas-%s' % retA['athenaVer'] 
    groupArea = retA['groupArea'] 
    cacheVer  = retA['cacheVer'] 
    nightVer  = retA['nightVer']
    cmtConfig = retA['cmtConfig']    
    # override run directory
    sString=re.sub('[\+]','.',workArea)
    runDir = re.sub('^%s' % sString, '', curDir)
    if runDir == curDir:
        errMsg  = "You need to run prun in a directory under %s. " % workArea
        errMsg += "If '%s' is a read-only directory, perhaps you did setup Athena without --testarea or the 'here' tag of asetup." % workArea
        tmpLog.error(errMsg)        
        sys.exit(EC_Config)
    elif runDir == '':
        runDir = '.'
    elif runDir.startswith('/'):
        runDir = runDir[1:]
    runDir = runDir+'/'
elif options.athenaTag != '':
    # get list of Athena projects
    listProjects = Client.getCachePrefixes(options.verbose)
    items = options.athenaTag.split(',')
    usingNightlies = False
    for item in items:
        # releases
        match = re.search('^(\d+\.\d+\.\d+)',item)
        if match != None:
            athenaVer = 'Atlas-%s' % match.group(1)
            # cache
	    cmatch = re.search('^(\d+\.\d+\.\d+\.\d+\.*\d*)$',item)
	    if cmatch != None:
		cacheVer += '_%s' % cmatch.group(1)
            else:
                cacheVer += '_%s' % match.group(1)
        else:
            # nightlies
            match = re.search('^(\d+\.\d+\.X|\d+\.X\.\d+)$',item)
            if match != None:
                athenaVer = 'Atlas-%s' % match.group(1)
        # project
        if item.startswith('Atlas') or item in listProjects + ['Athena', 'AthAthDerivation'] \
                or item.startswith('Athena'):
            # ignore AtlasOffline
            if item in ['AtlasOffline']:
                continue
            cacheVer = '-'+item+cacheVer
        # nightlies    
        if item.startswith('rel_'):
            usingNightlies = True
            if 'dev' in items:
                athenaVer = 'Atlas-dev'
            elif 'devval' in items:
                athenaVer = 'Atlas-devval'
            cacheVer  = '-AtlasOffline_%s' % item
	# CMTCONFIG
        if item in ['32','64']:
            tmpLog.warning("%s in --athenaTag is unsupported. Please use --cmtConfig instead" % item)
    # check cache
    if re.search('^-.+_.+$',cacheVer) == None:
        if re.search('^_\d+\.\d+\.\d+\.\d+$',cacheVer) != None:
            # use AtlasProduction
            cacheVer = '-AtlasProduction'+cacheVer
        elif 'AthAnalysisBase' in cacheVer or 'AthAnalysis' in cacheVer:
            # AthAnalysis
            cacheVer  = cacheVer + '_%s' % athenaVer
            athenaVer = ''
        else:
            # unknown
            cacheVer = ''
    # use dev nightlies
    if usingNightlies and athenaVer == '':
        athenaVer = 'Atlas-dev'

# set CMTCONFIG
options.cmtConfig = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig)

# set mana version
if options.useMana and options.manaVer != '': 
    if options.manaVer != '':
        tmpLog.info("checking manaVer=%s on CVMFS" % options.manaVer)
        sMana,oMana,options.manaVer,options.cmtConfig = MiscUtils.checkManaVersion(options.manaVer,options.cmtConfig)
        if not sMana:
            tmpLog.error(oMana)
            sys.exit(EC_Config)
        tmpLog.info("full manaVer=%s cmtConfig=%s" % (options.manaVer,options.cmtConfig))

# check CMTCONFIG
if not AthenaUtils.checkCmtConfig(cmtConfig,options.cmtConfig,options.noBuild):
    sys.exit(EC_Config)
    
# event picking
if options.eventPickEvtList != '':
    epLockedBy = 'prun'
    if not options.noSubmit:
        # request event picking
        epStat,epOutput = Client.requestEventPicking(options.eventPickEvtList,
                                                     options.eventPickDataType,
                                                     options.eventPickStreamName,
                                                     options.eventPickDS,
                                                     options.eventPickAmiTag,
                                                     [],
                                                     options.inputFileListName,
                                                     options.outDS,
                                                     epLockedBy,
                                                     fullExecString,
                                                     options.eventPickNumSites,
                                                     options.eventPickWithGUID,
                                                     options.ei_api,
                                                     options.verbose)
        # set input dataset 
        options.inDS = epOutput
    else:
        options.inDS = 'dummy'
    tmpLog.info('requested Event Picking service to stage input as %s' % options.inDS)

# additinal files
if options.extFile == '':
    options.extFile = []
else:
    tmpItems = options.extFile.split(',')
    options.extFile = []
    # convert * to .*
    for tmpItem in tmpItems:
        options.extFile.append(tmpItem.replace('*','.*'))

# user-specified merging script
if options.mergeScript != '':
    # enable merging
    options.mergeOutput = True
    # add it to extFile
    if not options.mergeScript in options.extFile:
        options.extFile.append(options.mergeScript)

# glue packages
options.gluePackages = options.gluePackages.split(',')
try:
    options.gluePackages.remove('')
except:
    pass

# set excludeFile
AthenaUtils.setExcludeFile(options.excludeFile)

# LFN matching
if options.match != '':
    # convert . to \.
    options.match = options.match.replace('.','\.')
    # convert * to .*
    options.match = options.match.replace('*','.*')

# LFN anti-matching
if options.antiMatch != '':
    # convert . to \.
    options.antiMatch = options.antiMatch.replace('.','\.')
    # convert * to .*
    options.antiMatch = options.antiMatch.replace('*','.*')

# get job script
jobScript = ''
if options.jobParams == '':
    tmpLog.error("you need to give --exec\n  prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'")
    sys.exit(EC_Config)
orig_execStr  = options.jobParams
orig_bexecStr = options.bexec

# replace : to = for backward compatibility
for optArg in ['RNDM']:
    options.jobParams = re.sub('%'+optArg+':','%'+optArg+'=',options.jobParams)

# check output dataset
if options.outDS == '':
    tmpLog.error("no outDS is given\n  prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'")
    sys.exit(EC_Config)

# avoid inDS+pfnList
if options.pfnList != '':
    # don't use inDS
    if options.inDS != '':
	tmpLog.error("--pfnList and --inDS cannot be used at the same time")
	sys.exit(EC_Config)
    # use site    
    if options.site == 'AUTO':
	tmpLog.error("--site must be specified when --pfnList is used")
	sys.exit(EC_Config)
        
# secondary datasets
if options.secondaryDSs != '':
    # parse
    tmpMap = {}
    for tmpItem in options.secondaryDSs.split(','):
        if "#" in tmpItem:
            tmpItems = tmpItem.split('#')
        else:
            tmpItems = tmpItem.split(':')
        if len(tmpItems) in [3,4,5]:
            tmpDsName = tmpItems[2]
            # change ^ to ,
            tmpDsName = tmpDsName.replace('^',',')
            # make map
            tmpMap[tmpDsName] = {'nFiles'     : int(tmpItems[1]),
                                 'streamName' : tmpItems[0],
                                 'pattern'    : '',
                                 'nSkip'      : 0,
                                 'files'      : []}
            # using filtering pattern
            if len(tmpItems) >= 4:
                tmpMap[tmpItems[2]]['pattern'] = tmpItems[3]
            # nSkip
            if len(tmpItems) >= 5:
                tmpMap[tmpItems[2]]['nSkip'] = int(tmpItems[4])
        else:         
            tmpLog.error("Wrong format %s in --secondaryDSs. Must be StreamName:nFilesPerJob:DatasetName[:Pattern[:nSkipFiles]]" \
                         % tmpItem)
            sys.exit(EC_Config)
    # set
    options.secondaryDSs = tmpMap
else:
    options.secondaryDSs = {}

# reusable secondary streams
if options.reusableSecondary == '':
    options.reusableSecondary = []
else:
    options.reusableSecondary = options.reusableSecondary.split(',')

# correct site
if options.site != 'AUTO':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_SHORT"
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site, getAll=True)
    if pID != '':
	if ',' in pID:
	    includedSite = pID.split(',')
	    pID = 'AUTO'
        options.site = pID
    # add ANALY
    if options.site != 'AUTO':
	if not options.site.startswith('ANALY_'):
	    options.site = 'ANALY_%s' % options.site
        # check
        if not Client.PandaSites.has_key(options.site):
            tmpLog.error("unknown siteID:%s" % origSite)
            sys.exit(EC_Config)
        # set cloud
        options.cloud = Client.PandaSites[options.site]['cloud']
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.site,'site',userBrokerLogs)
        
# get DN
distinguishedName = PsubUtils.getDN()

# get nickname
nickName = PsubUtils.getNickname()

if nickName == '':
    sys.exit(EC_Config)

# set Rucio accounting
PsubUtils.setRucioAccount(nickName,'prun',True)

# check nGBPerJob
if not options.nGBPerJob in [-1,'MAX']:
    # convert to int
    try:
        if options.nGBPerJob != 'MAX':
            options.nGBPerJob = int(options.nGBPerJob)
    except:
        tmpLog.error("nGBPerJob must be an integer or MAX")
        sys.exit(EC_Config)
    # check negative    
    if options.nGBPerJob <= 0:
        tmpLog.error("nGBPerJob must be positive")
        sys.exit(EC_Config)

# split options are mutually exclusive
if options.nFilesPerJob > 0 and options.nEventsPerJob > 0 and options.nGBPerJob != -1:
    tmpLog.error("split by files, split by events and split by file size can not be used simultaneously")
    sys.exit(EC_Config)

# split options are mutually exclusive
if options.nEventsPerJob > 0 and options.nGBPerJob != -1:
    tmpLog.error("split by events and split by file size can not be used simultaneously")
    sys.exit(EC_Config)



#####################################################################
# archive sources and send it to HTTP-reachable location

# create archive
if options.containerImage == '':
    if options.inTarBall == '':
        # copy RootCore packages 
        if options.useRootCore:
            # check $ROOTCOREDIR
            if not os.environ.has_key('ROOTCOREDIR'):
                tmpErrMsg  = '$ROOTCOREDIR is not definied in your enviornment. '
                tmpErrMsg += 'Please setup RootCore runtime beforehand'
                tmpLog.error(tmpErrMsg)
                sys.exit(EC_Config)
            # check grid_submit.sh
            rootCoreSubmitSh    = os.environ['ROOTCOREDIR'] + '/scripts/grid_submit.sh'
            rootCoreCompileSh   = os.environ['ROOTCOREDIR'] + '/scripts/grid_compile.sh'
            rootCoreRunSh       = os.environ['ROOTCOREDIR'] + '/scripts/grid_run.sh'
            rootCoreSubmitNbSh  = os.environ['ROOTCOREDIR'] + '/scripts/grid_submit_nobuild.sh'
            rootCoreCompileNbSh = os.environ['ROOTCOREDIR'] + '/scripts/grid_compile_nobuild.sh'
            rootCoreShList = [rootCoreSubmitSh,rootCoreCompileSh,rootCoreRunSh]
            if options.noBuild:
                rootCoreShList.append(rootCoreSubmitNbSh)
                if options.noCompile:
                    rootCoreShList.append(rootCoreCompileNbSh)
            for tmpShFile in rootCoreShList:
                if not os.path.exists(tmpShFile):
                    tmpErrMsg  = "%s doesn't exist. Please use a newer version of RootCore" % tmpShFile
                    tmpLog.error(tmpErrMsg)
                    sys.exit(EC_Config)
            tmpLog.info("copy RootCore packages to current dir")
            # destination
            pandaRootCoreWorkDirName = '__panda_rootCoreWorkDir'
            rootCoreDestWorkDir = curDir + '/' + pandaRootCoreWorkDirName
            # add all files to extFile
            options.extFile.append(pandaRootCoreWorkDirName + '/.*')
            # add to be deleted on exit
            delFilesOnExit.append(rootCoreDestWorkDir)
            if not options.noBuild:
                tmpStat = os.system('%s %s' % (rootCoreSubmitSh,rootCoreDestWorkDir))
            else:
                tmpStat = os.system('%s %s' % (rootCoreSubmitNbSh,rootCoreDestWorkDir))                
            tmpStat %= 255
            if tmpStat != 0:
                tmpErrMsg  = "%s failed with %s" % (rootCoreSubmitSh,tmpStat)
                tmpLog.error(tmpErrMsg)
                sys.exit(EC_Config)
            # copy build and run scripts
            shutil.copy(rootCoreRunSh,rootCoreDestWorkDir)            
            shutil.copy(rootCoreCompileSh,rootCoreDestWorkDir)
            if options.noCompile:
                shutil.copy(rootCoreCompileNbSh,rootCoreDestWorkDir)
        # gather Athena packages
        if options.useAthenaPackages:
            archiveName = ""
            if AthenaUtils.useCMake():
                # archive with cpack
                archiveName,archiveFullName = AthenaUtils.archiveWithCpack(True,tmpDir,options.verbose)
            # set extFile
            AthenaUtils.setExtFile(options.extFile)
            if options.libDS == '' and not options.noBuild:
                # archive sources
                archiveName,archiveFullName = AthenaUtils.archiveSourceFiles(workArea,runDir,curDir,tmpDir,
                                                                             options.verbose,options.gluePackages,
                                                                             dereferenceSymLinks=options.followLinks,
                                                                             archiveName=archiveName)
            else:
                # archive jobO
                archiveName,archiveFullName = AthenaUtils.archiveJobOFiles(workArea,runDir,curDir,
                                                                           tmpDir,options.verbose,
                                                                           archiveName=archiveName)
            # archive InstallArea
            if options.libDS == '':
                AthenaUtils.archiveInstallArea(workArea,groupArea,archiveName,archiveFullName,
                                               tmpDir,options.noBuild,options.verbose)
        # gather normal files
        if True:
            if options.useAthenaPackages:
                # go to workArea
                os.chdir(workArea)
                # gather files under work dir
                tmpLog.info("gathering files under %s/%s" % (workArea,runDir))
                archStartDir = runDir
                archStartDir = re.sub('/+$','',archStartDir)
            else:
                # go to work dir
                os.chdir(options.workDir)
                # gather files under work dir
                tmpLog.info("gathering files under %s" % options.workDir)
                archStartDir = '.'
            # get files in the working dir
            if options.noCompile:
                skippedExt = []
            else:
                skippedExt   = ['.o','.a','.so']
            skippedFlag  = False
            workDirFiles = []
            if options.followLinks:
                osWalkList = os.walk(archStartDir,followlinks=True)
            else:
                osWalkList = os.walk(archStartDir)                
            for tmpRoot,tmpDirs,tmpFiles in osWalkList:
                emptyFlag    = True
                for tmpFile in tmpFiles:
                    if options.useAthenaPackages:
                        if os.path.basename(tmpFile) == os.path.basename(archiveFullName):
                            if options.verbose:
                                print 'skip Athena archive %s' % tmpFile
                            continue
                    tmpPath = '%s/%s' % (tmpRoot,tmpFile)
                    # get size
                    try:
                        size = os.path.getsize(tmpPath)
                    except:
                        # skip dead symlink
                        if options.verbose:
                            type,value,traceBack = sys.exc_info()
                            print "  Ignore : %s:%s" % (type,value)
                        continue
                    # check exclude files
                    excludeFileFlag = False
                    for tmpPatt in AthenaUtils.excludeFile:
                        if re.search(tmpPatt,tmpPath) != None:
                            excludeFileFlag = True
                            break
                    if excludeFileFlag:
                        continue
                    # skipped extension
                    isSkippedExt = False
                    for tmpExt in skippedExt:
                        if tmpPath.endswith(tmpExt):
                            isSkippedExt = True
                            break
                    # check root
                    isRoot = False
                    if re.search('\.root(\.\d+)*$',tmpPath) != None:
                        isRoot = True
                    # extra files
                    isExtra = False
                    for tmpExt in options.extFile:
                        if re.search(tmpExt+'$',tmpPath) != None:
                            isExtra = True
                            break
                    # regular files
                    if not isExtra:
                        # unset emptyFlag even if all files are skipped
                        emptyFlag = False
                        # skipped extensions
                        if isSkippedExt:
                            print "  skip %s %s" % (str(skippedExt),tmpPath)
                            skippedFlag = True
                            continue
                        # skip root
                        if isRoot:
                            print "  skip root file %s" % tmpPath
                            skippedFlag = True
                            continue
                        # check size
                        if size > options.maxFileSize:
                            print "  skip large file %s:%sB>%sB" % (tmpPath,size,options.maxFileSize)
                            skippedFlag = True
                            continue
                    # remove ./
                    tmpPath = re.sub('^\./','',tmpPath)
                    # append
                    workDirFiles.append(tmpPath)
                    emptyFlag = False
                # add empty directory
                if emptyFlag and tmpDirs==[] and tmpFiles==[]:
                    tmpPath = re.sub('^\./','',tmpRoot)
                    # check exclude pattern
                    excludePatFlag = False
                    for tmpPatt in AthenaUtils.excludeFile:
                        if re.search(tmpPatt,tmpPath) != None:
                            excludePatFlag = True
                            break
                    if excludePatFlag:
                        continue
                    # skip tmpDir
                    if tmpPath.split('/')[-1] == tmpDir.split('/')[-1]:
                        continue
                    # append
                    workDirFiles.append(tmpPath)
            if skippedFlag:
                tmpLog.info("please use --maxFileSize or --extFile if you need to send the skipped files to WNs")
            # set archive name
            if not options.useAthenaPackages:    
                # create archive
                if options.libDS != '' or (options.noBuild and not options.noCompile):
                    # use 'jobO' for libDS/noBuild
                    archiveName     = 'jobO.%s.tar' % MiscUtils.wrappedUuidGen()
                else:
                    # use 'sources' for normal build
                    archiveName     = 'sources.%s.tar' % MiscUtils.wrappedUuidGen()
                archiveFullName = "%s/%s" % (tmpDir,archiveName)
            # collect files
            for tmpFile in workDirFiles:
                # avoid self-archiving
                if os.path.basename(tmpFile) == os.path.basename(archiveFullName):
                    if options.verbose:
                        print 'skip self-archiving for %s' % tmpFile
                    continue
                if os.path.islink(tmpFile):
                    status,out = commands.getstatusoutput("tar --exclude '.[a-zA-Z]*' -uh '%s' -f '%s'" % (tmpFile,archiveFullName))
                else:
                    status,out = commands.getstatusoutput("tar --exclude '.[a-zA-Z]*' -uf '%s' '%s'" % (archiveFullName,tmpFile))
                if options.verbose:
                    print tmpFile
                if status != 0 or out != '':
                    print out
        # go to tmpdir
        os.chdir(tmpDir)

        # make empty if archive doesn't exist
        if not os.path.exists(archiveFullName):
            commands.getstatusoutput('tar cvf %s --files-from /dev/null ' % archiveName)

        # compress
        status,out = commands.getstatusoutput('gzip %s' % archiveName)
        archiveName += '.gz'
        if status !=0 or options.verbose:
            print out

        # check archive
        status,out = commands.getstatusoutput('ls -l {0}'.format(archiveName))
        if options.verbose:
            print out
        if status != 0:
            tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'")
            sys.exit(EC_Archive)

        # check symlinks
        if options.useAthenaPackages:
            tmpLog.info("checking symbolic links")
            status,out = commands.getstatusoutput('tar tvfz %s' % archiveName)
            if status != 0:
                tmpLog.error("Failed to expand archive")
                sys.exit(EC_Archive)
            symlinks = []
            for line in out.split('\n'):
                items = line.split()
                if items[0].startswith('l') and items[-1].startswith('/'):
                    symlinks.append(line)
            if symlinks != []:
                tmpStr  = "Found some unresolved symlinks which may cause a problem\n"
                tmpStr += "     See, e.g., http://savannah.cern.ch/bugs/?43885\n"
                tmpStr += "   Please ignore if you believe they are harmless"
                tmpLog.warning(tmpStr)
                for symlink in symlinks:
                    print "  %s" % symlink
    else:
        # go to tmp dir
        os.chdir(tmpDir)
        # use a saved copy
        if options.libDS == '' and (options.noCompile or not options.noBuild):
            archiveName     = 'sources.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        else:
            archiveName     = 'jobO.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        # make copy to avoid name duplication
        shutil.copy(options.inTarBall,archiveFullName)

    # save
    if options.outTarBall != '':
        shutil.copy(archiveName,options.outTarBall)
    
    # upload source files
    if not options.noSubmit:
        # upload sources via HTTP POST
        tmpLog.info("upload source files")
        status,out = Client.putFile(archiveName,options.verbose,useCacheSrv=True,reuseSandbox=True)
        if out.startswith('NewFileName:'):
            # found the same input sandbox to reuse
            archiveName = out.split(':')[-1]
        elif out != 'True':
            print out
            tmpLog.error("failed to upload source files with %s" % status)
            sys.exit(EC_Post)
        # good run list
        if options.goodRunListXML != '':
            options.goodRunListXML = PsubUtils.uploadGzippedFile(options.goodRunListXML,curDir,tmpLog,delFilesOnExit,
                                                                 options.noSubmit,options.verbose)

# special handling
specialHandling = ''
if options.express:
    specialHandling += 'express,'
if options.debugMode:
    specialHandling += 'debug,'
specialHandling = specialHandling[:-1]
    


#####################################################################
# task making

# job name
jobName = 'prun.%s' % MiscUtils.wrappedUuidGen()

# make task
taskParamMap = {}
taskParamMap['taskName'] = options.outDS
if not options.allowTaskDuplication:
    taskParamMap['uniqueTaskName'] = True
taskParamMap['vo'] = 'atlas'
taskParamMap['architecture'] = AthenaUtils.getCmtConfigImg(athenaVer,cacheVer,nightVer,options.cmtConfig)
taskParamMap['transUses'] = athenaVer
if athenaVer != '':
    taskParamMap['transHome'] = 'AnalysisTransforms'+cacheVer+nightVer
else:
    taskParamMap['transHome'] = None
if options.containerImage != '':
    taskParamMap['processingType'] = 'panda-client-{0}-jedi-cont'.format(PandaToolsPkgInfo.release_version)
else:
    taskParamMap['processingType'] = 'panda-client-{0}-jedi-run'.format(PandaToolsPkgInfo.release_version)
if options.eventPickEvtList != '':
    taskParamMap['processingType'] += '-evp'
    taskParamMap['waitInput'] = 1
if options.goodRunListXML != '':
    taskParamMap['processingType'] += '-grl'
if options.prodSourceLabel == '':    
    taskParamMap['prodSourceLabel'] = 'user'
else:
    taskParamMap['prodSourceLabel'] = options.prodSourceLabel
if options.site != 'AUTO':
    taskParamMap['site'] = options.site
else:
    taskParamMap['site'] = None
taskParamMap['cloud'] = options.cloud
taskParamMap['excludedSite'] = options.excludedSite
if includedSite != None and includedSite != []:
    taskParamMap['includedSite'] = includedSite
else:
    taskParamMap['includedSite'] = None
if options.priority is not None:
    taskParamMap['currentPriority'] = options.priority
if options.nFiles != 0:
    taskParamMap['nFiles'] = options.nFiles
if options.nFilesPerJob != None:    
    taskParamMap['nFilesPerJob'] = options.nFilesPerJob
if not options.nGBPerJob in [-1,'MAX']:
    # don't set MAX since it is the defalt on the server side
    taskParamMap['nGBPerJob'] = options.nGBPerJob
if options.nEventsPerJob > 0:
    taskParamMap['nEventsPerJob'] = options.nEventsPerJob
    if options.nEventsPerFile <= 0:
        taskParamMap['useRealNumEvents'] = True
    if options.nJobs > 0 and options.nEvents < 0:
        taskParamMap['nEvents'] = options.nJobs * options.nEventsPerJob
taskParamMap['cliParams'] = fullExecString
if options.noEmail:
    taskParamMap['noEmail'] = True
if options.skipScout:
    taskParamMap['skipScout'] = True
if options.respectSplitRule:
    taskParamMap['respectSplitRule'] = True
if options.disableAutoRetry:
    taskParamMap['disableAutoRetry'] = 1
if options.workingGroup != None:
    taskParamMap['workingGroup'] = options.workingGroup
if options.official:
    taskParamMap['official'] = True
taskParamMap['nMaxFilesPerJob'] = options.maxNFilesPerJob
if options.useNewCode:
    taskParamMap['fixedSandbox'] = archiveName
if options.useRucio:
    taskParamMap['ddmBackEnd'] = 'rucio'
if options.maxCpuCount > 0:
    taskParamMap['walltime'] = -options.maxCpuCount
if options.memory > 0:
    taskParamMap['ramCount'] = options.memory
if options.nCore > 1:
    taskParamMap['coreCount'] = options.nCore
if options.skipFilesUsedBy != '':
    taskParamMap['skipFilesUsedBy'] = options.skipFilesUsedBy
taskParamMap['respectSplitRule'] = True
if options.maxAttempt >0 and options.maxAttempt <= 50:
    taskParamMap['maxAttempt'] = options.maxAttempt
# source URL
matchURL = re.search("(http.*://[^/]+)/",Client.baseURLCSRVSSL)
if matchURL != None:
    taskParamMap['sourceURL'] = matchURL.group(1)
# XML config
if options.loadXML != None:
    taskParamMap['loadXML'] = options.loadXML
# middle name
if options.addNthFieldOfInFileToLFN != '':
    taskParamMap['addNthFieldToLFN'] = options.addNthFieldOfInFileToLFN
    taskParamMap['useFileAsSourceLFN'] = True
elif options.addNthFieldOfInDSToLFN != '':
    taskParamMap['addNthFieldToLFN'] = options.addNthFieldOfInDSToLFN
# dataset names
outDatasetName = options.outDS
logDatasetName = re.sub('/$','.log/',options.outDS)
# log
taskParamMap['log'] = {'dataset': logDatasetName,
                       'container': logDatasetName,
                       'type':'template',
                       'param_type':'log',
                       'value':'{0}.$JEDITASKID.${{SN}}.log.tgz'.format(logDatasetName[:-1])
                       }
if options.addNthFieldOfInFileToLFN != '':
    loglfn  = '{0}.{1}'.format(*logDatasetName.split('.')[:2])
    loglfn += '${MIDDLENAME}.$JEDITASKID._${SN}.log.tgz'
    taskParamMap['log']['value'] = loglfn
if options.spaceToken != '':
    taskParamMap['log']['token'] = options.spaceToken

# job parametes
taskParamMap['jobParameters'] = [
    {'type':'constant',
     'value': '-j "" --sourceURL ${SURL}',
     },
    {'type':'constant',
     'value': '-r {0}'.format(runDir),
     },
    ]
if options.loadXML == None:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-p "',
         'padding':False,
         },
        ]
    taskParamMap['jobParameters'] += PsubUtils.convertParamStrToJediParam(options.jobParams,{},'',
                                                                          True,False,
                                                                          includeIO=False)
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '"',
         },
        ]
else:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-p "{0}"'.format(options.jobParams),
         },
        ]


# build
if options.containerImage == '':
    if options.noBuild and not options.noCompile:
        taskParamMap['jobParameters'] += [
            {'type':'constant',
             'value': '-a {0}'.format(archiveName),
             },
            ]
    else:
        taskParamMap['jobParameters'] += [
            {'type':'constant',
             'value': '-l ${LIB}',
             },
            ]
# output
if options.outputs != '':
    outMap = {}
    dsSuffix = []
    for tmpLFN in options.outputs.split(','):
        tmpDsSuffix = ''
        if ':' in tmpLFN:
            tmpDsSuffix,tmpLFN = tmpLFN.split(':')
            if tmpDsSuffix in dsSuffix:
                tmpErrMsg  = "dataset name suffix '%s' is used for multiple files in --outputs. " % tmpDsSuffix
                tmpErrMsg += 'each output must have a unique suffix.' 
                tmpLog.error(tmpErrMsg)
                sys.exit(EC_Config)
            dsSuffix.append(tmpDsSuffix)
        tmpNewLFN = tmpLFN
	# change * to XYZ and add .tgz
	if '*' in tmpNewLFN:
	    tmpNewLFN = tmpNewLFN.replace('*','XYZ')
	    tmpNewLFN += '.tgz'
        if len(outDatasetName.split('.')) > 2:
            lfn = '{0}.{1}'.format(*outDatasetName.split('.')[:2])
        else:
            lfn = outDatasetName[:-1]
        if options.addNthFieldOfInDSToLFN != '' or options.addNthFieldOfInFileToLFN != '':
            lfn += '${MIDDLENAME}'
        lfn += '.$JEDITASKID._${{SN/P}}.{0}'.format(tmpNewLFN)
        if tmpDsSuffix == '':
            tmpDsSuffix = tmpNewLFN
        dataset = '{0}_{1}/'.format(outDatasetName[:-1],tmpDsSuffix)
        taskParamMap['jobParameters'] += MiscUtils.makeJediJobParam(lfn,dataset,'output',hidden=True,
                                                                    destination=options.destSE,
                                                                    token=options.spaceToken,
                                                                    allowNoOutput=options.allowNoOutput)
        outMap[tmpLFN] = lfn
    if options.loadXML:
        taskParamMap['jobParameters'] += [ 
            {'type':'constant',
             'value': '-o "${XML_OUTMAP}"',
             },
            ]
    else:
        taskParamMap['jobParameters'] += [ 
            {'type':'constant',
             'value': '-o "{0}"'.format(str(outMap)),
             },
            ]
# input
if options.inDS != '': 
    tmpDict = {'type':'template',
               'param_type':'input',
               'value':'-i "${IN/T}"',
               'dataset':options.inDS,
               'exclude':'\.log\.tgz(\.\d+)*$',
               }
    if options.useLogAsInput:
        del tmpDict['exclude']
    if options.loadXML == None:
        tmpDict['expand'] = True
    if options.nSkipFiles != 0:
        tmpDict['offset'] = options.nSkipFiles
    if options.match != '':
        tmpDict['include'] = options.match
    if options.antiMatch != '':
        if 'exclude' in tmpDict:
            tmpDict['exclude'] += ','+options.antiMatch
        else:
            tmpDict['exclude'] = options.antiMatch
    if filesToBeUsed != []:
        tmpDict['files'] = filesToBeUsed
    taskParamMap['jobParameters'].append(tmpDict)
    taskParamMap['dsForIN'] = options.inDS
elif options.pfnList != '':
    taskParamMap['pfnList'] = PsubUtils.getListPFN(options.pfnList)
    # use noInput mecahism
    taskParamMap['noInput'] = True
    if options.nFiles == 0:
        taskParamMap['nFiles'] = len(taskParamMap['pfnList'])
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value':'-i "${IN/T}"',
         },
        ]
elif options.goodRunListXML != '':
    tmpDict = {'type':'template',
               'param_type':'input',
               'value':'-i "${IN/T}"',
               'dataset':'%%INDS%%',
	       'expand':True,
               'exclude':'\.log\.tgz(\.\d+)*$',
               'files':'%%INLFNLIST%%',
               }
    taskParamMap['jobParameters'].append(tmpDict)
    taskParamMap['dsForIN'] = '%%INDS%%'
else:
    # no input
    taskParamMap['noInput'] = True
    if options.nEvents > 0:
	taskParamMap['nEvents'] = options.nEvents
	if options.nJobs > 0:
	    taskParamMap['nEventsPerJob'] = options.nEvents / options.nJobs
        else:
            # set granularity
            if options.nEventsPerChunk > 0:
                taskParamMap['nEventsPerRange'] = options.nEventsPerChunk
            else:
                # use 1/20 by default
                taskParamMap['nEventsPerRange'] = options.nEvents / 20
                if taskParamMap['nEventsPerRange'] <= 0:
                    taskParamMap['nEventsPerRange'] = 1
    else:
        if options.nJobs > 0:
            taskParamMap['nEvents'] = options.nJobs
        else:
            taskParamMap['nEvents'] = 1
        taskParamMap['nEventsPerJob'] = 1

# param for DBR     
if options.dbRelease != '':
    dbrDS = options.dbRelease.split(':')[0]
    # change LATEST to DBR_LATEST
    if dbrDS == 'LATEST':
        dbrDS = 'DBR_LATEST'
    dictItem = {'type':'template',
                'param_type':'input',
                'value':'--dbrFile=${DBR}',
                'dataset':dbrDS,
                }
    taskParamMap['jobParameters'] += [dictItem]
    # no expansion
    if options.notExpandDBR:
        dictItem = {'type':'constant',
                    'value':'--noExpandDBR',
                    }
        taskParamMap['jobParameters'] += [dictItem]

# secondary
if options.secondaryDSs != {}:
    inMap = {}
    streamNames = []
    if options.inDS != '':
	inMap['IN'] = 'tmp_IN'
	streamNames.append('IN')
    for tmpDsName,tmpMap in options.secondaryDSs.iteritems():
        # make template item
        streamName = tmpMap['streamName']
        if options.loadXML == None:
            expandFlag = True
        else:
            expandFlag = False
        # re-usability
        reusableAtt = False
        if streamName in options.reusableSecondary:
            reusableAtt = True
        dictItem = MiscUtils.makeJediJobParam('${'+streamName+'}',tmpDsName,'input',hidden=True,
                                              expand=expandFlag,include=tmpMap['pattern'],offset=tmpMap['nSkip'],
                                              nFilesPerJob=tmpMap['nFiles'],reusableAtt=reusableAtt)
        taskParamMap['jobParameters'] += dictItem
        inMap[streamName] = 'tmp_'+streamName 
        streamNames.append(streamName)
    # make constant item
    strInMap = str(inMap)
    # set placeholders
    for streamName in streamNames:
        strInMap = strInMap.replace("'tmp_"+streamName+"'",'${'+streamName+'/T}')
    dictItem = {'type':'constant',
                'value':'--inMap "%s"' % strInMap,
                }
    taskParamMap['jobParameters'] += [dictItem]

# misc
jobParameters = ''
# given PFN 
if options.pfnList != '':
    jobParameters += '--givenPFN '
# use Athena packages
if options.useAthenaPackages:
    jobParameters += "--useAthenaPackages "
# use CMake
if AthenaUtils.useCMake():
    jobParameters += "--useCMake "
# use RootCore
if options.useRootCore:
    jobParameters += "--useRootCore "
# use mana
if options.useMana:
    jobParameters += "--useMana "
    if options.manaVer != '':
        jobParameters += "--manaVer %s " % options.manaVer
# root
if options.rootVer != '':
    jobParameters += "--rootVer %s " % options.rootVer
# write input to txt
if options.writeInputToTxt != '':
    jobParameters += "--writeInputToTxt %s " % options.writeInputToTxt
# debug parameters
if options.queueData != '':
    jobParameters += "--overwriteQueuedata=%s " % options.queueData
# JEM
if options.enableJEM:
    jobParameters += "--enable-jem "
    if options.configJEM != '':
        jobParameters += "--jem-config %s " % options.configJEM
# container
if options.containerImage != '':
    jobParameters += "--containerImage {0} ".format(options.containerImage)
# set task param
if jobParameters != '':
    taskParamMap['jobParameters'] += [ 
        {'type':'constant',
         'value': jobParameters,
         },
        ]

# force stage-in
if options.forceStaged or options.forceStagedSecondary:
    taskParamMap['useLocalIO'] = 1

# build step
if options.noBuild and not options.noCompile:
    pass
else:
    jobParameters = '-i ${IN} -o ${OUT} --sourceURL ${SURL} '
    jobParameters += '-r {0} '.format(runDir)
    # exec
    if options.bexec != '':
	jobParameters += '--bexec "{0}" '.format(urllib.quote(options.bexec))
    # use Athena packages
    if options.useAthenaPackages:
        jobParameters += "--useAthenaPackages "
    # use RootCore
    if options.useRootCore:
        jobParameters += "--useRootCore "
    # no compile
    if options.noCompile:
        jobParameters += "--noCompile "
    # use CMake
    if AthenaUtils.useCMake():
        jobParameters += "--useCMake "
    # use mana
    if options.useMana:
        jobParameters += "--useMana "
	if options.manaVer != '':
	    jobParameters += "--manaVer %s " % options.manaVer
    # root
    if options.rootVer != '':
        jobParameters += "--rootVer %s " % options.rootVer
    # cmt config
    if not options.cmtConfig in ['','NULL',None]:
        jobParameters += "--cmtConfig %s " % options.cmtConfig
    # debug parameters
    if options.queueData != '':
        jobParameters += "--overwriteQueuedata=%s " % options.queueData
    # container
    if options.containerImage != '':
        jobParameters += "--containerImage {0} ".format(options.containerImage)
    # set task param
    taskParamMap['buildSpec'] = {
        'prodSourceLabel':'panda',
        'archiveName':archiveName,
        'jobParameters':jobParameters,
        }
    if options.prodSourceLabel != '':
         taskParamMap['buildSpec']['prodSourceLabel'] = options.prodSourceLabel

# preprocessing step

# good run list
if options.goodRunListXML != '':
    jobParameters = "--goodRunListXML {0} ".format(options.goodRunListXML)
    if options.goodRunDataType != '':
        jobParameters += "--goodRunListDataType {0} ".format(options.goodRunDataType)
    if options.goodRunProdStep != '':
        jobParameters += "--goodRunListProdStep {0} ".format(options.goodRunProdStep)
    if options.goodRunListDS != '':
        jobParameters += "--goodRunListDS {0} ".format(options.goodRunListDS)
    jobParameters += "--sourceURL ${SURL} "
    # set task param
    taskParamMap['preproSpec'] = {
        'prodSourceLabel':'panda',
        'jobParameters':jobParameters,
        }
    if options.prodSourceLabel != '':
         taskParamMap['preproSpec']['prodSourceLabel'] = options.prodSourceLabel


# merging
if options.mergeOutput:
    jobParameters = '-r {0} '.format(runDir)
    if options.mergeScript != '':
        jobParameters += '-j "{0}" '.format(options.mergeScript)
    if options.rootVer != '':
        jobParameters += "--rootVer %s " % options.rootVer
    if options.useAthenaPackages:
        jobParameters += "--useAthenaPackages "
    if AthenaUtils.useCMake():
        jobParameters += "--useCMake "
    if options.useRootCore:
        jobParameters += "--useRootCore "
    if options.containerImage != '':
        jobParameters += "--containerImage {0} ".format(options.containerImage)
    else:
        if not (options.noBuild and not options.noCompile):
            jobParameters += '-l ${LIB} '
        else:
            jobParameters += '-a {0} '.format(archiveName)
            jobParameters += "--sourceURL ${SURL} "
    jobParameters += '${TRN_OUTPUT:OUTPUT} ${TRN_LOG:LOG}'
    taskParamMap['mergeSpec'] = {}
    taskParamMap['mergeSpec']['useLocalIO'] = 1
    taskParamMap['mergeSpec']['jobParameters'] = jobParameters
    taskParamMap['mergeOutput'] = True
    if options.nGBPerMergeJob > 0:
        taskParamMap['nGBPerMergeJob'] = options.nGBPerMergeJob


#####################################################################
# submission

exitCode = 0
dumpList = []
if not options.noSubmit:
    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,
                                          Client.PandaClouds[options.cloud]['pilotowners'],
                                          options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)

# submit task
for iSubmission, ioItem in enumerate(ioList):
    if options.verbose:
        print "== parameters =="
        print "Site       : %s" % options.site
        print "Athena     : %s" % athenaVer
        if groupArea != '':
            print "Group Area : %s" % groupArea
        if cacheVer != '':
            print "Cache      : %s" % cacheVer[1:]
        if nightVer != '':
            print "Nightly    : %s" % nightVer[1:]
        print "RunDir     : %s" % runDir
        print "exec       : %s" % options.jobParams

    if len(ioList) == 1:
        newTaskParamMap = taskParamMap
    else:
        # replace input and output
        options.inDS = ioItem['inDS']
        options.outDS = ioItem['outDS']
        newTaskParamMap = PsubUtils.replaceInputOutput(taskParamMap, ioItem['inDS'],
                                                       ioItem['outDS'], iSubmission)
    exitCode = 0
    tmpStr = ''
    taskID = None
    # check outDS format
    if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official,nickName,
                                    options.site,vomsFQAN,options.mergeOutput):
        tmpStr = "invalid output datasetname:%s" % options.outDS
        tmpLog.error(tmpStr)
        exitCode = EC_Config
    # check task parameters
    if exitCode == 0:
        exitCode, tmpStr = PsubUtils.checkTaskParam(newTaskParamMap, options.unlimitNumOutputs)
    if exitCode != 0 and len(ioList) == 1:
        sys.exit(exitCode)
    if options.noSubmit:
        if options.verbose:
            tmpLog.debug("==== taskParams ====")
            tmpKeys = newTaskParamMap.keys()
            tmpKeys.sort() 
            for tmpKey in tmpKeys:
                print '%s : %s' % (tmpKey, newTaskParamMap[tmpKey])
    if not options.noSubmit and exitCode == 0:
        tmpLog.info("submit {0}".format(options.outDS))
        status,tmpOut = Client.insertTaskParams(newTaskParamMap, options.verbose, True)
        # result
        if status != 0:
            tmpStr = "task submission failed with {0}".format(status)
            tmpLog.error(tmpStr)
            exitCode = EC_Submit
        if tmpOut[0] in [0,3]:
            tmpStr = tmpOut[1]
            tmpLog.info(tmpStr)
            try:
                m = re.search('jediTaskID=(\d+)', tmpStr)
                taskID = int(m.group(1))
            except Exception:
                pass
        else:
            tmpStr = "task submission failed. {0}".format(tmpOut[1])
            tmpLog.error(tmpStr)
            exitCode = EC_Submit 
    dumpItem = copy.deepcopy(vars(options))
    dumpItem['returnCode'] = exitCode
    dumpItem['returnOut'] = tmpStr
    dumpItem['jediTaskID'] = taskID
    if len(ioList) > 1:
        dumpItem['bulkSeqNumber'] = iSubmission
    dumpList.append(dumpItem)

# go back to current dir
os.chdir(curDir)
# dump
if options.dumpJson is not None:
    with open(options.dumpJson, 'w') as f:
        json.dump(dumpList, f)
# succeeded
sys.exit(exitCode)
