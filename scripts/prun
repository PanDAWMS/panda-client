#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import re
import os
import sys
import time
import atexit
import datetime
import optparse
import commands
import urllib
import shelve
import copy

# default cloud/site
defaultCloud = 'US'
defaultSite  = 'AUTO'

# suffix for shadow dataset
suffixShadow = "_shadow"

# error code
EC_Config    = 10
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80
EC_Submit    = 90

usage = """%prog [options]"""


# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--inDS',action='store',dest='inDS',default='',type='string',
                help='Name of an input dataset or dataset container')
optP.add_option('--outDS',action='store',dest='outDS',default='',type='string',
                help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--libDS',action='store',dest='libDS',default='',type='string',
                help='Name of a library dataset')
optP.add_option('--secondaryDSs',action='store',dest='secondaryDSs',default='',type='string',
                help='List of secondary datasets when the job requires multiple inputs. See PandaRun wiki page for detail')
optP.add_option('--site',action='store',dest='site',default=defaultSite,type='string',
                help='Site name where jobs are sent (default:%s' % defaultSite)
optP.add_option('--cloud',action='store',dest='cloud',default=None,type='string',
                help='Cloud where jobs are submitted (default:%s)' % defaultCloud)
optP.add_option('--match',action='store',dest='match',default='',type='string',
                help='Use only files matching with given pattern')
optP.add_option('--official',action='store_const',const=True,dest='official',default=False,
                help='Produce official dataset')
optP.add_option('--nFiles',action='store',dest='nFiles',default=0,type='int',
                help='Use a limited number of files in the input dataset')
optP.add_option('--nSkipFiles',action='store',dest='nSkipFiles',default=0,type='int',
                help='Skip N files in the input dataset')
optP.add_option('--nFilesPerJob',action='store',dest='nFilesPerJob',default=None,type='int',
                help='Number of files on which each sub-job runs (default 20)')
optP.add_option('--nJobs',action='store',dest='nJobs',default=-1,type='int',
                help='Maximum number of sub-jobs. If the number of input files (N_in) is less than nJobs*nFilesPerJob, only N_in/nFilesPerJob sub-jobs will be instantiated')
optP.add_option('--maxFileSize',action='store',dest='maxFileSize',default=1024*1024,type='int',
                help='Maximum size of files to be sent to WNs (default 1024*1024B)')
optP.add_option('--athenaTag',action='store',dest='athenaTag',default='',type='string',
                help='Tags to setup Athena on remote WNs, e.g., --athenaTag=AtlasProduction,14.2.24.3')
optP.add_option('--workDir',action='store',dest='workDir',default='.',type='string',
                help='All files under WORKDIR will be transfered to WNs (default=./)')
optP.add_option('--extFile',action='store',dest='extFile',default='',
                help='root or large files under WORKDIR are not sent to WNs by default. If you want to send some skipped files, specify their names, e.g., data.root,data.tgz')
optP.add_option('--inputFileList', action='store', dest='inputFileListName', default='',
                type='string', help='name of file which contains a list of files to be run in the input dataset')
optP.add_option('--crossSite',action='store',dest='crossSite',default=3,
                type='int',help='submit jobs to N sites at most when datasets in container split over many sites (N=3 by default)')
optP.add_option('--outputs',action='store',dest='outputs',default='',type='string',
                help='Names of output files. Comma separated. e.g., --outputs out1.dat,out2.txt')
optP.add_option('--excludedSite', action='store', dest='excludedSite',  default='',
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_option('--noSubmit',action='store_const',const=True,dest='nosubmit',default=False,
                help="Don't submit jobs")
optP.add_option('--processingType', action='store', dest='processingType',  default='prun',
                help="set processingType")
optP.add_option('--seriesLabel', action='store', dest='seriesLabel',  default='',
                help="set seriesLabel")
optP.add_option('--workingGroup', action='store', dest='workingGroup',  default=None,
                help="set workingGroup")
optP.add_option('--tmpDir',action='store',dest='tmpDir',default='',type='string',
                help='Temporary directory where an archive file is created')
optP.add_option('--voms', action='store', dest='vomsRoles',  default=None, type='string',
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_option('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_option('--spaceToken',action='store',dest='spaceToken',default='',type='string',
                help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--devSrv',action='store_const',const=True,dest='devSrv',default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--exec',action='store',dest='jobParams',default='',type='string',
                help='execution string. e.g., --exec "./myscript arg1 arg2"')
optP.add_option('--bexec',action='store',dest='bexec',default='',type='string',
                help='execution string for build stage. e.g., --bexec "make"')
optP.add_option('--myproxy',action='store',dest='myproxy',default='pandaprx.usatlas.bnl.gov',type='string',
                help='Name of the myproxy server')
optP.add_option('-v',action='store_const',const=True,dest='verbose',default=False,
                help='Verbose')
# internal parameters
optP.add_option('--panda_srvURL', action='store', dest='panda_srvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_srcName', action='store', dest='panda_srcName', default='',
                type='string', help='internal parameter')

# parse options
options,args = optP.parse_args()
if options.verbose:
    print options

# display version
if options.version:
    from pandatools import PandaToolsPkgInfo
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# set dummy CMTSITE
if not os.environ.has_key('CMTSITE'):
    os.environ['CMTSITE'] = ''

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# use dev server
if options.devSrv:
    Client.useDevServer()
    
# set server
if options.panda_srvURL != '':
    Client.setServer(options.panda_srvURL)

# version check
PsubUtils.checkPandaClientVer(options.verbose)

# exclude sites
if options.excludedSite != '':
    Client.excludeSite(options.excludedSite)

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0

# libDS
libds_file = '%s/libds_prun.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS
    
# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# add allowed sites
if not siteSpecified:
    tmpSt = Client.addAllowedSites(options.verbose)
    if not tmpSt:
        tmpLog.error("Failed to get allowed site list")
        sys.exit(EC_Config)

# save current dir
curDir = os.path.realpath(os.getcwd())

# check working dir
options.workDir = os.path.realpath(options.workDir)
if options.workDir != curDir and (not curDir.startswith(options.workDir+'/')):
    tmpLog.error("you need to run prun in a directory under %s" % options.workDir)
    sys.exit(EC_Config)

# run dir
runDir = '.'
if curDir != options.workDir:
    # remove special characters
    wDirString=re.sub('[\+]','.',options.workDir)
    runDir = re.sub('^'+wDirString+'/','',curDir)

# parse tag
athenaVer = ''
cacheVer  = ''
nightVer  = ''
if options.athenaTag != '':
    items = options.athenaTag.split(',')
    for item in items:
        # releases
        match = re.search('^(\d+\.\d+\.\d+)',item)
        if match != None:
            athenaVer = 'Atlas-%s' % match.group(1)
            # cache
	    cmatch = re.search('^(\d+\.\d+\.\d+\.\d+)$',item)
	    if cmatch != None:
		cacheVer += '_%s' % cmatch.group(1)
        # project
        if item.startswith('Atlas'):
            # ignore AtlasOffline
            if item in ['AtlasOffline']:
                continue
            cacheVer = '-'+item+cacheVer
        # nightlies    
        if item.startswith('rel_'):
            if 'dev' in items:
                nightVer  = '/dev'
            elif 'bugfix' in items:
                nightVer  = '/bugfix'
            else:
                tmpLog.error("unsupported nightly %s" % line)
                sys.exit(EC_Config)
    # check cache
    if re.search('^-.+_.+$',cacheVer) == None:
        cacheVer = ''


# additinal files
if options.extFile == '':
    options.extFile = []
else:
    tmpItems = options.extFile.split(',')
    options.extFile = []
    # convert * to .*
    for tmpItem in tmpItems:
        options.extFile.append(tmpItem.replace('*','.*'))


# LFN matching
if options.match != '':
    # convert * to .*
    options.match = options.match.replace('*','.*')

# get job script
jobScript = ''
if options.jobParams == '':
    tmpLog.error("you need to give --exec\n  prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'")
    sys.exit(EC_Config)


# check output dataset
if options.outDS == '':
    tmpLog.error("no outDS is given\n  prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'")
    sys.exit(EC_Config)

# secondary datasets
if options.secondaryDSs != '':
    if options.inDS == '':
        tmpLog.error("Primary input dataset is not defined using --inDS although --secondaryDSs is set")
        sys.exit(EC_Config)
    # parse
    tmpMap = {}
    for tmpItem in options.secondaryDSs.split(','):
        tmpItems = tmpItem.split(':')
        if len(tmpItems) in [3,4,5]:
            tmpMap[tmpItems[2]] = {'nFiles'     : int(tmpItems[1]),
                                   'streamName' : tmpItems[0],
                                   'pattern'    : '',
                                   'nSkip'      : 0,
                                   'files'      : []}
            # using filtering pattern
            if len(tmpItems) >= 4:
                tmpMap[tmpItems[2]]['pattern'] = tmpItems[3]
            # nSkip
            if len(tmpItems) >= 5:
                tmpMap[tmpItems[2]]['nSkip'] = int(tmpItems[4])
        else:         
            tmpLog.error("Wrong format %s in --secondaryDSs. Must be StreamName:nFilesPerJob:DatasetName[:Pattern[:nSkipFiles]]" \
                         % tmpItem)
            sys.exit(EC_Config)
    # set
    options.secondaryDSs = tmpMap
else:
    options.secondaryDSs = {}

# correct site
if options.site != 'AUTO':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_ATLAS_1"
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        tmpLog.error("unknown siteID:%s" % origSite)
        sys.exit(EC_Config)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

# get DN
distinguishedName = PsubUtils.getDN()
if distinguishedName == '':
    sys.exit(EC_Config)

# check outDS format
if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official):
    tmpLog.error("invalid output datasetname:%s" % options.outDS)
    sys.exit(EC_Config)

# check if output dataset already exists
outputDSexist = False
outputContExist = False
tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
if len(tmpDatasets) != 0:
    if original_outDS_Name.endswith('/'):
        outputContExist = True
    else:
        outputDSexist = True

# check if shadow dataset exists
shadowDSexist = False
tmpDatasets = Client.getDatasets("%s%s" % (options.outDS,suffixShadow),options.verbose)
if len(tmpDatasets) != 0:
    shadowDSexist = True

# set site/cloud if output/lib dataset already exists
if outputDSexist:
    if options.verbose:
        tmpLog.debug("get locations for outDS:%s" % options.outDS)
    # get locations for outDS    
    outDSlocations = Client.getLocations(options.outDS,[],'',True,options.verbose)
    if outDSlocations == []:
        tmpLog.error("cannot find locations for existing output datasete:%s" % options.outDS)
        sys.exit(EC_Config)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for outDSlocation in outDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(outDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            tmpLog.info("set site:%s cloud:%s because outDS:%s already exists at %s" % \
                        (options.site,options.cloud,options.outDS,outDSlocations))                
            break
    # not found
    if not fFlag:
        tmpLog.error("cannot find supported sites for existing output datasete:%s" % options.outDS)
        sys.exit(EC_Config)
elif options.libDS != '':
    if options.verbose:
        tmpLog.debug("get locations for libDS:%s" % options.libDS)
    # get locations for libDS    
    libDSlocations = Client.getLocations(options.libDS,[],'',True,options.verbose)
    if libDSlocations == []:
        tmpLog.error("cannot find locations for existing lib datasete:%s" % options.libDS)
        sys.exit(EC_Config)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for libDSlocation in libDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(libDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            tmpLog.info("set site:%s cloud:%s because libDS:%s exists at %s" % \
                        (options.site,options.cloud,options.libDS,libDSlocations))                
            break
    # not found
    if not fFlag:
        tmpLog.error("cannot find supported sites for existing lib datasete:%s" % options.libDS)
        sys.exit(EC_Config)

        
# set cloud for brokerage
expCloudFlag = False
if options.cloud == None:
    options.cloud = PsubUtils.getCloudUsingFQAN(defaultCloud,options.verbose)
else:
    # use cloud explicitly    
    expCloudFlag = True
if not PsubUtils.checkValidCloud(options.cloud):
    tmpLog.error("unsupported cloud:%s" % options.cloud)
    sys.exit(EC_Config)


# get files in input dataset
if options.inDS != '':
    # query files in shadow dataset
    shadowList = []
    if shadowDSexist:
	for tmpItem in Client.queryFilesInDataset("%s%s" % (options.outDS,suffixShadow),options.verbose):
            shadowList.append(tmpItem)
        # query files in PandaDB
        tmpShadowList = Client.getFilesInUseForAnal(options.outDS,options.verbose)
        for tmpItem in tmpShadowList:
            if not tmpItem in shadowList:
                shadowList.append(tmpItem)
    elif outputContExist:
	shadowList = Client.getFilesInShadowDataset(options.outDS,suffixShadow,options.verbose)
    # query files in dataset
    inputFileMap = Client.getFilesInDatasetWithFilter(options.inDS,options.match,shadowList,options.inputFileListName,options.verbose)
    # sort
    inputFileList = inputFileMap.keys()
    inputFileList.sort()
    # query files in secondary datasets
    for tmpDsName,tmpValMap in options.secondaryDSs.iteritems():
        tmpSecFileMap = Client.getFilesInDatasetWithFilter(tmpDsName,tmpValMap['pattern'],shadowList,'',options.verbose)
        # sort
        tmpSecFileList = tmpSecFileMap.keys()
        tmpSecFileList.sort()
        # append
        for tmpSecFile in tmpSecFileList:
            tmpSecFileMap[tmpSecFile]['LFN'] = tmpSecFile
            options.secondaryDSs[tmpDsName]['files'].append(tmpSecFileMap[tmpSecFile])
        # skip files    
        options.secondaryDSs[tmpDsName]['files'] = options.secondaryDSs[tmpDsName]['files'][options.secondaryDSs[tmpDsName]['nSkip']:]
    # get locations
    if inputFileList != []:
        if options.site == "AUTO":
            dsLocationMap = Client.getLocations(options.inDS,inputFileList,options.cloud,False,
                                                options.verbose,expCloud=expCloudFlag)
            # no location
            if dsLocationMap == {}:
                if expCloudFlag:
                    tmpLog.error("could not find supported/online locations in the %s cloud for %s" % (options.cloud,options.inDS))
                else:
                    tmpLog.error("could not find supported/online locations for %s" % options.inDS)
                sys.exit(EC_Dataset)
            # run brokerage
            tmpSites = []
            for tmpItem in dsLocationMap.values():
                tmpSites += tmpItem
            status,out = Client.runBrokerage(tmpSites,athenaVer,verbose=options.verbose,trustIS=True)
            if status != 0:
                tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)
                sys.exit(EC_Config)
            if out.startswith('ERROR :'):
		tmpLog.error('brokerage failed')
		print out
		sys.exit(EC_Config)                
            if not Client.PandaSites.has_key(out):
                tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
                sys.exit(EC_Config)
            # set site/cloud
            options.site  = out
            options.cloud = Client.PandaSites[options.site]['cloud']
        # scan local replica catalog
        dsLocation = Client.PandaSites[options.site]['ddm']
        if Client.getLFC(dsLocation) != None:
	    tmpLog.info("scanning LFC %s for %s" % (Client.getLFC(dsLocation),options.site))
            # LFC
            if options.nFiles == 0 and options.nSkipFiles != 0:
                missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose)
            else:
                missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose,
                                                     options.nFiles+options.nSkipFiles)                
        elif Client.getLRC(dsLocation) != None:
	    tmpLog.info("scanning LRC %s for %s" % (Client.getLRC(dsLocation),options.site))
            # LRC
            missList = Client.getMissLFNsFromLRC(inputFileMap,Client.getLRC(dsLocation),options.verbose)
        else:
            missList = []
        if options.verbose:
            tmpLog.debug("%s holds %s files" % (options.site,len(inputFileList)-len(missList)))
        # No files available
        if len(inputFileList) == len(missList):
            tmpLog.error("No files available at %s" % options.site)
            sys.exit(EC_Dataset)
        # remove missing files
        newInputFiles = []
        for inputFile in inputFileList:
            if not inputFile in missList:
                newInputFiles.append(inputFile)
        inputFileList = newInputFiles
    # skip files
    inputFileList = inputFileList[options.nSkipFiles:]
    # no available input files
    if inputFileList == []:
        tmpStr  = "all input files had already been used in %s\n" % options.inDS
        tmpStr += "  prun runs on files which were failed or were not used in\n"
        tmpStr += "  previous submissions if it runs with the same inDS and outDS"
        tmpLog.error(tmpStr)
        sys.exit(EC_Dataset)
    # truncate
    if options.nFiles != 0:
        inputFileList = inputFileList[:options.nFiles]
else:
    # set cloud for no input dataset
    if options.site == "AUTO":
        # get sites belonging to a cloud
        tmpSites = []
        for tmpID,spec in Client.PandaSites.iteritems():
            if spec['cloud']==options.cloud and spec['status']=='online':
                # exclude long,xrootd,local queues
                if Client.isExcudedSite(tmpID):
                    continue
                tmpSites.append(tmpID)
        status,out = Client.runBrokerage(tmpSites,athenaVer,verbose=options.verbose,trustIS=True)    
        if status != 0:
            tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)  
            sys.exit(EC_Config)
	if out.startswith('ERROR :'):
	    tmpLog.error('brokerage failed')
	    print out
	    sys.exit(EC_Config)                
        if not Client.PandaSites.has_key(out):
            tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
            sys.exit(EC_Config)
        # set site
        options.site = out


#####################################################################
# archive sources and send it to HTTP-reachable location

# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (curDir,commands.getoutput('uuidgen'))
else:
    tmpDir = '%s/%s' % (options.tmpDir,commands.getoutput('uuidgen'))    
os.makedirs(tmpDir)

# exit action
def _onExit(dir):
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir)

# create archive
if options.panda_srcName != '':
    # reuse src
    tmpLog.info('reuse source files')
    archiveName = options.panda_srcName
    # go to tmp dir
    os.chdir(tmpDir)
else:
    # go to workdir
    os.chdir(options.workDir)

    # gather files under work dir
    if options.libDS == '':
        tmpLog.info("gathering files under %s" % options.workDir)

        # get files in the working dir
        skippedExt   = ['.o','.a','.so']
        skippedFlag  = False
        workDirFiles = []
        for tmpRoot,tmpDirs,tmpFiles in os.walk('.'):
            emptyFlag    = True
            for tmpFile in tmpFiles:
                tmpPath = '%s/%s' % (tmpRoot,tmpFile)
                # get size
                try:
                    size = os.path.getsize(tmpPath)
                except:
                    # skip dead symlink
                    if options.verbose:
                        type,value,traceBack = sys.exc_info()
                        print "  Ignore : %s:%s" % (type,value)
                    continue
                # skipped extension
                isSkippedExt = False
                for tmpExt in skippedExt:
                    if tmpPath.endswith(tmpExt):
                        isSkippedExt = True
                        break
                # check root
                isRoot = False
                if re.search('\.root(\.\d+)*$',tmpPath) != None:
                    isRoot = True
                # extra files
                isExtra = False
                for tmpExt in options.extFile:
                    if re.search(tmpExt+'$',tmpPath) != None:
                        isExtra = True
                        break
                # regular files
                if not isExtra:
                    # skipped extensions
                    if isSkippedExt:
                        print "  skip %s %s" % (str(skippedExt),tmpPath)
                        skippedFlag = True
                        continue
                    # skip root
                    if isRoot:
                        print "  skip root file %s" % tmpPath
                        skippedFlag = True
                        continue
                    # check size
                    if size > options.maxFileSize:
                        print "  skip large file %s:%sB>%sB" % (tmpPath,size,options.maxFileSize)
                        skippedFlag = True
                        continue

                # remove ./
                tmpPath = re.sub('^\./','',tmpPath)
                # append
                workDirFiles.append(tmpPath)
		emptyFlag = False
            # add empty directory		
	    if emptyFlag and tmpDirs==[]:
		tmpPath = re.sub('^\./','',tmpRoot)
                # skip tmpDir
                if tmpPath.split('/')[-1] == tmpDir.split('/')[-1]:
                    continue
                # append
                workDirFiles.append(tmpPath)
        if skippedFlag:
            tmpLog.info("please use --maxFileSize or --extFile if you need to send the skipped files to WNs")

    # create archive
    if options.libDS == '':
        archiveName     = 'sources.%s.tar' % commands.getoutput('uuidgen')
        archiveFullName = "%s/%s" % (tmpDir,archiveName)

        # collect files
        for tmpFile in workDirFiles:
            if os.path.islink(tmpFile):
                status,out = commands.getstatusoutput('tar -rh %s -f %s' % (tmpFile,archiveFullName))
            else:
                status,out = commands.getstatusoutput('tar rf %s %s' % (archiveFullName,tmpFile))
            if options.verbose:
                print tmpFile
            if status != 0 or out != '':
                print out

    # go to tmpdir
    os.chdir(tmpDir)

    # compress
    if options.libDS == '':
        status,out = commands.getstatusoutput('gzip %s' % archiveName)
        archiveName += '.gz'
        if status !=0 or options.verbose:
            print out

        # check archive
        status,out = commands.getstatusoutput('ls -l %s' % archiveName)
        if options.verbose:
            print out
        if status != 0:
            tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'") 
            sys.exit(EC_Archive)

# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec


# max total filesize on each WN
maxTotalSize = long(10*1024*1024*1024)

# set number of files per job
if options.inDS != '' and options.nFilesPerJob == None:
    # count total size for inputs
    totalSize = 0
    for tmpLFN in inputFileList:
        vals = inputFileMap[tmpLFN]
        try:
            totalSize += long(vals['fsize'])
        except:
            pass
    # the number of files per max total
    tmpNSplit,tmpMod = divmod(totalSize,maxTotalSize)
    if tmpMod != 0:
        tmpNSplit += 1
    tmpNFiles,tmpMod = divmod(len(inputFileList),tmpNSplit)
    # set upper limit
    upperLimitOnFiles = 20
    if tmpNFiles > upperLimitOnFiles:
        tmpNFiles = upperLimitOnFiles
    # check again just in case
    iDiv = 0
    subTotal = 0
    for tmpLFN in inputFileList:
        vals =inputFileMap[tmpLFN]
        try:
            subTotal += long(vals['fsize'])
        except:
            pass
        iDiv += 1
        if iDiv >= tmpNFiles:
            # check
            if subTotal > maxTotalSize:
                # decrement
                tmpNFiles -= 1
                break
            # reset
            iDiv = 0
            subTotal = 0
    # set
    options.nFilesPerJob = tmpNFiles
                                                            

# calculate number of jobs
if options.nJobs == -1:
    if options.inDS == '':
        options.nJobs = 1
    else:
        rest = len(inputFileList) % options.nFilesPerJob
        if rest == 0:
            options.nJobs = len(inputFileList) / options.nFilesPerJob
        else:
            options.nJobs = (len(inputFileList)-rest) / options.nFilesPerJob
            options.nJobs += 1


# read jobID
jobDefinitionID = 0
jobid_file = '%s/pjobid.dat' % os.environ['PANDA_CONFIG_ROOT']
if os.path.exists(jobid_file):
    try:
        # read line
        tmpJobIdFile = open(jobid_file)
        tmpID = tmpJobIdFile.readline()
        tmpJobIdFile.close()
        # remove \n
        tmpID = tmpID.replace('\n','')
        # convert to int
        jobDefinitionID = long(tmpID) + 1
    except:
        pass

# check permission
if not Client.checkSiteAccessPermission(options.site,options.workingGroup,options.verbose):
    sys.exit(EC_Config)

if options.verbose:
    print "== parameters =="
    print "Site       : %s" % options.site
    print "Athena     : %s" % athenaVer
    if cacheVer != '':
        print "Cache      : %s" % cacheVer[1:]
    print "RunDir     : %s" % runDir
    print "exec       : %s" % options.jobParams


# full execution string
fullExecString = PsubUtils.convSysArgv()

# job name
jobName = 'prun.%s' % commands.getoutput('uuidgen')

# make jobs
jobList = []

# buildJob
if options.libDS == '': 
    jobB = JobSpec()
    jobB.jobDefinitionID   = jobDefinitionID
    jobB.jobName           = jobName
    jobB.metadata          = fullExecString
    if athenaVer != '':
        jobB.AtlasRelease  = athenaVer
        jobB.homepackage   = 'AnalysisTransforms'+cacheVer+nightVer
    jobB.transformation    = '%s/buildGen-00-00-01' % Client.baseURLSUB
    jobB.destinationDBlock = 'user%s.%s.lib._%s' % (time.strftime('%y',time.gmtime()),distinguishedName,
                                                    time.time())
    jobB.destinationSE     = options.site
    jobB.prodSourceLabel   = 'panda'
    if options.processingType != '':    
        jobB.processingType = options.processingType
    if options.seriesLabel != '':    
        jobB.prodSeriesLabel = options.seriesLabel
    jobB.workingGroup      = options.workingGroup    
    jobB.assignedPriority  = 2000
    jobB.computingSite     = options.site
    jobB.cloud             = Client.PandaSites[options.site]['cloud']
    # lib.tgz
    fileBO = FileSpec()
    fileBO.lfn               = '%s.lib.tgz' % jobB.destinationDBlock
    fileBO.type              = 'output'
    fileBO.dataset           = jobB.destinationDBlock
    fileBO.destinationSE     = jobB.destinationSE
    fileBO.destinationDBlock = jobB.destinationDBlock
    jobB.addFile(fileBO)
    fileBI = FileSpec()
    fileBI.lfn = archiveName
    fileBI.type = 'input'
    jobB.jobParameters     = '-i %s -o %s ' % (fileBI.lfn,fileBO.lfn)
    if options.bexec != '':
        jobB.jobParameters += '--bexec "%s" ' % urllib.quote(options.bexec)
        jobB.jobParameters += '-r %s ' % runDir
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        jobB.jobParameters += "--sourceURL %s " % matchURL.group(1)
    # log    
    file = FileSpec()
    file.lfn               = '%s.log.tgz' % jobB.destinationDBlock
    file.type              = 'log'
    file.dataset           = jobB.destinationDBlock
    file.destinationSE     = jobB.destinationSE
    file.destinationDBlock = jobB.destinationDBlock
    jobB.addFile(file)
    # set space token
    for file in jobB.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    if options.verbose:    
        tmpLog.debug(jobB.jobParameters)
    jobList.append(jobB)
else:
    # query files in lib dataset to reuse libraries
    tmpLog.info("query files in libDS:%s" % options.libDS)
    tmpList = Client.queryFilesInDataset(options.libDS,options.verbose)
    tmpFileList = []
    tmpGUIDmap = {}
    for fileName in tmpList.keys():
        # ignore log file
        if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
            continue
        tmpFileList.append(fileName)
        tmpGUIDmap[fileName] = tmpList[fileName]['guid'] 
    # incomplete libDS
    if tmpFileList == []:
        # query files in dataset from Panda
	status,tmpMap = Client.queryLastFilesInDataset([options.libDS],options.verbose)
        # look for lib.tgz
	for fileName in tmpMap[options.libDS]:
            # ignore log file
            if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
                continue
            tmpFileList.append(fileName)
            tmpGUIDmap[fileName] = None
    # incomplete libDS
    if tmpFileList == []:
        tmpLog.error("dataset %s is empty" % (options.libDS))
        sys.exit(EC_Dataset)
    # check file list                
    if len(tmpFileList) != 1:
        tmpLog.error("dataset %s contains multiple lib files %s" % (options.libDS,tmpFileList))
        sys.exit(EC_Dataset)
    # instantiate FileSpec
    fileBO = FileSpec()
    fileBO.lfn = tmpFileList[0]
    fileBO.GUID = tmpGUIDmap[fileBO.lfn]
    fileBO.dataset = options.libDS
    fileBO.destinationDBlock = options.libDS
    fileBO.status = 'ready'


# modify outDS name when container is used for output
if original_outDS_Name.endswith('/'):
    options.outDS = re.sub('/$','.%s' % options.site,options.outDS)
    # check outDS
    tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
    if len(tmpDatasets) != 0:
        outputDSexist = True

# get files in existing output dataset
existingFilesInOutDS = {}
if options.outputs != '' and outputDSexist:
    # query files in dataset from DDM
    tmpLog.info("query files in %s" % options.outDS)
    existingFilesInOutDS = Client.queryFilesInDataset(options.outDS,options.verbose)
    # query files in dataset from Panda
    status,tmpMap = Client.queryLastFilesInDataset([options.outDS],options.verbose)
    for tmpLFN in tmpMap[options.outDS]:
        if not tmpLFN in existingFilesInOutDS:
            existingFilesInOutDS[tmpLFN] = None

if options.inDS != '':
    tmpLog.info("%s files are missing or skipped at %s" % (len(missList),options.site))
    tmpLog.info("use %s files" % len(inputFileList)) 

# disp datasetname to identify inputs
commonDispName = 'user_disp.%s' % commands.getoutput('uuidgen')

# runJobs
indexOffsetMap = {}
for iJob in range(options.nJobs):
    jobR = JobSpec()
    jobR.jobDefinitionID   = jobDefinitionID
    jobR.jobName           = jobName
    jobR.metadata          = fullExecString
    if athenaVer != '':
        jobR.AtlasRelease  = athenaVer
        jobR.homepackage   = 'AnalysisTransforms'+cacheVer+nightVer
    jobR.transformation    = '%s/runGen-00-00-02' % Client.baseURLSUB
    jobR.destinationDBlock = options.outDS
    jobR.destinationSE     = options.site
    jobR.prodSourceLabel   = 'user'
    if options.processingType != '':
        jobR.processingType = options.processingType
    if options.seriesLabel != '':    
        jobR.prodSeriesLabel = options.seriesLabel
    jobR.workingGroup      = options.workingGroup    
    jobR.assignedPriority  = 1000
    jobR.computingSite     = options.site
    jobR.cloud             = Client.PandaSites[options.site]['cloud']
    jobR.jobParameters     = '-j "%s" ' % jobScript
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        jobR.jobParameters += "--sourceURL %s " % matchURL.group(1)
    if options.jobParams != '':
        # random seed
        tmpJobO = options.jobParams
        for tmpItem in tmpJobO.split():
            match = re.search('%RNDM=(.+)$',tmpItem)
            if match:
                tmpRndmNum = int(match.group(1)) + iJob
                # replace parameters
                tmpJobO = tmpJobO.replace(match.group(0),'%s' % tmpRndmNum)
        jobR.jobParameters += '-p "%s" ' % urllib.quote(tmpJobO)
    # source files
    fileS = FileSpec()
    fileS.lfn            = fileBO.lfn
    fileS.GUID           = fileBO.GUID
    fileS.type           = 'input'
    fileS.status         = fileBO.status
    fileS.dataset        = fileBO.destinationDBlock
    fileS.dispatchDBlock = fileBO.destinationDBlock
    jobR.addFile(fileS)
    jobR.jobParameters  += '-l %s ' % fileS.lfn    
    # input files
    if options.inDS != '':
        totalSize = 0
        indexIn = iJob*options.nFilesPerJob
        inList = inputFileList[indexIn:indexIn+options.nFilesPerJob]
        if inList == []:
            break
        for tmpLFN in inList:
            vals = inputFileMap[tmpLFN]
            fileI = FileSpec()
            fileI.lfn        = tmpLFN
            fileI.GUID       = vals['guid']
	    fileI.fsize      = vals['fsize']
	    fileI.md5sum     = vals['md5sum']
            fileI.dataset    = options.inDS
            fileI.prodDBlock = options.inDS
            fileI.type       = 'input'
            fileI.status     = 'ready'
            fileI.dispatchDBlock = commonDispName
            jobR.addFile(fileI)
            try:
                totalSize += long(fileI.fsize)
            except:
                pass
        tmpInputMap = {'IN': copy.deepcopy(inList)}  
        # secondary datasets
        secMap = {}
        for tmpSecDS,tmpSecVal in options.secondaryDSs.iteritems():
            tmpIndexSec = iJob*tmpSecVal['nFiles']
            tmpSecList  = tmpSecVal['files'][tmpIndexSec:tmpIndexSec+tmpSecVal['nFiles']]
            # check length
            if len(tmpSecList) < tmpSecVal['nFiles']:
                errStr  = "%s contains %s files which is insufficient for splitting. " % (tmpSecDS,len(tmpSecVal['files']))
                errStr += "nJobs*nFilesPerJob=%s*%s=%s files are required" % (options.nJobs,tmpSecVal['nFiles'],
                                                                              options.nJobs*tmpSecVal['nFiles'])
                tmpLog.error(errStr)            
                sys.exit(EC_Split)
            # add FileSpec
            tmpSecLfnList = []
            for vals in tmpSecList:
                fileI = FileSpec()
                fileI.lfn        = vals['LFN']
                fileI.GUID       = vals['guid']
                fileI.fsize      = vals['fsize']
                fileI.md5sum     = vals['md5sum']
                fileI.dataset    = tmpSecDS
                fileI.prodDBlock = tmpSecDS
                fileI.type       = 'input'
                fileI.status     = 'ready'
                fileI.dispatchDBlock = commonDispName
                jobR.addFile(fileI)
                inList.append(fileI.lfn)
                tmpSecLfnList.append(fileI.lfn)
                try:
                    totalSize += long(fileI.fsize)
                except:
                    pass
            # append to map    
            tmpInputMap[tmpSecVal['streamName']] = tmpSecLfnList
        # size check    
        if totalSize > maxTotalSize:
            errStr  = "A subjob has %s input files and requires %sMB of disk space." \
                  % (len(inList), (totalSize >> 20))
            errStr += "   It must be less than %sMB to avoid overflowing the remote disk." \
                  % (maxTotalSize >> 20)
            errStr += "   Please split the job using --nFilesPerJob."
            tmpLog.error(errStr)            
            sys.exit(EC_Split)
        # set parameter
        jobR.jobParameters += '-i "%s" ' % inList
	if options.secondaryDSs != {}:
	    jobR.jobParameters += '--inMap "%s" ' % tmpInputMap
    # output files
    outMap = {}
    if options.outputs != '':
        for tmpLFN in options.outputs.split(','):
            # set offset to 0 for fresh output dataset
            if existingFilesInOutDS == {}:
                indexOffsetMap[tmpLFN] = 0
            # offset is already obtained
            if indexOffsetMap.has_key(tmpLFN):
                idxOffset = indexOffsetMap[tmpLFN]
                getOffset = False
            else:
                idxOffset = 0
                getOffset = True
            # new LFN
            tmpNewLFN = '%s._%05d.%s' % (jobR.destinationDBlock,idxOffset+iJob+1,tmpLFN)
            # change * to XYZ and add .tgz
	    if tmpNewLFN.find('*') != -1:
		tmpNewLFN = tmpNewLFN.replace('*','XYZ')
		tmpNewLFN = '%s.tgz' % tmpNewLFN
            # get offset
            if getOffset:
                oldHead = '%s._%05d.' % (jobR.destinationDBlock,idxOffset+iJob+1)
                filePatt = tmpNewLFN.replace(oldHead,'%s\._(\d+)\.' % jobR.destinationDBlock)
                idxOffset = PsubUtils.getMaxIndex(existingFilesInOutDS,filePatt)
                # append
                existingFilesInOutDS[tmpLFN] = idxOffset
                # correct
                newHead = '%s._%05d.' % (jobR.destinationDBlock,idxOffset+iJob+1)
                tmpNewLFN = tmpNewLFN.replace(oldHead,newHead)
            # set file spec
            file = FileSpec()
            file.lfn               = tmpNewLFN              
            file.type              = 'output'
            file.dataset           = jobR.destinationDBlock        
            file.destinationSE     = jobR.destinationSE
            file.destinationDBlock = jobR.destinationDBlock
            jobR.addFile(file)
            outMap[tmpLFN] = file.lfn
        # set parameter
        jobR.jobParameters += '-o "%s" ' % str(outMap)
    # log
    file = FileSpec()
    file.lfn               = '%s._$PANDAID.log.tgz' % jobR.destinationDBlock
    file.type              = 'log'
    file.dataset           = jobR.destinationDBlock    
    file.destinationSE     = jobR.destinationSE
    file.destinationDBlock = jobR.destinationDBlock
    jobR.addFile(file)
    jobR.jobParameters += '-r %s ' % runDir
    # set space token
    for file in jobR.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    if options.verbose:    
        tmpLog.debug(jobR.jobParameters)
    jobList.append(jobR)


# no submit 
if not options.nosubmit:

    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,
                                          Client.PandaClouds[options.cloud]['pilotowners'],
                                          options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)

    # upload sources via HTTP POST
    if options.libDS == '': 
        tmpLog.info("upload source files")
        status,out = Client.putFile(archiveName,options.verbose)
        if out != 'True':
            print out
            tmpLog.error("failed to upload source files with %s" % status)
            sys.exit(EC_Post)

    # register output dataset
    if not outputDSexist:
        Client.addDataset(options.outDS,options.verbose)

    # register output dataset container
    if original_outDS_Name.endswith('/'):
        # create
        if not outputContExist:
            Client.createContainer(original_outDS_Name,options.verbose)
        # add dataset
        if not outputDSexist:
            Client.addDatasetsToContainer(original_outDS_Name,[options.outDS],options.verbose)

    # register shadow dataset
    if not outputDSexist:
        Client.addDataset("%s%s" % (options.outDS,suffixShadow),options.verbose)

    # register libDS
    if options.libDS == '':
        Client.addDataset(jobB.destinationDBlock,options.verbose)

    # submit
    tmpLog.info("submit to %s" % options.site)
    status,out = Client.submitJobs(jobList,options.verbose)
    if not Client.PandaSites[options.site]['status'] in ['online','brokeroff']:
	tmpLog.warning("%s is %s. Your jobs will wait until it becomes online" % \
	    (options.site,Client.PandaSites[options.site]['status']))

    # result
    print '==================='
    if out == []:
        tmpLog.error("Job submission was denied")
        sys.exit(EC_Submit)
    outstr   = ''
    buildStr = ''
    runStr   = ''
    for index,o in enumerate(out):
        if o != None:
            if index==0:
                # set JobID
                jobID = o[1]
            if index==0 and options.libDS=='':
                outstr += "  > build\n"
                outstr += "    PandaID=%s\n" % o[0]
                buildStr = '%s' % o[0]            
            elif (index==1 and options.libDS=='') or \
                 (index==0 and options.libDS!=''):
                outstr += "  > run\n"
                outstr += "    PandaID=%s" % o[0]
                runStr = '%s' % o[0]                        
            elif index+1==len(out):
                outstr += "-%s" % o[0]
                runStr += '-%s' % o[0]                                    
    print ' JobID  : %s' % jobID
    print ' Status : %d' % status
    print outstr

    # create dir for DB
    dbdir = os.path.expanduser(os.environ['PANDA_CONFIG_ROOT'])
    if not os.path.exists(dbdir):
        os.makedirs(dbdir)

    # record jobID
    tmpJobIdFile = open(jobid_file,'w')
    tmpJobIdFile.write(str(jobID))
    tmpJobIdFile.close()

    # record libDS
    if options.libDS == '':
        tmpFile = open(libds_file,'w')
        tmpFile.write(jobB.destinationDBlock)
        tmpFile.close()

# go back to current dir
os.chdir(curDir)

# try another site if input files remain
options.crossSite -= 1
if options.crossSite > 0 and options.inDS != '' and not siteSpecified:
    if missList != []:
        anotherTry = True
        # nfiles
        if options.nFiles != 0:
            if options.nFiles > len(inputFileMap):
                fullExecString = re.sub('--nFiles\s*=*\d+',
                                        '--nFiles=%s' % (options.nFiles-len(inputFileMap)),
                                        fullExecString)
            else:
                anotherTry = False
 	# decrement crossSite counter
        fullExecString = re.sub(' --crossSite\s*=*\d+','',fullExecString)
        fullExecString += ' --crossSite=%s' % options.crossSite
        # don't reuse site
        match = re.search('--excludedSite\s*=*\S+',fullExecString)
        if match != None:
            # append site
            fullExecString = re.sub(match.group(0),'%s,%s' % (match.group(0),options.site),fullExecString)
        else:
            # add option
            fullExecString += ' --excludedSite=%s' % options.site
        # set list of input files
        inputTmpfile = '%s/intmp.%s' % (tmpDir,commands.getoutput('uuidgen'))
        iFile = open(inputTmpfile,'w')
        for tmpMiss in missList:
            iFile.write(tmpMiss+'\n')
        iFile.close()
        fullExecString = re.sub(' --inputFileList\s*=*\S+','',fullExecString)
        fullExecString += ' --inputFileList=%s' % inputTmpfile
        # source name
        if not '--panda_srcName' in fullExecString:
            fullExecString += ' --panda_srcName=%s' % archiveName
        # server URL
        if not '--panda_srvURL' in fullExecString:
            fullExecString += ' --panda_srvURL=%s,%s' % (Client.baseURL,Client.baseURLSSL)

        # run prun
        if anotherTry:
            tmpLog.info("trying other sites for the missing files")
            com = 'prun ' + fullExecString
            if options.verbose:
                tmpLog.debug(com)
            status = os.system(com)
            # delete tmp files
            commands.getoutput('\rm -f %s' % inputTmpfile)
            # exit
            sys.exit(status)

# succeeded
sys.exit(0)
