#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import os
import re
import sys
import time
import copy
import atexit
import commands
import optparse
import shelve
import datetime
import urllib
import random
import fcntl
import types
import traceback
import pickle

####################################################################

# error code
EC_Config    = 10
EC_CMT       = 20
EC_Extractor = 30
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80
EC_Submit    = 90

#@ Number of events to skip in the file
nEventsToSkip=0
#@ Events blok counter per file
nSkips =0

# default cloud/site
defaultCloud = None

# suffix for shadow dataset
suffixShadow = "_shadow"


# max size per job
maxTotalSize = long(10*1024*1024*1024)
        

usage = """%prog [options] <jobOption1.py> [<jobOption2.py> [...]]

'%prog --help' prints a summary of the options

  HowTo is available at https://twiki.cern.ch/twiki/bin/view/Atlas/PandaAthena"""


# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
# special options
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--split', action='store', dest='split',  default=-1,
                type='int',    help='Number of sub-jobs to which a job is split')
optP.add_option('--nFilesPerJob', action='store', dest='nFilesPerJob',  default=-1, type='int', help='Number of files on which each sub-job runs')
optP.add_option('--nEventsPerJob', action='store', dest='nEventsPerJob',  default=-1,
                type='int',    help='Number of events on which each sub-job runs')
optP.add_option('--nEventsPerFile', action='store', dest='nEventsPerFile',  default=0,
                type='int',    help='Number of events per file')
optP.add_option('--site', action='store', dest='site',  default="AUTO",
                type='string',    help='Site name where jobs are sent (default:AUTO)')
optP.add_option('--inDS',  action='store', dest='inDS',  default='',
                type='string', help='Name of an input dataset')
optP.add_option('--minDS',  action='store', dest='minDS',  default='',
                type='string', help='Dataset name for minimum bias stream')
optP.add_option('--nMin',  action='store', dest='nMin',  default=-1,
                type='int', help='Number of minimum bias files per one signal file')
optP.add_option('--cavDS',  action='store', dest='cavDS',  default='',
                type='string', help='Dataset name for cavern stream')
optP.add_option('--nCav',  action='store', dest='nCav',  default=-1,
                type='int', help='Number of cavern files per one signal file')
optP.add_option('--libDS', action='store', dest='libDS', default='',
                type='string', help='Name of a library dataset')
optP.add_option('--useCommonHalo', action='store_const', const=False, dest='useCommonHalo',  default=True,
                help="use an integrated DS for BeamHalo")
optP.add_option('--beamHaloDS',  action='store', dest='beamHaloDS',  default='',
                type='string', help='Dataset name for beam halo')
optP.add_option('--beamHaloADS',  action='store', dest='beamHaloADS',  default='',
                type='string', help='Dataset name for beam halo A-side')
optP.add_option('--beamHaloCDS',  action='store', dest='beamHaloCDS',  default='',
                type='string', help='Dataset name for beam halo C-side')
optP.add_option('--nBeamHalo',  action='store', dest='nBeamHalo',  default=-1,
                type='int', help='Number of beam halo files per sub job')
optP.add_option('--nBeamHaloA',  action='store', dest='nBeamHaloA',  default=-1,
                type='int', help='Number of beam halo files for A-side per sub job')
optP.add_option('--nBeamHaloC',  action='store', dest='nBeamHaloC',  default=-1,
                type='int', help='Number of beam halo files for C-side per sub job')
optP.add_option('--useCommonGas', action='store_const', const=False, dest='useCommonGas',  default=True,
                help="use an integrated DS for BeamGas")
optP.add_option('--beamGasDS',  action='store', dest='beamGasDS',  default='',
                type='string', help='Dataset name for beam gas')
optP.add_option('--beamGasHDS',  action='store', dest='beamGasHDS',  default='',
                type='string', help='Dataset name for beam gas Hydrogen')
optP.add_option('--beamGasCDS',  action='store', dest='beamGasCDS',  default='',
                type='string', help='Dataset name for beam gas Carbon')
optP.add_option('--beamGasODS',  action='store', dest='beamGasODS',  default='',
                type='string', help='Dataset name for beam gas Oxygen')
optP.add_option('--nBeamGas',  action='store', dest='nBeamGas',  default=-1,
                type='int', help='Number of beam gas files per sub job')
optP.add_option('--nBeamGasH',  action='store', dest='nBeamGasH',  default=-1,
                type='int', help='Number of beam gas files for Hydrogen per sub job')
optP.add_option('--nBeamGasC',  action='store', dest='nBeamGasC',  default=-1,
                type='int', help='Number of beam gas files for Carbon per sub job')
optP.add_option('--nBeamGasO',  action='store', dest='nBeamGasO',  default=-1,
                type='int', help='Number of beam gas files for Oxygen per sub job')
optP.add_option('--outDS', action='store', dest='outDS', default='',
                type='string', help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--destSE',action='store', dest='destSE',default='',
                type='string', help='Destination strorage element. All outputs go to DESTSE (default :%BNL_ATLAS_2)')
optP.add_option('--nFiles', '--nfiles', action='store', dest='nfiles',  default=0,
                type='int',    help='Use an limited number of files in the input dataset')
optP.add_option('--nSkipFiles', action='store', dest='nSkipFiles',  default=0,
                type='int',    help='Skip N files in the input dataset')
optP.add_option('-v', action='store_const', const=True, dest='verbose',  default=False,
                help='Verbose')
optP.add_option('-l', '--long', action='store_const', const=True, dest='long',  default=False,
                help='Send job to a long queue')
optP.add_option('--blong', action='store_const', const=True, dest='blong',  default=False,
                help='Send build job to a long queue')
optP.add_option('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_option('--cloud',action='store', dest='cloud',default=None,
                type='string', help='cloud where jobs are submitted. default is set according to your VOMS country group')
optP.add_option('--noBuild', action='store_const', const=True, dest='nobuild',  default=False,
                help='Skip buildJob')
optP.add_option('--individualOutDS', action='store_const', const=True, dest='individualOutDS',  default=False,
                help='Create individual output dataset for each data-type. By default, all output files are added to one output dataset')
optP.add_option('--noRandom', action='store_const', const=True, dest='norandom',  default=False,
                help='Enter random seeds manually')
optP.add_option('--memory', action='store', dest='memory',  default=-1,
                type='int',    help='Required memory size')
optP.add_option('--maxCpuCount', action='store', dest='maxCpuCount', default=-1, type='int',
                help='Required CPU count in seconds. Mainly to extend time limit for looping detection')
optP.add_option('--official', action='store_const', const=True, dest='official',  default=False,
                help='Produce official dataset')
optP.add_option('--extFile', action='store', dest='extFile',  default='',
                help='pathena exports files with some special extensions (.C, .dat, .py .xml) in the current directory. If you want to add other files, specify their names, e.g., data1,root,data2.doc')
optP.add_option('--extOutFile', action='store', dest='extOutFile',  default='',
                help='define extra output files, e.g., output1.txt,output2.dat')
optP.add_option('--supStream', action='store', dest='supStream',  default='',
                help='suppress some output streams. e.g., ESD,TAG ')
optP.add_option('--gluePackages', action='store', dest='gluePackages',  default='',
                help='list of glue packages which pathena cannot fine due to empty i686-slc4-gcc34-opt. e.g., External/AtlasHepMC,External/Lhapdf')
optP.add_option('--excludedSite', action='store', dest='excludedSite',  default='',
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_option('--noSubmit', action='store_const', const=True, dest='nosubmit',  default=False,
                help="Don't submit jobs")
optP.add_option('--processingType', action='store', dest='processingType',  default='pathena',
                help="set processingType")
optP.add_option('--seriesLabel', action='store', dest='seriesLabel',  default='',
                help="set seriesLabel")
optP.add_option('--workingGroup', action='store', dest='workingGroup',  default=None,
                help="set workingGroup")
optP.add_option('--generalInput', action='store_const', const=True, dest='generalInput',  default=False,
                help='Read input files with general format except POOL,ROOT,ByteStream')
optP.add_option('--crossSite',action='store',dest='crossSite',default=3,
                type='int',help='submit jobs to N sites at most when datasets in container split over many sites (N=3 by default)')
optP.add_option('--tmpDir', action='store', dest='tmpDir', default='',
                type='string', help='Temporary directory in which an archive file is created')
optP.add_option('--shipInput', action='store_const', const=True, dest='shipinput',  default=False,
                help='Ship input files to remote WNs')
optP.add_option('--noLock', action='store_const', const=True, dest='nolock',  default=False,
                help="Don't create a lock for local database access")
optP.add_option('--fileList', action='store', dest='filelist', default='',
                type='string', help='List of files in the input dataset to be run')
optP.add_option('--myproxy', action='store', dest='myproxy', default='pandaprx.usatlas.bnl.gov',
                type='string', help='Name of the myproxy server')
optP.add_option('--dbRelease', action='store', dest='dbRelease', default='',
                type='string', help='DBRelease or CDRelease (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz')
optP.add_option('--dbRunNumber', action='store', dest='dbRunNumber', default='',
                type='string', help='RunNumber for DBRelease or CDRelease. If this option is used some redundant files are removed to save disk usage when unpacking DBRelease tarball. e.g., 0091890')
optP.add_option('--addPoolFC', action='store', dest='addPoolFC',  default='',
                help="file names to be inserted into PoolFileCatalog.xml except input files. e.g., MyCalib1.root,MyGeom2.root") 
optP.add_option('--skipScan', action='store_const', const=True, dest='skipScan', default=False,
                help='Skip LRC/LFC lookup at job submission')
optP.add_option('--inputFileList', action='store', dest='inputFileList', default='',
                type='string', help='name of file which contains a list of files to be run in the input dataset')
optP.add_option('--removeFileList', action='store', dest='removeFileList', default='',
                type='string', help='name of file which contains a list of files to be removed from the input dataset')
optP.add_option('--corCheck', action='store_const', const=True, dest='corCheck',  default=False,
                help='Enable a checker to skip corrupted files')
optP.add_option('--prestage', action='store_const', const=True, dest='prestage',  default=False,
                help='EXPERIMENTAL : Enable prestager. Make sure that you are authorized')
optP.add_option('--voms', action='store', dest='vomsRoles',  default=None, type='string',
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_option('--useNextEvent', action='store_const', const=True, dest='useNextEvent',  default=False,
                help="Set this option if your jobO uses theApp.nextEvent() e.g. for G4")
optP.add_option('--ara', action='store_const', const=True, dest='ara',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--ares', action='store_const', const=True, dest='ares',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--araOutFile', action='store', dest='araOutFile',  default='',
                help='define output files for ARA, e.g., output1.root,output2.root')
optP.add_option('--trf', action='store', dest='trf',  default=False,
                help='run transformation, e.g. --trf "csc_atlfast_trf.py %IN %OUT.AOD.root %OUT.ntuple.root -1 0"')
optP.add_option('--spaceToken', action='store', dest='spaceToken', default='',
                type='string', help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--notSkipMissing', action='store_const', const=True, dest='notSkipMissing',  default=False,
                help='If input files are not read from SE, they will be skipped by default. This option disables the functionality')
optP.add_option('--burstSubmit', action='store', dest='burstSubmit', default='',
                type='string', help="Please don't use this option. Only for site validation by experts")
optP.add_option('--removeBurstLimit', action='store_const', const=True, dest='removeBurstLimit', default=False,
                help="Please don't use this option. Only for site validation by experts")
optP.add_option('--devSrv', action='store_const', const=True, dest='devSrv',  default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--useAIDA', action='store_const', const=True, dest='useAIDA',  default=False,
                help="use AIDA")
optP.add_option('--inputType', action='store', dest='inputType', default='',
                type='string', help='File type in input dataset which contains multiple file types')
optP.add_option('--mcData', action='store', dest='mcData', default='',
                type='string', help='Create a symlink with linkName to .dat which is contained in input file')
optP.add_option('--pfnList', action='store', dest='pfnList', default='',
                type='string', help='Name of file which contains a list of input PFNs. Those files can be un-registered in DDM')
optP.add_option('--useExperimental', action='store_const', const=True, dest='useExperimental',  default=False,
                help='use experimental features')
# athena options
optP.add_option('-c',action='store',dest='singleLine',type='string',default='',metavar='COMMAND',
                help='One-liner, runs before any jobOs')
optP.add_option('-p',action='store',dest='preConfig',type='string',default='',metavar='BOOTSTRAP',
                help='location of bootstrap file')
# internal parameters
optP.add_option('--panda_srvURL', action='store', dest='panda_srvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_runConfig', action='store', dest='panda_runConfig', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_srcName', action='store', dest='panda_srcName', default='',
                type='string', help='internal parameter')


# parse options
options,args = optP.parse_args()
if options.verbose:
    print options

# display version
if options.version:
    from pandatools import PandaToolsPkgInfo
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# use dev server
if options.devSrv:
    Client.useDevServer()

# set server
if options.panda_srvURL != '':
    Client.setServer(options.panda_srvURL)

# version check
PsubUtils.checkPandaClientVer(options.verbose)

# exclude sites
if options.excludedSite != '':
    Client.excludeSite(options.excludedSite)

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0

# error
if options.outDS == '':
    tmpLog.error("no outDS is given\n pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)
if options.split < -1 :
    tmpLog.error("Number of jobs should be a positive integer")
    sys.exit(EC_Config)
if options.shipinput and options.inDS != '' and options.pfnList != '':
    tmpLog.error("--shipInput, --pfnList and --inDS cannot be used at the same time")
    sys.exit(EC_Config)

# libDS
libds_file = '%s/libds_pathena.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS

# absolute path for PFN list
if options.pfnList != '':
    options.pfnList = os.path.realpath(options.pfnList)

# burst submission
if options.burstSubmit != '':
    # don't scan LRC/LFC
    options.skipScan = True
    # reset cloud/site. They will be overwritten at submission
    options.cloud = None
    options.site  = None
    # disable individual output
    options.individualOutDS = False
    # check libDS stuff
    if options.libDS != '' or options.nobuild:
        tmpLog.error("--libDS or --nobuild cannot be used together with --burstSubmit")
        sys.exit(EC_Config)
        

# split options are mutually exclusive
if (options.nFilesPerJob > 0 and options.nEventsPerJob > 0):
    tmpLog.error("split by files and split by events can not be defined sumaltaneously")
    sys.exit(EC_Config)

# check DBRelease
if options.dbRelease != '' and options.dbRelease.find(':') == -1:
    tmpLog.error("invalid argument for --dbRelease. Must be DatasetName:FileName")  
    sys.exit(EC_Config)
    
# additinal files
options.extFile = options.extFile.split(',')
options.extOutFile = options.extOutFile.split(',')
try:
    options.extOutFile.remove('')
except:
    pass

# glue packages
options.gluePackages = options.gluePackages.split(',')
try:
    options.gluePackages.remove('')
except:
    pass

# set ara on when ares is used
if options.ares:
    options.ara = True

# output files for ARA
if options.ara and options.araOutFile == '':
    tmpLog.error("--araOutFile is needed when ARA (--ara) is used")
    sys.exit(EC_Config)
for tmpName in options.araOutFile.split(','):
    if tmpName != '':
        options.extOutFile.append(tmpName)

# file list
tmpList = options.filelist.split(',')
options.filelist = []
for tmpItem in tmpList:
    if tmpItem == '':
        continue
    # wild card
    tmpItem = tmpItem.replace('*','.*')
    # append
    options.filelist.append(tmpItem) 
# read file list from file
if options.inputFileList != '':
    rFile = open(options.inputFileList)
    for line in rFile:
        line = re.sub('\n','',line)
        options.filelist.append(line)
    rFile.close()

# removed files
if options.removeFileList == '':
    # empty
    options.removeFileList = []
else:
    # read from file
    rList = []
    rFile = open(options.removeFileList)
    for line in rFile:
        line = re.sub('\n','',line)        
        rList.append(line)
    rFile.close()
    options.removeFileList = rList

# file type
options.inputType = options.inputType.split(',')
try:
    options.inputType.remove('')
except:
    pass

# suppressed streams
options.supStream = options.supStream.upper().split(',')
try:
    options.supStream.remove('')
except:
    pass

# set nFilesPerJob for MC data
if options.mcData != '':
    options.nFilesPerJob = 1
    
# set nfiles
if options.nFilesPerJob > 0 and options.nfiles == 0 and options.split > 0:
    options.nfiles = options.nFilesPerJob * options.split

# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# add allowed sites# 
if (not siteSpecified) and options.burstSubmit == '':
    tmpSt = Client.addAllowedSites(options.verbose)
    if not tmpSt:
        tmpLog.error("Failed to get allowed site list")
        sys.exit(EC_Config)

# set cloud according to country FQAN
expCloudFlag = False
if options.cloud == None and options.burstSubmit == '':
    options.cloud = PsubUtils.getCloudUsingFQAN(defaultCloud,options.verbose)
elif options.cloud != None:
    # use cloud explicitly
    expCloudFlag = True

# correct site
if options.site != 'AUTO' and options.burstSubmit == '':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_ATLAS_1"
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        tmpLog.error("unknown siteID:%s" % origSite)
        sys.exit(EC_Config)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

# check cloud
if options.burstSubmit == '':
    foundCloud = False
    for tmpID,spec in Client.PandaSites.iteritems():
        if options.cloud == spec['cloud']:
            foundCloud = True
            break
    if not foundCloud:
        tmpLog.error("unsupported cloud:%s" % options.cloud)
        sys.exit(EC_Config)


# get DN
distinguishedName = PsubUtils.getDN()
if distinguishedName == '':
    sys.exit(EC_Config)

# check outDS format
if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official):
    tmpLog.error("invalid output datasetname:%s" % options.outDS)
    sys.exit(EC_Config)

# save current dir
currentDir = os.path.realpath(os.getcwd())

# get Athena versions
stA,retA = AthenaUtils.getAthenaVer()
# failed
if not stA:
    sys.exit(EC_CMT)
workArea  = retA['workArea'] 
athenaVer = retA['athenaVer'] 
groupArea = retA['groupArea'] 
cacheVer  = retA['cacheVer'] 
nightVer  = retA['nightVer']

# get run directory
# remove special characters                    
sString=re.sub('[\+]','.',workArea)
runDir = re.sub('^%s' % sString, '', currentDir)
if runDir == currentDir:
    tmpLog.error("you need to run pathena in a directory under %s" % workArea)
    sys.exit(EC_Config)
elif runDir == '':
    runDir = '.'
elif runDir.startswith('/'):
    runDir = runDir[1:]
runDir = runDir+'/'

# get job options
jobO = ''
if options.trf:
    # use trf's parameters
    jobO = options.trf
else:
    # get jobOs from command-line
    if options.preConfig != '':
        jobO += '-p %s ' % options.preConfig
    if options.singleLine != '':
        options.singleLine = options.singleLine.replace('"','\'')
        jobO += '-c "%s" ' % options.singleLine
    for arg in args:
        jobO += ' %s' % arg
if jobO == "":
    tmpLog.error("no jobOptions is given\n   pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)

# ARA uses trf I/F
if options.ara:
    if options.ares:
        jobO = "athena.py " + jobO        
    elif jobO.endswith(".C"):
        jobO = "root -l " + jobO
    else:
        jobO = "python " + jobO        
    options.trf = jobO


if options.panda_runConfig == '':
    # extract run configuration    
    tmpLog.info('extracting run configuration')
    # run ConfigExtractor for normal jobO 
    ret,runConfig = AthenaUtils.extractRunConfig(jobO,options.supStream,options.useAIDA,options.shipinput,options.trf)
else:
    # load from file
    ret = True
    tmpRunConfFile = open(options.panda_runConfig)
    runConfig = pickle.load(tmpRunConfFile)
    tmpRunConfFile.close()
if not options.trf:
    # extractor failed
    if not ret:
        sys.exit(EC_Extractor)
    # shipped files
    if runConfig.other.inputFiles:
        for fileName in runConfig.other.inputFiles:
            # append .root for tag files
            if runConfig.other.inColl:
                match = re.search('\.root(\.\d+)*$',fileName)
                if match == None:
                    fileName = '%s.root' % fileName
            # check ship files in the current dir
            if not os.path.exists(fileName):
                tmpLog.error("%s needs exist in the current directory when --shipInput is used" % fileName)
                sys.exit(EC_Extractor)
            # append to extFile
            options.extFile.append(fileName)
            if not runConfig.input.shipFiles:
                runConfig.input['shipFiles'] = []
            runConfig.input['shipFiles'].append(fileName)
    # generator files
    if runConfig.other.rndmGenFile:
        # append to extFile
        for fileName in runConfig.other.rndmGenFile:
            options.extFile.append(fileName)
    # Condition file
    if runConfig.other.condInput:
        # append to extFile
        for fileName in runConfig.other.condInput:
            if options.addPoolFC == "":
                options.addPoolFC = fileName
            else:
                options.addPoolFC += ",%s" % fileName
    # set default ref name
    if not runConfig.input.collRefName:
        runConfig.input.collRefName = 'Token'
    # check dupication in extOutFile
    if runConfig.output.alloutputs != False:
        if options.verbose:
            tmpLog.debug("output files : %s" % str(runConfig.output.alloutputs)) 
        for tmpExtOutFile in tuple(options.extOutFile):
            if tmpExtOutFile in runConfig.output.alloutputs:
                tmpLog.warning("removed %s from extOutFile since it is automatically extracted from Athena. You don't need to specify it in extOutFile"
                            % tmpExtOutFile)
                options.extOutFile.remove(tmpExtOutFile)
else:
    # parse parameters for trf
    oneOut = False
    # replace ; for job sequence
    tmpString = re.sub(';',' ',jobO)
    # look for %OUT
    for tmpItem in tmpString.split():
        match = re.search('\%OUT\.(.+)',tmpItem)
        if match:
            # append basenames to extOutFile
            tmpOutName = match.group(1)
            if not tmpOutName in options.extOutFile:
                options.extOutFile.append(tmpOutName)
                oneOut = True
    # warning if no output
    if not oneOut:
        if not options.ara:
            tmpLog.warning("argument of --trf doesn't contain any %OUT")

# no output jobs
tmpOutKeys = runConfig.output.keys()
for tmpIgnorKey in ['outUserData','alloutputs']:
    try:
        tmpOutKeys.remove(tmpIgnorKey)
    except:
        pass
if tmpOutKeys == [] and options.extOutFile == []:
    errStr  = "No output stream was extracted from jobOs or --trf. "
    if not options.trf:
	errStr += "If your job defines an output without Athena framework "
	errStr += "(e.g., using ROOT.TFile.Open instead of THistSvc) "
	errStr += "please specify the output filename by using --extOutFile. "
	errStr += "Or if you define the output with a relatively new mechanism "
	errStr += "please report it to Savannah to update the automatic extractor" 
    tmpLog.error(errStr)  
    sys.exit(EC_Extractor)

# set extOutFile to runConfig
if options.extOutFile != []:
    runConfig.output['extOutFile'] = options.extOutFile

# check ship files in the current dir
if not runConfig.input.shipFiles:
    runConfig.input.shipFiles = []
for file in runConfig.input.shipFiles:
    if not os.path.exists(file):
        tmpLog.error("%s needs exist in the current directory when using --shipInput" % file)
        sys.exit(EC_Extractor)

# get random number
runConfig.other['rndmNumbers'] = []
if not runConfig.other.rndmStream:
    runConfig.other.rndmStream = []
if len(runConfig.other.rndmStream) != 0:
    if options.norandom:
        print
        print "Initial random seeds need to be defined."
        print "Enter two numbers for each random stream."
        print "  e.g., PYTHIA : 4789899 989240512"
        print
    for stream in runConfig.other.rndmStream:
        if options.norandom:
            # enter manually
            while True:
                randStr = raw_input("%s : " % stream)
                num = randStr.split()
                if len(num) == 2:
                    break
                print " Two numbers are needed"
            runConfig.other.rndmNumbers.append([int(num[0]),int(num[1])])
        else:
            # automatic
            runConfig.other.rndmNumbers.append([random.randint(1,5000000),random.randint(1,5000000)])
    if options.norandom:
        print
if runConfig.other.G4RandomSeeds:
    if options.norandom:
        print
        print "Initial G4 random seeds need to be defined."
        print "Enter one positive number."
        print
        # enter manually
        while True:
            num = raw_input("SimFlags.SeedsG4=")
            try:
                num = int(num)
                if num > 0:
                    runConfig.other.G4RandomSeeds = num
                    break
            except:
                pass
        print    
    else:
        # automatic
        runConfig.other.G4RandomSeeds = random.randint(1,10000)
else:
    # set -1 to disable G4 Random Seeds
    runConfig.other.G4RandomSeeds = -1
    
# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (currentDir,commands.getoutput('uuidgen'))
else:
    tmpDir = '%s/%s' % (options.tmpDir,commands.getoutput('uuidgen'))    
os.makedirs(tmpDir)

# set tmp dir in Client
Client.setGlobalTmpDir(tmpDir)

# exit action
def _onExit(dir):
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir)


#####################################################################
# archive sources and send it to HTTP-reachable location

if options.panda_srcName != '':
    # reuse src
    tmpLog.info('reuse source files')
    archiveName = options.panda_srcName
    # go to tmp dir
    os.chdir(tmpDir)
else:
    # extract jobOs with full pathnames
    for tmpItem in jobO.split():
        if re.search('^/.*\.py$',tmpItem) != None:
            # set random name to avoid overwriting
            tmpName = tmpItem.split('/')[-1]
            tmpName = '%s_%s' % (commands.getoutput('uuidgen'),tmpName)
            # set
            AthenaUtils.fullPathJobOs[tmpItem] = tmpName
            
    # copy some athena specific files
    AthenaUtils.copyAthenaStuff(currentDir)

    # set extFile
    AthenaUtils.setExtFile(options.extFile)

    archiveName = ""
    if options.libDS == '' and not options.nobuild:
        # archive sources
	archiveName,archiveFullName = AthenaUtils.archiveSourceFiles(workArea,runDir,currentDir,tmpDir,
                                                                     options.verbose,options.gluePackages) 
    else:
        # archive jobO
	archiveName,archiveFullName = AthenaUtils.archiveJobOFiles(workArea,runDir,currentDir,
                                                                   tmpDir,options.verbose)

    # archive InstallArea
    if options.libDS == '':
	AthenaUtils.archiveInstallArea(workArea,groupArea,archiveName,archiveFullName,
                                       tmpDir,options.nobuild,options.verbose)

    # back to tmp dir        
    os.chdir(tmpDir)

    # remove some athena specific files
    AthenaUtils.deleteAthenaStuff(currentDir)

    # compress
    status,out = commands.getstatusoutput('gzip %s' % archiveName)
    archiveName += '.gz'
    if status != 0 or options.verbose:
        print out

    # check archive
    status,out = commands.getstatusoutput('ls -l %s' % archiveName)
    if status != 0:
        print out
        tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'") 
        sys.exit(EC_Archive)

    # check symlinks
    tmpLog.info("checking symbolic links")
    status,out = commands.getstatusoutput('tar tvfz %s' % archiveName)
    if status != 0:
        tmpLog.error("Failed to expand archive")
        sys.exit(EC_Archive)
    symlinks = []    
    for line in out.split('\n'):
        items = line.split()
        if items[0].startswith('l') and items[-1].startswith('/'):
            symlinks.append(line)
    if symlinks != []:
        tmpStr  = "Found some unresolved symlinks which may cause a problem\n"
        tmpStr += "     See, e.g., http://savannah.cern.ch/bugs/?43885\n"
        tmpStr += "   Please ignore if you believe they are harmless"
        tmpLog.warning(tmpStr)
        for symlink in symlinks:
            print "  %s" % symlink

    # put sources/jobO via HTTP POST
    if not options.nosubmit:
        tmpLog.info("uploading source/jobO files")
        status,out = Client.putFile(archiveName,options.verbose)
        if out != 'True':
            print out
            tmpLog.error("Failed with %s" % status)
            sys.exit(EC_Post)


####################################################################3
# datasets 

# check if output dataset is unique
outputDSexist = False
outputContExist = False
tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
if len(tmpDatasets) != 0:
    if original_outDS_Name.endswith('/'):
        outputContExist = True
    else:
        outputDSexist = True

# check if shadow dataset exists
shadowDSexist = False
tmpDatasets = Client.getDatasets("%s%s" % (options.outDS,suffixShadow),options.verbose)
if len(tmpDatasets) != 0:
    shadowDSexist = True

# set location when outDS or libDS already exists
if outputDSexist:
    if options.verbose:
        tmpLog.debug("get locations for outDS:%s" % options.outDS)
    outDSlocations = Client.getLocations(options.outDS,[],options.cloud,True,options.verbose)
    if outDSlocations == []:
        tmpLog.error("cannot find locations for existing output dataset:%s" % options.outDS)
        sys.exit(EC_Dataset)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for outDSlocation in outDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(outDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            tmpLog.info("set site:%s cloud:%s because outDS:%s already exists at %s" % \
                        (options.site,options.cloud,options.outDS,outDSlocations))                
            break
    # not found
    if not fFlag:
        tmpLog.error("cannot find supported sites for existing output datasete:%s" % options.outDS)
        sys.exit(EC_Dataset)
elif options.libDS != '':
    if options.verbose:
        tmpLog.debug("get locations for libDS:%s" % options.libDS)
    libDSlocations = Client.getLocations(options.libDS,[],options.cloud,True,options.verbose)
    if libDSlocations == []:
        tmpLog.error("cannot find locations for existing lib dataset:%s" % options.libDS)
        sys.exit(EC_Dataset)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for libDSlocation in libDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(libDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            tmpLog.info("set site:%s cloud:%s because libDS:%s exists at %s" % \
                        (options.site,options.cloud,options.libDS,libDSlocations))                
            break
    # not found
    if not fFlag:
        tmpLog.error("cannot find supported sites for existing lib datasete:%s" % options.libDS)
        sys.exit(EC_Dataset)
        

# input datasets    
if options.inDS != '' or options.shipinput or options.pfnList != '':
    if options.inDS != '':
        # query files in shadow dataset
        shadowList = []
        if shadowDSexist:
            for tmpItem in Client.queryFilesInDataset("%s%s" % (options.outDS,suffixShadow),options.verbose):
                shadowList.append(tmpItem)
            # query files in PandaDB
            tmpShadowList = Client.getFilesInUseForAnal(options.outDS,options.verbose)
            for tmpItem in tmpShadowList:
                if not tmpItem in shadowList:
                    shadowList.append(tmpItem)
        elif outputContExist:
            shadowList = Client.getFilesInShadowDataset(options.outDS,suffixShadow,options.verbose)
        # query files in dataset
        tmpLog.info("query files in %s" % options.inDS)
        inputFileMap  = Client.queryFilesInDataset(options.inDS,options.verbose)
        # remove files
        for tmpKey in inputFileMap.keys():
            if tmpKey in options.removeFileList:
                del inputFileMap[tmpKey]
        # remove log, and check matching
        for fileName in inputFileMap.keys():
            # ignore log file
            if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
                del inputFileMap[fileName]
                continue
            # check type
            if options.inputType != []:
                matchType = False
                for tmpType in options.inputType:
                    if tmpType in fileName:
                        matchType = True
                        break
                if not matchType:
                    del inputFileMap[fileName]                    
                    continue
            # filename matching        
            if options.filelist != []:
                # check matching    
                matchFlag = False
                for pattern in options.filelist:
                    if re.search('\*',pattern) != None:
                        # wildcard matching
			if re.search(pattern,fileName) != None:
			    matchFlag = True
			    break
                    else:
                        # normal matching
                        if pattern == fileName:
                            matchFlag =True
                            break
                # doesn't match
                if not matchFlag:
                    del inputFileMap[fileName]
        # get locations when site==AUTO
        if options.site == "AUTO":
            dsLocationMap,dsLocationMapBack = Client.getLocations(options.inDS,inputFileMap,options.cloud,False,options.verbose,
                                                                  expCloud=expCloudFlag,getReserved=True)
	    # no location
            if dsLocationMap == {} and dsLocationMapBack == {}:
                if expCloudFlag:
                    tmpLog.error("could not find supported/online sites in the %s cloud for %s" % (options.cloud,options.inDS))
                else:
                    tmpLog.error("could not find supported/online sites for %s" % options.inDS)
                sys.exit(EC_Dataset)
            # run brorage
            tmpDsLocationMapList = [dsLocationMap]
            if dsLocationMapBack != {}:
                tmpDsLocationMapList.append(dsLocationMapBack)
            tmpBrokerErr = ''   
            for idxDsLocationMap,tmpDsLocationMap in enumerate(tmpDsLocationMapList):    
                tmpSites = []
                for tmpItem in tmpDsLocationMap.values():
                    tmpSites += tmpItem
		if tmpSites == []:
                    continue
                status,out = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose,trustIS=True)
                if status != 0:
                    tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)
                    sys.exit(EC_Config)
                if out.startswith('ERROR :'):
		    if idxDsLocationMap == 0:
			tmpBrokerErr += out
			tmpBrokerErr += " Could not find sites in the %s cloud.\n" % options.cloud
		    else:
			if tmpBrokerErr != '':
			    tmpBrokerErr += out
			    tmpBrokerErr += " Could not find sites in other clouds.\n"
			else:
			    tmpBrokerErr += out
			    tmpBrokerErr += " Could not find sites.\n"
                    if idxDsLocationMap+1 >= len(tmpDsLocationMapList):
                        tmpLog.error('brokerage failed')
                        print tmpBrokerErr[:-1]
                        sys.exit(EC_Config)
                    continue    
                if not Client.PandaSites.has_key(out):
                    tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
                    sys.exit(EC_Config)
                break    
	    # set site/cloud
            options.site  = out
            options.cloud = Client.PandaSites[options.site]['cloud']
            # destination
            if options.destSE == '':
                options.destSE = options.site
            if options.verbose:
                tmpLog.debug("chosen site=%s destSE=%s" % (options.site,options.destSE))
        # scan local replica catalog
        if options.skipScan:
            # skip remote scan
            missList = []
        else:
            dsLocation = Client.PandaSites[options.site]['ddm']
            if Client.getLRC(dsLocation) != None:
		tmpLog.info("scanning LFC %s for %s" % (Client.getLRC(dsLocation),options.site))
                # LRC
                missList = Client.getMissLFNsFromLRC(inputFileMap,Client.getLRC(dsLocation),options.verbose,
                                                     options.nfiles+options.nSkipFiles)
            elif Client.getLFC(dsLocation) != None:
		tmpLog.info("scanning LFC %s for %s" % (Client.getLFC(dsLocation),options.site))
                # LFC
                if options.nfiles == 0 and options.nSkipFiles != 0:
                    missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose)
                else:
                    missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose,
                                                         options.nfiles+options.nSkipFiles)
            else:
                missList = []
            # choose min missList
            if options.verbose:
                tmpLog.debug("%s holds %s files" % (dsLocation,len(inputFileMap)-len(missList)))
        # No files available
        if len(inputFileMap) == len(missList):
            tmpLog.error("No files available at %s" % options.site)
            sys.exit(EC_Dataset)
        # remove missing
        for fileName in inputFileMap.keys():
            # missing at the site
            if fileName in missList:
                del inputFileMap[fileName]                    
                continue
        # remove shadow
        for fileName in shadowList:
            if inputFileMap.has_key(fileName):
                del inputFileMap[fileName]
        # no input
        if len(inputFileMap) == 0:
            tmpStr  = "all input files had already been used in %s\n" % options.inDS
            tmpStr += "  pathena runs on files which were failed or were not used in\n"
            tmpStr += "  previous submissions if it runs with the same inDS and outDS"
            tmpLog.error(tmpStr)
            sys.exit(EC_Dataset)
        # make list
        inputFileList = inputFileMap.keys()
        inputFileList.sort()
    elif options.pfnList != '':
        # read PFNs from a file
        rFile = open(options.pfnList)
        for line in rFile:
            line = re.sub('\n','',line)
            inputFileList.append(line)
            inputFileList.sort()
        rFile.close()
    else:
        # ship input files
        devidedByGUID = False 
        # extract GUIDs
        guidCollMap,guidCollList = AthenaUtils.getGUIDfromColl(athenaVer,runConfig.input.shipFiles,
                                                               currentDir,
                                                               runConfig.input.collRefName,
                                                               options.verbose)
        # if works
        if guidCollList != []:
            # use GUIDs for looping
            inputFileList = guidCollList
            # use GUID boundaries
            devidedByGUID = True
        else:
            # use input collections for looping
            inputFileList = runConfig.input.shipFiles
    # skip files
    if options.nSkipFiles > len(inputFileList):
        tmpStr  = "the number of files in %s is less than nSkipFiles\n" % options.inDS
        tmpStr += " N of files=%s : nSkipFiles=%s" % (len(inputFileList),options.nSkipFiles)
        tmpLog.error(tmpStr)
        sys.exit(EC_Dataset)
    inputFileList = inputFileList[options.nSkipFiles:]
    # use limited number of files
    if options.nfiles > 0:
        inputFileList = inputFileList[:options.nfiles]

    # set # of events for shipInput
    if options.shipinput and options.nFilesPerJob == -1 and options.nEventsPerJob == -1:
	# non GUID boundaries
	if not devidedByGUID:
	    options.nEventsPerJob = 2000

    # set # of split
    #@some interesting default behaviour here
    #@ If number of jobs is "undefined" or "not specified by user"
    #@ split dataset in number of chunsk with nFilesPerJob files per job
    #@ if nFilesPerJob are not specified by user then split AOD files list in chunks of 10 files
    #@ split other file types in chunks of 20 files.
    #
    if options.nEventsPerJob != -1:
        if options.nEventsPerFile == 0:
            options.nEventsPerFile = Client.nEvents(options.inDS,options.verbose,(not options.shipinput),inputFileList,currentDir)
	if options.nEventsPerJob > options.nEventsPerFile:
	    tmpDiv,tmpMod = divmod(options.nEventsPerJob,options.nEventsPerFile)
	    options.nFilesPerJob = tmpDiv
	    if tmpMod != 0:
		options.nFilesPerJob += 1
	    options.nEventsPerJob = -1

	    
    if options.split == -1:
        # count total size for inputs
        totalSize = 0 
        for fileName in inputFileList:
            try:
                vals = inputFileMap[fileName]
                totalSize += long(vals['fsize'])
            except:
                pass
        #@ If number of jobs is not defined then....
        #@ For splitting by files case
        if(options.nEventsPerJob == -1):
            if options.nFilesPerJob > 0:
                defaultNFile = options.nFilesPerJob
            else:
                defaultNFile = 20
            tmpNSplit,tmpMod = divmod(len(inputFileList),defaultNFile)
            if tmpMod != 0:
                tmpNSplit += 1
            # check size limit
            if totalSize/tmpNSplit > maxTotalSize:
                # reset to meet the size limit
                tmpNSplit,tmpMod = divmod(totalSize,maxTotalSize)
                if tmpMod != 0:
                    tmpNSplit += 1
                # calculate N files
                divF,modF = divmod(len(inputFileList),tmpNSplit)
		if divF == 0:
		    divF = 1
		# reset tmpNSplit
                tmpNSplit,tmpMod = divmod(len(inputFileList),divF)
                if tmpMod != 0:
                    tmpNSplit += 1
                # check again just in case
                iDiv = 0
                subTotal = 0
                for fileName in inputFileList:
                    vals = inputFileMap[fileName]
                    try:
                        subTotal += long(vals['fsize'])
                    except:
                        pass
                    iDiv += 1
                    if iDiv >= divF:
                        # check
                        if subTotal > maxTotalSize:
                            # recalcurate
                            if divF != 1:
                                divF -= 1
                            tmpNSplit,tmpMod = divmod(len(inputFileList),divF)
                            if tmpMod != 0:
                                tmpNSplit += 1
                            break
			# reset
                        iDiv = 0
                        subTotal = 0
            # set            
            options.split = tmpNSplit
        #@ For splitting by events case
        else:
            #@ split by number of events defined
            defaultNFile=1 #Each job has one input file in this case
            #@ tmpNSplit - number of jobs per file in case of splitting by event number
            tmpNSplit, tmpMod = divmod(options.nEventsPerFile, options.nEventsPerJob)
            if tmpMod != 0:
                tmpNSplit +=1
            #@ Number of Jobs calculated here:
            options.split = tmpNSplit*len(inputFileList)

    # input stream for minimum bias
    if options.trf and jobO.find('%MININ') != -1:
        runConfig.input.inMinBias = True
    if runConfig.input.inMinBias:
        if options.minDS == "":
            # read from stdin   
            print
            print "This job uses Minimum-Bias stream"
            while True:
                minbiasDataset = raw_input("Enter dataset name for Minimum Bias : ")
                minbiasDataset = minbiasDataset.strip()
                if minbiasDataset != "":
                    break
        else:
            minbiasDataset = options.minDS
        # query files in dataset
        tmpLog.info("query files in %s" % minbiasDataset)
        tmpList = Client.queryFilesInDataset(minbiasDataset,options.verbose)
	minbiasList = []
        for item in tmpList.keys():
            # remove log
            if re.search('log\.tgz(\.\d+)*',item) != None:
                continue
            minbiasList.append((item,tmpList[item]))
        # sort
        minbiasList.sort()
        # number of files per one signal
        if options.nMin < 0:
            while True:
                tmpStr = raw_input("Enter the number of Minimum-Bias files per one signal file : ")
                try:
                    options.nMin = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(minbiasList) < options.nMin:
            tmpLog.error("%s contains only %s files which is less than %s" % \
                         (minbiasDataset,len(minbiasList),options.nMin))
            sys.exit(EC_Dataset)

    # input stream for cavern
    if options.trf and jobO.find('%CAVIN') != -1:
        runConfig.input.inCavern = True
    if runConfig.input.inCavern:
        if options.cavDS == "":
            # read from stdin                  
            print
            print "This job uses Cavern stream"
            while True:
                cavernDataset = raw_input("Enter dataset name for Cavern : ")
                cavernDataset = cavernDataset.strip()
                if cavernDataset != "":
                    break
        else:
            cavernDataset = options.cavDS
        # query files in dataset
        tmpLog.info("query files in %s" % cavernDataset)
        tmpList = Client.queryFilesInDataset(cavernDataset,options.verbose)
	cavernList = []
        for item in tmpList.keys():
            # remove log
            if re.search('log\.tgz(\.\d+)*',item) != None:
                continue
            cavernList.append((item,tmpList[item]))
        # sort
        cavernList.sort()
        # number of files per one signal
        if options.nCav < 0:
            while True:
                tmpStr = raw_input("Enter the number of Cavern files per one signal file : ")
                try:
                    options.nCav = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(cavernList) < options.nCav:
            tmpLog.error("%s contains only %s files which is less than %s" % \
                         (cavernDataset,len(cavernList),options.nCav))
            sys.exit(EC_Dataset)
    # input stream for beam halo
    if options.trf and jobO.find('%BHIN') != -1:
        runConfig.input.inBeamHalo = True 
    if runConfig.input.inBeamHalo:
	# use common DS
	if options.useCommonHalo:
	    if options.beamHaloDS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloDataset = raw_input("Enter dataset name for BeamHalo : ")
                    beamHaloDataset = beamHaloDataset.strip()
                    if beamHaloDataset != "":
                        break
            else:
                beamHaloDataset = options.beamHaloDS
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloDataset)
            tmpList = Client.queryFilesInDataset(beamHaloDataset,options.verbose)
	    beamHaloList = []
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamHaloList.append((item,tmpList[item]))
            # sort
            beamHaloList.sort()
            # number of files per one sub job
            if options.nBeamHalo < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamHalo files per one sub job : ")
                    try:
                        options.nBeamHalo = int(tmpStr)
                        break
                    except:
                        pass
            # check # of files
            if len(beamHaloList) < options.nBeamHalo:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloDataset,len(beamHaloList),options.nBeamHalo))
                sys.exit(EC_Dataset)
	else:	
            # get DS for A-side        
            if options.beamHaloADS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloAdataset = raw_input("Enter dataset name for BeamHalo A-side : ")
                    beamHaloAdataset = beamHaloAdataset.strip()
                    if beamHaloAdataset != "":
                        break
            else:
                beamHaloAdataset = options.beamHaloADS
            # get DS for C-side
            if options.beamHaloCDS == "":
                # read from stdin                  
                while True:
                    beamHaloCdataset = raw_input("Enter dataset name for BeamHalo C-side : ")
                    beamHaloCdataset = beamHaloCdataset.strip()
                    if beamHaloCdataset != "":
                        break
            else:
                beamHaloCdataset = options.beamHaloCDS
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloAdataset)
            tmpList = Client.queryFilesInDataset(beamHaloAdataset,options.verbose)
	    beamHaloAList = []
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamHaloAList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamHaloCdataset)
            tmpList = Client.queryFilesInDataset(beamHaloCdataset,options.verbose)
	    beamHaloCList = []
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamHaloCList.append((item,tmpList[item]))
            # sort
            beamHaloAList.sort()
            beamHaloCList.sort()
            # number of files per one sub job
            if options.nBeamHaloA < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamHalo files for A-side per one sub job : ")
                    try:
                        options.nBeamHaloA = int(tmpStr)
                        break
                    except:
                        pass
            if options.nBeamHaloC < 0:
                # use default ratio
                options.nBeamHaloC = int(0.02/1.02*options.nBeamHaloA)
            # check # of files
            if len(beamHaloAList) < options.nBeamHaloA:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloAdataset,len(beamHaloAList),options.nBeamHaloA))
                sys.exit(EC_Dataset)
            if len(beamHaloCList) < options.nBeamHaloC:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloCdataset,len(beamHaloCList),options.nBeamHaloC))
                sys.exit(EC_Dataset)
    # input stream for beam gas
    if options.trf and jobO.find('%BGIN') != -1:
        runConfig.input.inBeamGas = True  
    if runConfig.input.inBeamGas: 
	# use common DS
	if options.useCommonGas:
            # get BeamGas DS
            if options.beamGasDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasDataset = raw_input("Enter dataset name for BeamGas : ")
                    beamGasDataset = beamGasDataset.strip()
                    if beamGasDataset != "":
                        break
            else:
                beamGasDataset = options.beamGasDS
            # query files in dataset
            tmpLog.info("query files in %s" % beamGasDataset)
            tmpList = Client.queryFilesInDataset(beamGasDataset,options.verbose)
	    beamGasList = []
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasList.append((item,tmpList[item]))
            # sort
            beamGasList.sort()
            # number of files per one sub job
            if options.nBeamGas < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamGas files per one sub job : ")
                    try:
                        options.nBeamGas = int(tmpStr)
                        break
                    except:
                        pass
            # check # of files
            if len(beamGasList) < options.nBeamGas:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasDataset,len(beamGasList),options.nBeamGas))
                sys.exit(EC_Dataset)
        else:
            # get DS for H
            if options.beamGasHDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasHdataset = raw_input("Enter dataset name for BeamGas Hydrogen : ")
                    beamGasHdataset = beamGasHdataset.strip()
                    if beamGasHdataset != "":
                        break
            else:
                beamGasHdataset = options.beamGasHDS
            # get DS for C
            if options.beamGasCDS == "":
                # read from stdin                  
                while True:
                    beamGasCdataset = raw_input("Enter dataset name for BeamGas Carbon : ")
                    beamGasCdataset = beamGasCdataset.strip()
                    if beamGasCdataset != "":
                        break
            else:
                beamGasCdataset = options.beamGasCDS
            # get DS for O
            if options.beamGasODS == "":
                # read from stdin                  
                while True:
                    beamGasOdataset = raw_input("Enter dataset name for BeamGas Oxygen : ")
                    beamGasOdataset = beamGasOdataset.strip()
                    if beamGasOdataset != "":
                        break
            else:
                beamGasOdataset = options.beamGasODS
            # query files in dataset
            tmpLog.info("query files in %s" % beamGasHdataset)
            tmpList = Client.queryFilesInDataset(beamGasHdataset,options.verbose)
	    beamGasHList = []
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasHList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasCdataset)
            tmpList = Client.queryFilesInDataset(beamGasCdataset,options.verbose)
	    beamGasCList = []
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasCList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasOdataset)
            tmpList = Client.queryFilesInDataset(beamGasOdataset,options.verbose)
	    beamGasOList = []
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasOList.append((item,tmpList[item]))
            # sort
            beamGasHList.sort()
            beamGasCList.sort()
            beamGasOList.sort()        
            # number of files per one sub job
            if options.nBeamGasH < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamGas files for Hydrogen per one sub job : ")
                    try:
                        options.nBeamGasH = int(tmpStr)
                        break
                    except:
                        pass
            if options.nBeamGasC < 0:
                # use default ratio
                options.nBeamGasC = int(options.nBeamGasH*7/90)
            if options.nBeamGasO < 0:
                # use default ratio
                options.nBeamGasO = int(options.nBeamGasH*3/90)
            # check # of files
            if len(beamGasHList) < options.nBeamGasH:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasHdataset,len(beamGasHList),options.nBeamGasH))
                sys.exit(EC_Dataset)
            if len(beamGasCList) < options.nBeamGasC:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasCdataset,len(beamGasCList),options.nBeamGasC))
                sys.exit(EC_Dataset)
            if len(beamGasOList) < options.nBeamGasO:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasOdataset,len(beamGasOList),options.nBeamGasO))
                sys.exit(EC_Dataset)
else:
    if options.split <= 0:
        options.split = 1

# get DB datasets
dbrFiles  = {}
dbrDsList = []
if options.trf or options.dbRelease != '':
    if options.trf:
        # parse jobO for TRF
        tmpItems = jobO.split()
    else:
        # mimic a trf parameter to reuse following algorithm
        tmpItems = ['%DB='+options.dbRelease]
    # look for DBRelease
    for tmpItem in tmpItems:
        match = re.search('%DB=([^:]+):(.+)$',tmpItem)
        if match:
            tmpDbrDS  = match.group(1)
            tmpDbrLFN = match.group(2)
            # get files in the dataset
            if not tmpDbrDS in dbrDsList:
                tmpLog.info("query files in %s" % tmpDbrDS)
                tmpList = Client.queryFilesInDataset(tmpDbrDS,options.verbose)
                # append
                for tmpLFN,tmpVal in tmpList.iteritems():
                    dbrFiles[tmpLFN] = tmpVal
                dbrDsList.append(tmpDbrDS)
            # check
            if not dbrFiles.has_key(tmpDbrLFN):
                tmpLog.error("%s is not in %s" (tmpDbrLFN,tmpDbrDS))
		sys.exit(EC_Dataset)




# choose site automatically when it is still AUTO
if options.site == "AUTO":
    # get sites belonging to a cloud and others
    tmpPriSites = []
    tmpSecSites = []
    for tmpID,spec in Client.PandaSites.iteritems():
        if spec['status']=='online':
            # exclude long,xrootd,local queues
            if Client.isExcudedSite(tmpID):
                continue
            if spec['cloud']==options.cloud:
                tmpPriSites.append(tmpID)
            elif not expCloudFlag:
                tmpSecSites.append(tmpID)
    tmpSitesList = [tmpPriSites]
    if not expCloudFlag:
        tmpSitesList.append(tmpSecSites)
    # run brokerage
    tmpBrokerErr = ''
    for idxTmpSites,tmpSites in enumerate(tmpSitesList):
        status,out = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose,trustIS=True)    
        if status != 0:
            tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)  
            sys.exit(EC_Config)
        if out.startswith('ERROR :'):
            tmpBrokerErr += out
            if idxTmpSites == 0:
                tmpBrokerErr += " Could not find sites in the %s cloud.\n" % options.cloud
            else:
                tmpBrokerErr += " Could not find sites in other clouds.\n"
            if idxTmpSites+1 >= len(tmpSitesList):
                tmpLog.error('brokerage failed')
                print tmpBrokerErr[:-1]
                sys.exit(EC_Config)
            continue    
        if not Client.PandaSites.has_key(out):
            tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
            sys.exit(EC_Config)
        break    
    # set site
    options.site = out

# long queue
if options.long and not options.site.startswith('ANALY_LONG_'):
    tmpsite = re.sub('ANALY_','ANALY_LONG_',options.site)
    tmpsite = re.sub('_\d+$','',tmpsite)
    # if sitename exists
    if Client.PandaSites.has_key(tmpsite):
        options.site = tmpsite

        
# modify outDS name when container is used for output
if original_outDS_Name.endswith('/'):
    options.outDS = re.sub('/$','.%s' % options.site,options.outDS)
    # check outDS
    tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
    if len(tmpDatasets) != 0:
        outputDSexist = True

# index
indexFiles   = 0
indexCavern  = 0
indexMin     = 0
indexBHalo   = 0
indexBHaloA  = 0
indexBHaloC  = 0
indexBGas    = 0
indexBGasH   = 0
indexBGasC   = 0
indexBGasO   = 0
indexNT      = 0
indexHIST    = 0
indexRDO     = 0
indexESD     = 0
indexAOD     = 0
indexAANT    = 0
indexTAG     = 0
indexTHIST   = 0
indexIROOT   = 0
indexEXT     = 0
indexStream1 = 0
indexStream2 = 0
indexStreamG = 0
indexBS      = 0
indexSelBS   = 0
indexMeta    = 0
indexMS      = 0

# get maximum index
def getIndex(list,pattern):
    maxIndex = 0
    for item in list:
        match = re.match(pattern,item)
        if match != None:
            tmpIndex = int(match.group(1))
            if maxIndex < tmpIndex:
                maxIndex = tmpIndex
    return maxIndex


# get files for individualOutDS
def getFilesWithSuffix(fileMap,suffix):
    tmpDsName = "%s_%s" % (options.outDS,suffix)
    tmpLog.info("query files in %s" % tmpDsName)
    tmpList = Client.queryFilesInDataset(tmpDsName,options.verbose)
    for tmpLFN,tmpVal in tmpList.iteritems():
        if not tmpLFN in fileMap:
            fileMap[tmpLFN] = tmpVal
        

# increase index
if outputDSexist:
    # query files in dataset from DDM
    tmpLog.info("query files in %s" % options.outDS)
    tmpList = Client.queryFilesInDataset(options.outDS,options.verbose)
    # query files in dataset from Panda
    status,tmpMap = Client.queryLastFilesInDataset([options.outDS],options.verbose)
    for tmpLFN in tmpMap[options.outDS]:
        if not tmpLFN in tmpList:
            tmpList[tmpLFN] = None
    # query files for individualOutDS:
    if options.individualOutDS:
        if runConfig.output.outHist:
            getFilesWithSuffix(tmpList,'HIST')
        if runConfig.output.outRDO:
            getFilesWithSuffix(tmpList,'RDO')
        if runConfig.output.outEDS:
            getFilesWithSuffix(tmpList,'ESD')
        if runConfig.output.outAOD:
            getFilesWithSuffix(tmpList,'AOD')
        if runConfig.output.outTAG:
            getFilesWithSuffix(tmpList,'TAG')
        if runConfig.output.outStream1:
            getFilesWithSuffix(tmpList,'Stream1')
        if runConfig.output.outStream2:
            getFilesWithSuffix(tmpList,'Stream2')
        if runConfig.output.outBS:
            getFilesWithSuffix(tmpList,'BS')
        if runConfig.output.outSelBS:    
            getFilesWithSuffix(tmpList,'SelBS')
        if runConfig.output.outNtuple:    
            for sName in runConfig.output.outNtuple:
                getFilesWithSuffix(tmpList,sName)
        if runConfig.output.outTHIST:
            for sName in runConfig.output.outTHIST:
                getFilesWithSuffix(tmpList,sName)
        if runConfig.output.outAANT:
            for sName in runConfig.output.outAANT:
                getFilesWithSuffix(tmpList,sName)
        if runConfig.output.outIROOT:
            for sIndex,sName in enumerate(runConfig.output.outIROOT):
                getFilesWithSuffix(tmpList,'iROOT%s' % sIndex)
        if runConfig.output.extOutFile:
            for sIndex,sName in enumerate(options.extOutFile):
                getFilesWithSuffix(tmpList,'EXT%s' % sIndex)
        if runConfig.output.outStreamG:
            for sName in runConfig.output.outStreamG:
                getFilesWithSuffix(tmpList,sName)                
        if runConfig.output.outMeta:
            iMeta = 0
            for sName,sAsso in runConfig.output.outMeta:
                getFilesWithSuffix(tmpList,'META%s' % iMeta)
                iMeta += 1
        if runConfig.output.outMS:
            for sName,sAsso in runConfig.output.outMS:
                getFilesWithSuffix(tmpList,sName)                                
    # index
    indexHIST    = getIndex(tmpList,"%s\.hist\._(\d+)\.root" % options.outDS)
    indexRDO     = getIndex(tmpList,"%s\.RDO\._(\d+)\.pool\.root" % options.outDS)    
    indexESD     = getIndex(tmpList,"%s\.ESD\._(\d+)\.pool\.root" % options.outDS)
    indexAOD     = getIndex(tmpList,"%s\.AOD\._(\d+)\.pool\.root" % options.outDS)
    indexTAG     = getIndex(tmpList,"%s\.TAG\._(\d+)\.coll\.root" % options.outDS)
    indexStream1 = getIndex(tmpList,"%s\.Stream1\._(\d+)\.pool\.root" % options.outDS)
    indexStream2 = getIndex(tmpList,"%s\.Stream2\._(\d+)\.pool\.root" % options.outDS)
    indexBS      = getIndex(tmpList,"%s\.BS\._(\d+)\.data" % options.outDS)
    if runConfig.output.outSelBS:
        indexSelBS   = getIndex(tmpList,"%s\.%s\._(\d+)\.data" % (options.outDS,runConfig.output.outSelBS))
    if runConfig.output.outNtuple:
        for sName in runConfig.output.outNtuple:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.root" % (options.outDS,sName))
            if tmpIndex > indexNT:
                indexNT  = tmpIndex
    if runConfig.output.outTHIST:            
        for sName in runConfig.output.outTHIST:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.root" % (options.outDS,sName))
            if tmpIndex > indexTHIST:
                indexTHIST  = tmpIndex
    if runConfig.output.outAANT:            
        for aName,sName in runConfig.output.outAANT:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.root" % (options.outDS,sName))
            if tmpIndex > indexAANT:
                indexAANT  = tmpIndex
    if runConfig.output.outIROOT:            
        for sIndex,sName in enumerate(runConfig.output.outIROOT):
            tmpIndex = getIndex(tmpList,"%s\.iROOT%s\._(\d+)\.%s" % (options.outDS,sIndex,sName))
            if tmpIndex > indexIROOT:
                indexIROOT  = tmpIndex
    if runConfig.output.extOutFile: 
        for sIndex,sName in enumerate(runConfig.output.extOutFile):
            # change * to X and add .tgz
            if sName.find('*') != -1:
                sName = sName.replace('*','XYZ')
                sName = '%s.tgz' % sName
            tmpIndex = getIndex(tmpList,"%s\.EXT%s\._(\d+)\.%s" % (options.outDS,sIndex,sName))
            if tmpIndex > indexEXT:
                indexEXT  = tmpIndex
    if runConfig.output.outStreamG:            
        for sName in runConfig.output.outStreamG:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.pool\.root" % (options.outDS,sName))
            if tmpIndex > indexStreamG:
                indexStreamG = tmpIndex
    if runConfig.output.outMeta:            
        for sName,sAsso in runConfig.output.outMeta:
            iMeta = 0
            if sAsso == 'None':
                tmpIndex = getIndex(tmpList,"%s\.META%s\._(\d+)\.root" % (options.outDS,iMeta))
                iMeta += 1
                if tmpIndex > indexMeta:
                    indexMeta = tmpIndex
    if runConfig.output.outMS:                
        for sName,sAsso in runConfig.output.outMS:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.pool\.root" % (options.outDS,sName))
            if tmpIndex > indexMS:
                indexMS = tmpIndex

# check permission
if not Client.checkSiteAccessPermission(options.site,options.workingGroup,options.verbose):
    sys.exit(EC_Config)

if options.verbose:
    print "== parameters =="
    print "Site       : %s" % options.site
    print "Athena     : %s" % athenaVer
    if groupArea != '':
        print "Group Area : %s" % groupArea
    if cacheVer != '':
        print "ProdCache  : %s" % cacheVer[1:]
    if nightVer != '':
        print "Nightly    : %s" % nightVer[1:]        
    print "RunDir     : %s" % runDir
    print "jobO       : %s" % jobO.lstrip()


####################################################################3
# submit jobs

# read jobID
jobDefinitionID = 0
jobid_file = '%s/pjobid.dat' % os.environ['PANDA_CONFIG_ROOT']
if os.path.exists(jobid_file):
    try:
        # read line
        tmpJobIdFile = open(jobid_file)
        tmpID = tmpJobIdFile.readline()
        tmpJobIdFile.close()
        # remove \n
        tmpID = tmpID.replace('\n','')
        # convert to int
        jobDefinitionID = long(tmpID) + 1
    except:
        pass

# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec

jobList = []

# full execution string
fullExecString = PsubUtils.convSysArgv()

# job name
jobName = commands.getoutput('uuidgen')

# build job
if options.nobuild:
    pass
elif options.libDS == '': 
    jobB = JobSpec()
    jobB.jobDefinitionID   = jobDefinitionID
    jobB.jobName           = jobName
    jobB.AtlasRelease      = 'Atlas-%s' % athenaVer
    jobB.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
    jobB.transformation    = '%s/buildJob-00-00-03' % Client.baseURLSUB
    hostName = commands.getoutput('hostname').split('.')[0]
    jobB.destinationDBlock = 'user%s.%s.%s.%s.lib._%06d' % (time.strftime('%y',time.gmtime()),distinguishedName,
                                                               time.strftime('%m%d%H%M%S',time.gmtime()),
                                                               datetime.datetime.utcnow().microsecond,
                                                               jobDefinitionID)
    if options.useExperimental and options.destSE != '':
        # write outputs to destSE
        jobB.destinationSE     = options.destSE
    else:
        jobB.destinationSE     = options.site
    jobB.prodSourceLabel   = 'panda'        
    if options.processingType != '':    
        jobB.processingType = options.processingType
    if options.seriesLabel != '':    
        jobB.prodSeriesLabel = options.seriesLabel
    jobB.workingGroup      = options.workingGroup    
    jobB.assignedPriority  = 2000
    jobB.computingSite     = options.site
    if options.burstSubmit == '':
        jobB.cloud = Client.PandaSites[options.site]['cloud']
    jobB.metadata = fullExecString
    fileBO = FileSpec()
    fileBO.lfn = '%s.lib.tgz' % jobB.destinationDBlock
    fileBO.type = 'output'
    fileBO.dataset = jobB.destinationDBlock
    fileBO.destinationDBlock = jobB.destinationDBlock
    fileBO.destinationSE = jobB.destinationSE
    jobB.addFile(fileBO)
    fileBI = FileSpec()
    fileBI.lfn = archiveName
    fileBI.type = 'input'
    jobB.jobParameters     = '-i %s -o %s' % (fileBI.lfn,fileBO.lfn)
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        jobB.jobParameters += " --sourceURL %s " % matchURL.group(1)
    # log    
    file = FileSpec()
    file.lfn  = '%s.log.tgz' % jobB.destinationDBlock
    file.type = 'log'
    file.dataset = jobB.destinationDBlock
    file.destinationDBlock = jobB.destinationDBlock
    file.destinationSE = jobB.destinationSE
    jobB.addFile(file)
    # set space token
    for file in jobB.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                if options.burstSubmit == '':
                    defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                    file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    jobList.append(jobB)
else:
    # query files in lib dataset to reuse libraries
    tmpLog.info("query files in %s" % options.libDS)
    tmpList = Client.queryFilesInDataset(options.libDS,options.verbose)
    tmpFileList = []
    tmpGUIDmap = {}
    for fileName in tmpList.keys():
        # ignore log file
        if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
            continue
        tmpFileList.append(fileName)
        tmpGUIDmap[fileName] = tmpList[fileName]['guid'] 
    # incomplete libDS
    if tmpFileList == []:
        # query files in dataset from Panda
	status,tmpMap = Client.queryLastFilesInDataset([options.libDS],options.verbose)
        # look for lib.tgz
	for fileName in tmpMap[options.libDS]:
            # ignore log file
            if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
                continue
            tmpFileList.append(fileName)
            tmpGUIDmap[fileName] = None
    # incomplete libDS
    if tmpFileList == []:
        tmpLog.error("lib dataset %s is empty" % options.libDS)
        sys.exit(EC_Dataset)
    # check file list                
    if len(tmpFileList) != 1:
        tmpLog.error("dataset %s contains multiple lib.tgz files : %s" % (options.libDS,tmpFileList))
        sys.exit(EC_Dataset)
    # instantiate FileSpec
    fileBO = FileSpec()
    fileBO.lfn = tmpFileList[0]
    fileBO.GUID = tmpGUIDmap[fileBO.lfn]
    fileBO.dataset = options.libDS
    fileBO.destinationDBlock = options.libDS
    if fileBO.GUID != 'NULL':
        fileBO.status = 'ready'
    
# run athena            

if options.inDS != '' and options.burstSubmit == '':
    tmpLog.info("%s files are missing or skipped at %s" % (len(missList),options.site))
    tmpLog.info("use %s files" % len(inputFileList)) 

# disp datasetname to identify inputs
commonDispName = 'user_disp.%s' % commands.getoutput('uuidgen')

# split job
#@ Loop for jobs here
for iSubJob in range(options.split):
    # terminate condition: no remaining files
    if (options.inDS != '' or options.shipinput or options.pfnList != '') and indexFiles >= len(inputFileList):
        break
    # instantiate sub-job
    jobR = JobSpec()
    jobR.jobDefinitionID   = jobDefinitionID
    jobR.jobName           = jobName
    jobR.AtlasRelease      = 'Atlas-%s' % athenaVer
    jobR.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
    jobR.transformation    = '%s/runAthena-00-00-11' % Client.baseURLSUB
    if options.inDS == '':
        jobR.prodDBlock        = 'NULL'
    else:
        jobR.prodDBlock        = options.inDS
    jobR.destinationDBlock = options.outDS
    if options.useExperimental and options.destSE != '':
        # write outputs to destSE
        jobR.destinationSE     = options.destSE
    else:
        jobR.destinationSE     = options.site
    jobR.prodSourceLabel   = 'user'        
    if options.processingType != '':
        jobR.processingType = options.processingType
    if options.seriesLabel != '':    
        jobR.prodSeriesLabel = options.seriesLabel
    jobR.workingGroup      = options.workingGroup            
    jobR.assignedPriority  = 1000
    jobR.metadata          = fullExecString    
    # memory
    if options.memory != -1:
        jobR.minRamCount = options.memory
    # CPU count
    if options.maxCpuCount != -1:
        jobR.maxCpuCount = options.maxCpuCount
    jobR.computingSite = options.site
    if options.burstSubmit == '':
        jobR.cloud = Client.PandaSites[options.site]['cloud']
    # source files
    if not options.nobuild:
        fileS = FileSpec()
        fileS.lfn     = fileBO.lfn
        fileS.GUID    = fileBO.GUID
        fileS.type    = 'input'
        fileS.status  = fileBO.status
        fileS.dataset = fileBO.destinationDBlock
        fileS.dispatchDBlock = fileBO.destinationDBlock
        jobR.addFile(fileS)
    # input files
    inList       = []
    minList      = []
    cavList      = []
    bhaloList    = []
    bgasList     = []
    guidBoundary = []
    if options.inDS != '' or options.shipinput or options.pfnList != '':
        # calculate N files
        (divF,modF) = divmod(len(inputFileList),options.split)
        if modF != 0:
            divF += 1
        # if split by files was specified
        if options.nFilesPerJob > 0:
            divF = options.nFilesPerJob 
        # if split by events was specified then
        if options.nEventsPerJob > 0:
           #@Calculate how many events to skip
           nEventsToSkip = nSkips*options.nEventsPerJob
           # @Increment number of skipped blocks
           nSkips = nSkips + 1
           #Take just one file
           divF = 1
           # @ If splitting of file per event is complete then take the next file
           if nEventsToSkip >= options.nEventsPerFile :
               nEventsToSkip = 0
               nSkips        = 1
               indexFiles   += divF
        # File Selector    
        tmpList = inputFileList[indexFiles:indexFiles+divF]
        if options.inDS != '':
            totalSize = 0
	    for fileName in tmpList:
                vals = inputFileMap[fileName]
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
		file.GUID           = vals['guid']
		file.fsize          = vals['fsize']
		file.md5sum         = vals['md5sum']
                file.dataset        = options.inDS
                file.prodDBlock     = options.inDS
                file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                inList.append(fileName)
                try:
                    totalSize += long(file.fsize)
                except:
                    pass
            # size check    
            if totalSize > maxTotalSize:
                tmpStr  = "A subjob has %s input files and requires %sMB of disk space.\n" \
                          % (len(tmpList), (totalSize >> 20))
                tmpStr += "  It must be less than %sMB to avoid overflowing the remote disk.\n" \
                          % (maxTotalSize >> 20)
                tmpStr += "  Please split the job using --nFilesPerJob."
                tmpLog.error(tmpStr)
                sys.exit(EC_Split)
        else:        
	    for fileName in tmpList:
                # use GUID boundaries or not
                if devidedByGUID:
                    # collect GUIDs
                    guidBoundary.append(fileName)
                    # fileName is a GUID in this case 
                    realFileName = guidCollMap[fileName]
                    if not realFileName in inList:
                        inList.append(realFileName)
                else:
                    inList.append(fileName)
        # Minimum Bias
        if runConfig.input.inMinBias:
            if indexMin+options.nMin*divF >= len(minbiasList):
                # re-use files when Minimum-Bias files are not enough   
                indexMin = 0   
            tmpList = minbiasList[indexMin:indexMin+options.nMin*divF]
            indexMin += options.nMin*divF
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = minbiasDataset
                file.prodDBlock     = minbiasDataset
                file.dispatchDBlock = commonDispName
                file.type       = 'input'
                file.status     = 'ready'
                jobR.addFile(file)
                minList.append(fileName)
            # Cavern
            if indexCavern+options.nCav*divF >= len(cavernList):
                # re-use files when Cavern files are not enough   
                indexCavern = 0   
            tmpList = cavernList[indexCavern:indexCavern+options.nCav*divF]
            indexCavern += options.nCav*divF
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = cavernDataset
                file.prodDBlock     = cavernDataset
                file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                cavList.append(fileName)
        # BeamHalo
        if runConfig.input.inBeamHalo:
            if options.useCommonHalo:
                # integrated dataset
                if indexBHalo+options.nBeamHalo >= len(beamHaloList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHalo = 0   
                tmpList = beamHaloList[indexBHalo:indexBHalo+options.nBeamHalo]
                indexBHalo += options.nBeamHalo
            else:
                # separate datasets
                if indexBHaloA+options.nBeamHaloA >= len(beamHaloAList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHaloA = 0   
                if indexBHaloC+options.nBeamHaloC >= len(beamHaloCList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHaloC = 0   
                tmpList = beamHaloAList[indexBHaloA:indexBHaloA+options.nBeamHaloA] + \
                          beamHaloCList[indexBHaloC:indexBHaloC+options.nBeamHaloC]
                indexBHaloA += options.nBeamHaloA
                indexBHaloC += options.nBeamHaloC
            tmpIndex = 0
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                if options.useCommonHalo:
                    # integrated dataset
                    file.dataset        = beamHaloDataset
                    file.prodDBlock     = beamHaloDataset
                    file.dispatchDBlock = commonDispName
                else:
                    # separate datasets
                    if tmpIndex < options.nBeamHaloA:
                        file.dataset        = beamHaloAdataset
                        file.prodDBlock     = beamHaloAdataset
                        file.dispatchDBlock = commonDispName
                    else:
                        file.dataset        = beamHaloCdataset
                        file.prodDBlock     = beamHaloCdataset
                        file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                bhaloList.append(fileName)
                tmpIndex += 1
        # BeamGas
        if runConfig.input.inBeamGas:        
            if options.useCommonGas:
                # integrated dataset
                if indexBGas+options.nBeamGas >= len(beamGasList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasO = 0   
                tmpList = beamGasList[indexBGas:indexBGas+options.nBeamGas]
                indexBGas += options.nBeamGas
            else:
                # separate dataset
                if indexBGasH+options.nBeamGasH >= len(beamGasHList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasH = 0   
                if indexBGasC+options.nBeamGasC >= len(beamGasCList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasC = 0   
                if indexBGasO+options.nBeamGasO >= len(beamGasOList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasO = 0   
                tmpList = beamGasHList[indexBGasH:indexBGasH+options.nBeamGasH] + \
                          beamGasCList[indexBGasC:indexBGasC+options.nBeamGasC] + \
                          beamGasOList[indexBGasO:indexBGasO+options.nBeamGasO]
                indexBGasH += options.nBeamGasH
                indexBGasC += options.nBeamGasC
                indexBGasO += options.nBeamGasO
            tmpIndex = 0
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                if options.useCommonHalo:
                    # integrated dataset
                    file.dataset        = beamGasDataset
                    file.prodDBlock     = beamGasDataset
                    file.dispatchDBlock = commonDispName
                else:
                    # separate datasets
                    if tmpIndex < options.nBeamGasH:
                        file.dataset        = beamGasHdataset
                        file.prodDBlock     = beamGasHdataset
                        file.dispatchDBlock = commonDispName
                    elif tmpIndex < (options.nBeamGasH+options.nBeamGasC):
                        file.dataset        = beamGasCdataset
                        file.prodDBlock     = beamGasCdataset
                        file.dispatchDBlock = commonDispName
                    else:
                        file.dataset        = beamGasOdataset
                        file.prodDBlock     = beamGasOdataset
                        file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                bgasList.append(fileName)
                tmpIndex += 1
        #@
        #@ important
        #@ Done with an input files description of the job.
        # Increment pointer (index) to a next block of files
        #@ If split by events is requested Index file is incremented in a different place (above)

        if options.nEventsPerJob < 0:
          indexFiles += divF

            
    # output files
    outMap = {}
    if runConfig.output.outNtuple:
        indexNT += 1
        for sName in runConfig.output.outNtuple:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.root' % (jobR.destinationDBlock,sName,indexNT)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('ntuple'):
                outMap['ntuple'] = []
            outMap['ntuple'].append((sName,file.lfn))
    if runConfig.output.outHist:
        indexHIST += 1
        file = FileSpec()
        file.lfn  = '%s.hist._%05d.root' % (jobR.destinationDBlock,indexHIST)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_HIST'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['hist'] = file.lfn
    if runConfig.output.outRDO:
        indexRDO += 1        
        file = FileSpec()
        file.lfn  = '%s.RDO._%05d.pool.root' % (jobR.destinationDBlock,indexRDO)        
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_RDO'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['RDO'] = file.lfn
    if runConfig.output.outESD:
        indexESD += 1        
        file = FileSpec()
        file.lfn  = '%s.ESD._%05d.pool.root' % (jobR.destinationDBlock,indexESD)        
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_ESD'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['ESD'] = file.lfn
    if runConfig.output.outAOD:
        indexAOD += 1                
        file = FileSpec()
        file.lfn  = '%s.AOD._%05d.pool.root' % (jobR.destinationDBlock,indexAOD)        
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_AOD'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['AOD'] = file.lfn
    if runConfig.output.outTAG:
        indexTAG += 1                        
        file = FileSpec()
        file.lfn  = '%s.TAG._%05d.coll.root' % (jobR.destinationDBlock,indexTAG)                
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_TAG'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['TAG'] = file.lfn
    if runConfig.output.outAANT:
        indexAANT += 1
        sNameList = []
        for aName,sName in runConfig.output.outAANT:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.root' % (jobR.destinationDBlock,sName,indexAANT)       
            file.type = 'output'
            file.dataset = jobR.destinationDBlock        
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            if not sName in sNameList:
                sNameList.append(sName)
                jobR.addFile(file)
            if not outMap.has_key('AANT'):
                outMap['AANT'] = []
            outMap['AANT'].append((aName,sName,file.lfn))
    if runConfig.output.outTHIST:
        indexTHIST += 1
        for sName in runConfig.output.outTHIST:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.root' % (jobR.destinationDBlock,sName,indexTHIST)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('THIST'):
                outMap['THIST'] = []
            outMap['THIST'].append((sName,file.lfn))
    if runConfig.output.outIROOT:
        indexIROOT += 1
        for sIndex,sName in enumerate(runConfig.output.outIROOT):
            file = FileSpec()
            file.lfn  = '%s.iROOT%s._%05d.%s' % (jobR.destinationDBlock,sIndex,indexIROOT,sName)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_iROOT%s' % sIndex
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('IROOT'):
                outMap['IROOT'] = []
            outMap['IROOT'].append((sName,file.lfn))
    if options.extOutFile:
        indexEXT += 1
        for sIndex,sName in enumerate(options.extOutFile):
            # change * to X and add .tgz
            origSName = sName
            if sName.find('*') != -1:
                sName = sName.replace('*','XYZ')
                sName = '%s.tgz' % sName
            file = FileSpec()
            file.lfn  = '%s.EXT%s._%05d.%s' % (jobR.destinationDBlock,sIndex,indexEXT,sName)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_EXT%s' % sIndex
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('IROOT'):
                outMap['IROOT'] = []
            outMap['IROOT'].append((origSName,file.lfn))
    if runConfig.output.outStream1:
        indexStream1 += 1                                        
        file = FileSpec()
        file.lfn  = '%s.Stream1._%05d.pool.root' % (jobR.destinationDBlock,indexStream1)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_Stream1'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['Stream1'] = file.lfn
    if runConfig.output.outStream2:
        indexStream2 += 1                                        
        file = FileSpec()
        file.lfn  = '%s.Stream2._%05d.pool.root' % (jobR.destinationDBlock,indexStream2)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_Stream2'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['Stream2'] = file.lfn
    if runConfig.output.outBS:
        indexBS += 1                                        
        file = FileSpec()
        file.lfn  = '%s.BS._%05d.data' % (jobR.destinationDBlock,indexBS)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_BS'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['BS'] = file.lfn
    if runConfig.output.outSelBS:
        indexSelBS += 1                                        
        file = FileSpec()
        file.lfn  = '%s.%s._%05d.data' % (jobR.destinationDBlock,runConfig.output.outSelBS,indexSelBS)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_SelBS'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        if not outMap.has_key('IROOT'):
            outMap['IROOT'] = []
        outMap['IROOT'].append(('%s.*.data' % runConfig.output.outSelBS,file.lfn))
    if runConfig.output.outStreamG:
        indexStreamG += 1
        for sName in runConfig.output.outStreamG:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.pool.root' % (jobR.destinationDBlock,sName,indexStreamG)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('StreamG'):
                outMap['StreamG'] = []
            outMap['StreamG'].append((sName,file.lfn))
    if runConfig.output.outMeta:
        iMeta = 0
	indexMeta += 1
        for sName,sAsso in runConfig.output.outMeta:
            foundLFN = ''
            if sAsso == 'None':
                # non-associated metadata
                file = FileSpec()
                file.lfn  = '%s.META%s._%05d.root' % (jobR.destinationDBlock,iMeta,indexMeta)
                file.type = 'output'
                file.dataset = jobR.destinationDBlock
                file.destinationDBlock = jobR.destinationDBlock
                if options.individualOutDS:
                    tmpSuffix = '_META%s' % iMeta
                    file.dataset += tmpSuffix
                    file.destinationDBlock += tmpSuffix
                file.destinationSE = jobR.destinationSE
                jobR.addFile(file)
                iMeta += 1
                foundLFN = file.lfn
            elif outMap.has_key(sAsso):
                # Stream1,2
                foundLFN = outMap[sAsso]
            elif sAsso in ['StreamRDO','StreamESD','StreamAOD']:
                # RDO,ESD,AOD
                stKey = re.sub('^Stream','',sAsso)
                if outMap.has_key(stKey):
                    foundLFN = outMap[stKey]
            else:
                # general stream
                if outMap.has_key('StreamG'):
                    for tmpStName,tmpLFN in outMap['StreamG']:
                        if tmpStName == sAsso:
                            foundLFN = tmpLFN
            if foundLFN != '':
                if not outMap.has_key('Meta'):
                    outMap['Meta'] = []
                outMap['Meta'].append((sName,foundLFN))
    if runConfig.output.outMS:
	indexMS += 1
        for sName,sAsso in runConfig.output.outMS:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.pool.root' % (jobR.destinationDBlock,sName,indexMS)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('IROOT'):
                outMap['IROOT'] = []
            outMap['IROOT'].append((sAsso,file.lfn))
    if runConfig.output.outUserData:
        for sAsso in runConfig.output.outUserData:
            # look for associated LFN
            foundLFN = ''            
            if outMap.has_key(sAsso):
                # Stream1,2
                foundLFN = outMap[sAsso]
            elif sAsso in ['StreamRDO','StreamESD','StreamAOD']:
                # RDO,ESD,AOD
                stKey = re.sub('^Stream','',sAsso)
                if outMap.has_key(stKey):
                    foundLFN = outMap[stKey]
            else:
                # general stream
                if outMap.has_key('StreamG'):
                    for tmpStName,tmpLFN in outMap['StreamG']:
                        if tmpStName == sAsso:
                            foundLFN = tmpLFN
            if foundLFN != '':
                if not outMap.has_key('UserData'):
                    outMap['UserData'] = []
                outMap['UserData'].append(foundLFN)
                                            
    # log
    file = FileSpec()
    file.lfn  = '%s._$PANDAID.log.tgz' % jobR.destinationDBlock
    file.type = 'log'
    file.dataset = jobR.destinationDBlock    
    file.destinationDBlock = jobR.destinationDBlock
    if options.individualOutDS:
        # use original outDS for log, which guarantees location registration and shadow tracing
        pass
    file.destinationSE = jobR.destinationSE
    jobR.addFile(file)
    # set space token
    for file in jobR.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                if options.burstSubmit == '':                
                    defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                    file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # job parameters
    param = ''
    if not options.nobuild:
        param  += '-l %s ' % fileS.lfn
    param += '-r %s ' % runDir
    if not options.trf:
        tmpJobO = jobO        
        # modify one-liner for G4 random seeds
        if runConfig.other.G4RandomSeeds > 0:
            if options.singleLine != '':
                tmpJobO = re.sub('-c "%s" ' % options.singleLine,
                                 '-c "%s;from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=%s" ' \
                                 % (options.singleLine,runConfig.other.G4RandomSeeds+iSubJob),
                                 jobO)
            else:
                tmpJobO = ('-c "from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=%s" ' \
                           % (runConfig.other.G4RandomSeeds+iSubJob)) + jobO
        # replace full-path jobOs
        for tmpFullName,tmpLocalName in AthenaUtils.fullPathJobOs.iteritems():
            tmpJobO = re.sub(tmpFullName,tmpLocalName,tmpJobO)
        # set jobO parameter
        param += '-j "%s" ' % urllib.quote(tmpJobO)
        # DBRelease
        if options.dbRelease != '':
            tmpItems = options.dbRelease.split(':')
            tmpDbrDS  = tmpItems[0]
            tmpDbrLFN = tmpItems[1]
            # instantiate  FileSpec
            fileName = tmpDbrLFN
            vals     = dbrFiles[tmpDbrLFN]
            file = FileSpec()
            file.lfn            = fileName
            file.GUID           = vals['guid']
            file.fsize          = vals['fsize']
            file.md5sum         = vals['md5sum']
            file.dataset        = tmpDbrDS
            file.prodDBlock     = tmpDbrDS
            file.dispatchDBlock = tmpDbrDS
            file.type       = 'input'
            file.status     = 'ready'
            jobR.addFile(file)
            # set DBRelease parameter
            param += '--dbrFile %s ' % file.lfn
            if options.dbRunNumber != '':
                param += '--dbrRun %s ' % options.dbRunNumber
    else:
        # replace parameters for TRF
        tmpJobO = jobO
        # output : basenames are in outMap['IROOT'] trough extOutFile
        tmpOutMap = []
        for tmpName,tmpLFN in outMap['IROOT']:
            tmpJobO = tmpJobO.replace('%OUT.' + tmpName,tmpName)
            # set correct name in outMap
            tmpOutMap.append((tmpName,tmpLFN))
        # set output for normal TRF (not for ARA)
        if not options.ara:
            outMap['IROOT'] = tmpOutMap 
        # input
	inPattList = [('%IN',inList),('%MININ',minList),('%CAVIN',cavList),
                      ('%BHIN',bhaloList),('%BGIN',bgasList)]    
	for tmpPatt,tmpInList in inPattList:
            if tmpJobO.find(tmpPatt) != -1 and len(tmpInList) > 0:
                tmpJobO = AthenaUtils.replaceParam(tmpPatt,tmpInList,tmpJobO)
        # DBRelease
        for tmpItem in tmpJobO.split():
            match = re.search('%DB=([^:]+):(.+)$',tmpItem)
            if match:
                tmpDbrDS  = match.group(1)
                tmpDbrLFN = match.group(2)
                # instantiate  FileSpec
                fileName = tmpDbrLFN
                vals     = dbrFiles[tmpDbrLFN]
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = tmpDbrDS
                file.prodDBlock     = tmpDbrDS
                file.dispatchDBlock = tmpDbrDS
                file.type       = 'input'
                file.status     = 'ready'
                jobR.addFile(file)
                inList.append(fileName)
                # replace parameters
                tmpJobO = tmpJobO.replace(match.group(0),tmpDbrLFN)
        # random seed
        for tmpItem in tmpJobO.split():
            match = re.search('%RNDM=(.+)$',tmpItem)
            if match:
                tmpRndmNum = int(match.group(1)) + iSubJob
                # replace parameters
                tmpJobO = tmpJobO.replace(match.group(0),'%s' % tmpRndmNum)
        # skipEvent
        if tmpJobO.find('%SKIPEVENTS') != -1:
            tmpJobO = tmpJobO.replace('%SKIPEVENTS','%s' % nEventsToSkip)
        # set jobO parameter
        param += '-j "%s" ' % urllib.quote(tmpJobO)		
    param += '-i "%s" ' % inList
    param += '-m "%s" ' % minList
    param += '-n "%s" ' % cavList
    if bhaloList != []:
        param += '--beamHalo "%s" ' % bhaloList
    if bgasList != []:
        param += '--beamGas "%s" ' % bgasList
    param += '-o "%s" ' % outMap
    if runConfig.input.inColl:
        param += '-c '
    if runConfig.input.inBS:
        param += '-b '
    if runConfig.input.backNavi:
        param += '-e '
    if options.shipinput:
        param += '--shipInput '
        # GUID boundaries
        if devidedByGUID:
            param += '--guidBoundary "%s" ' % guidBoundary
            param += '--collRefName %s ' % runConfig.input.collRefName
    pStr1 = ''    
    if runConfig.other.rndmStream != []:
        pStr1 = "AtRndmGenSvc=Service('AtRndmGenSvc');AtRndmGenSvc.Seeds=["
        for stream in runConfig.other.rndmStream:
            num = runConfig.other.rndmNumbers[runConfig.other.rndmStream.index(stream)]
            pStr1 += "'%s %d %d'," % (stream,num[0]+iSubJob,num[1]+iSubJob)
        pStr1 += "]"

    # @ If split by event option was invoked
    pStr2 = ''
    if options.nEventsPerJob > 0 and (not options.trf):
        # @ Number of events to be processed per job
        param1 = "theApp.EvtMax=%s" % options.nEventsPerJob
        # @ possibly skip events in a file
        if runConfig.input.noInput:
            pStr2 = param1
        else:
            param2 = "EventSelector.SkipEvents=%s" % nEventsToSkip
            # @ Form a string to add to job parameters
            pStr2 = '%s;%s' % (param1,param2)
    # parameter
    if pStr1 != '' or pStr2 != '':
	if pStr1 == '' or pStr2 == '':
	    param += '-f "%s" ' % (pStr1+pStr2)
	else:
            param += '-f "%s;%s" ' % (pStr1,pStr2)
    # libDS 
    if options.libDS != "" or options.nobuild:
        param += '-a %s ' % archiveName
    # addPoolFC
    if options.addPoolFC != "":
        param += '--addPoolFC %s ' % options.addPoolFC
    # use corruption checker
    if options.corCheck:
        param += '--corCheck '
    # disable to skip missing files
    if options.notSkipMissing:
        param += '--notSkipMissing '
    # given PFN 
    if options.pfnList != '':
        param += '--givenPFN '
    # create symlink for MC data
    if options.mcData != '':
        param += '--mcData %s ' % options.mcData
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        param += " --sourceURL %s " % matchURL.group(1)
    # run TRF
    if options.trf:
        param += '--trf '
    # use ARA 
    if options.ara:
        param += '--ara '
    # general input format
    if options.generalInput:
        param += '--generalInput '
    # use theApp.nextEvent
    if options.useNextEvent:
        param += '--useNextEvent '
    # assign    
    jobR.jobParameters = param

    if options.verbose:
        tmpLog.debug(param)

    jobList.append(jobR)


# check output dataset length for individualOutDS
tmpIdvOutDsList = []
if options.individualOutDS:
    for tmpFile in jobList[-1].Files:
        if tmpFile.type in ['output','log']:
            if not tmpFile.dataset in tmpIdvOutDsList:
                if not PsubUtils.checkOutDsName(tmpFile.dataset,distinguishedName,options.official):
                    tmpLog.info("a suffix is added to datasetname for each output stream when --individualOutDS is used. e.g. outDS_suffix:%s. Please use shorter name for --outDS" % tmpFile.dataset)
                    sys.exit(EC_Config)
                tmpIdvOutDsList.append(tmpFile.dataset)


# no submit 
if not options.nosubmit:

    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,
                                          Client.PandaClouds[options.cloud]['pilotowners'],
                                          options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)

    # normal/burst submission
    if options.burstSubmit == '':
        # normal submission

        # register output dataset
	tmpIdvOutDsList = []
        if not outputDSexist:
            if not options.individualOutDS:
                Client.addDataset(options.outDS,options.verbose)
                tmpIdvOutDsList.append(options.outDS)
            else:
                # register individual datasets
                for tmpFile in jobList[-1].Files:
                    if tmpFile.type in ['output','log']:
                        if not tmpFile.dataset in tmpIdvOutDsList:
                            Client.addDataset(tmpFile.dataset,options.verbose)
                            tmpIdvOutDsList.append(tmpFile.dataset)
                            
        # register output dataset container
        if original_outDS_Name.endswith('/'):
            # create
            if not outputContExist:
                Client.createContainer(original_outDS_Name,options.verbose)
            # add dataset
            if not outputDSexist:
                for tmpIdvOutDS in tmpIdvOutDsList:
                    Client.addDatasetsToContainer(original_outDS_Name,[tmpIdvOutDS],options.verbose)

        # register shadow dataset
        if not outputDSexist:
            Client.addDataset("%s%s" % (options.outDS,suffixShadow),options.verbose)

        # register libDS
        if options.libDS == '' and (not options.nobuild):
            Client.addDataset(jobB.destinationDBlock,options.verbose)

        # submit
        tmpLog.info("submit to %s" % options.site)
        status,out = Client.submitJobs(jobList,options.verbose)
        if not Client.PandaSites[options.site]['status'] in ['online','brokeroff']:
            tmpLog.warning("%s is %s. Your jobs will wait until it becomes online" % \
                           (options.site,Client.PandaSites[options.site]['status']))
    else:
        print "\n=========="
        # burst submission
        origLibDS = jobB.destinationDBlock
        origOutDS = options.outDS
        prevLibDS = jobB.destinationDBlock
        prevOutDS = options.outDS
        # loop over all sites
        for tmpSite in options.burstSubmit.split(','):
            # cloud
            tmpCloud = Client.PandaSites[tmpSite]['cloud']
            newJobList = []
            newLibDS = '%s.%s' % (origLibDS,tmpSite)
            newOutDS = '%s.%s' % (origOutDS,tmpSite)
            repPatt  = {}
            if options.removeBurstLimit:
                # use all jobs
                limitedJobList = jobList
            else:
                # use buildJob and one runAthena by default
                limitedJobList = jobList[:2]
            for tmpJob in limitedJobList:
                job = copy.deepcopy(tmpJob)
                # set cloud/site
                job.cloud          = tmpCloud
                job.computingSite  = tmpSite
                job.destinationSE  = job.computingSite
                # set destDBlock
                if job.prodSourceLabel == 'panda':
                    job.destinationDBlock = newLibDS
                    # pattern to modify LFN
                    subLibDSPatt = '^'+prevLibDS
                else:
                    job.destinationDBlock = newOutDS
                    # pattern to modify LFN
                    subOutDSPatt = '^'+prevOutDS
                # correct files
                for file in job.Files:
                    # process output/log and lib.tgz
                    if file.type == 'input':
                        # skip buildJob
                        if job.prodSourceLabel == 'panda':
                            continue
                        # only lib.tgz
                        if re.search(subLibDSPatt,file.dataset) == None:
                            continue
                        # set datasets 
                        file.dataset           = newLibDS
                        file.prodDBlock        = newLibDS
                        file.dispatchDBlock    = newLibDS
                    else:
                        # set datasets
                        if job.prodSourceLabel == 'panda':
                            file.dataset           = newLibDS
                            file.prodDBlock        = newLibDS                    
                            file.destinationDBlock = newLibDS
                        else:
                            file.dataset           = newOutDS
                            file.prodDBlock        = newOutDS                    
                            file.destinationDBlock = newOutDS
                        file.destinationSE = job.destinationSE
                    # modify LFN
                    oldLFN = file.lfn
                    if job.prodSourceLabel == 'panda' or file.type == 'input':
                        newLFN = re.sub(subLibDSPatt,file.dataset,oldLFN)
                    else:
                        newLFN = re.sub(subOutDSPatt,file.dataset,oldLFN)
                    file.lfn = newLFN
                    # pattern for parameter replacement
                    repPatt[oldLFN] = newLFN
                # modify jobParams
                for oldLFN,newLFN in repPatt.iteritems():
                    job.jobParameters = re.sub(oldLFN,newLFN,job.jobParameters)
                # append
                newJobList.append(job)
            # submit
            tmpLog.info("submit to %s" % tmpSite)
            status,out = Client.submitJobs(newJobList,options.verbose)
            if status==0:
                tmpLog.info(" OK")
		jobID = out[0][1]
                # record jobID
		tmpJobIdFile = open(jobid_file,'w')
                tmpJobIdFile.write(str(jobID))
                tmpJobIdFile.close()
            else:
                tmpLog.info(" NG : %s" % status)
            time.sleep(2)
        # don't update DB
        sys.exit(0)


    print '==================='
    if out == []:
        tmpLog.error("Job submission was denied")
        sys.exit(EC_Submit)
    outstr   = ''
    buildStr = ''
    runStr   = ''
    for index,o in enumerate(out):
        if o != None:
            if index==0:
                # set JobID
                jobID = o[1]
            if index==0 and options.libDS=='' and (not options.nobuild):
                outstr += "  > build\n"
                outstr += "    PandaID=%s\n" % o[0]
                buildStr = '%s' % o[0]            
            elif (index==1 and options.libDS=='') or \
                 (index==0 and (options.libDS!='' or options.nobuild)):
                outstr += "  > run\n"
                outstr += "    PandaID=%s" % o[0]
                runStr = '%s' % o[0]                        
            elif index+1==len(out):
                outstr += "-%s" % o[0]
                runStr += '-%s' % o[0]                                    
    print ' JobID  : %s' % jobID
    print ' Status : %d' % status
    print outstr

    # create dir for DB
    dbdir = os.path.expanduser(os.environ['PANDA_CONFIG_ROOT'])
    if not os.path.exists(dbdir):
        os.makedirs(dbdir)

    # record jobID
    tmpJobIdFile = open(jobid_file,'w')
    tmpJobIdFile.write(str(jobID))
    tmpJobIdFile.close()

    # record libDS
    if options.libDS == '' and not options.nobuild:
        tmpFile = open(libds_file,'w')
        tmpFile.write(jobB.destinationDBlock)
        tmpFile.close()

# go back to current dir
os.chdir(currentDir)

# try another site if input files remain
options.crossSite -= 1
if options.crossSite > 0 and options.inDS != '' and not siteSpecified:
    if missList != []:
        anotherTry = True
        # nfiles
        if options.nfiles != 0:
            if options.nfiles > len(inputFileMap):
                fullExecString = re.sub('--nFiles\s*=*\d+',
                                        '--nFiles=%s' % (options.nfiles-len(inputFileMap)),
                                        fullExecString)
            else:
                anotherTry = False
 	# decrement crossSite counter
        fullExecString = re.sub(' --crossSite\s*=*\d+','',fullExecString)
        fullExecString += ' --crossSite=%s' % options.crossSite
        # don't reuse site
        match = re.search('--excludedSite\s*=*\S+',fullExecString)
        if match != None:
            # append site
            fullExecString = re.sub(match.group(0),'%s,%s' % (match.group(0),options.site),fullExecString)
        else:
            # add option
            fullExecString += ' --excludedSite=%s' % options.site
        # remove --fileList
        fullExecString = re.sub('--fileList\s*=*\S+','',fullExecString)
        # set list of input files
        inputTmpfile = '%s/intmp.%s' % (tmpDir,commands.getoutput('uuidgen'))
        iFile = open(inputTmpfile,'w')
        for tmpMiss in missList:
            iFile.write(tmpMiss+'\n')
        iFile.close()
        fullExecString = re.sub(' --inputFileList\s*=*\S+','',fullExecString)
        fullExecString += ' --inputFileList=%s' % inputTmpfile
        # source name
        if not '--panda_srcName' in fullExecString:
            fullExecString += ' --panda_srcName=%s' % archiveName
        # server URL
        if not '--panda_srvURL' in fullExecString:
            fullExecString += ' --panda_srvURL=%s,%s' % (Client.baseURL,Client.baseURLSSL)
        # run config
        confTmpfile = ''
        if not '--panda_runConfig' in fullExecString:
            conTmpfile = '%s/conftmp.%s' % (tmpDir,commands.getoutput('uuidgen'))
            cFile = open(conTmpfile,'w')
            pickle.dump(runConfig,cFile)
            cFile.close()
            fullExecString += ' --panda_runConfig=%s' % conTmpfile

        # run pathena
        if anotherTry:
            tmpLog.info("trying other sites for the missing files")
            com = 'pathena ' + fullExecString
            if options.verbose:
                tmpLog.debug(com)
            status = os.system(com)
            # delete tmp files
            commands.getoutput('\rm -f %s' % inputTmpfile)
            commands.getoutput('\rm -f %s' % conTmpfile)            
            # exit
            sys.exit(status)

# succeeded
sys.exit(0)
